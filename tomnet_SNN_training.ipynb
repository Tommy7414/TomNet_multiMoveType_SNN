{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcfd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from __future__ import annotations\n",
    "from typing import Literal\n",
    "import torch, torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate, utils as snn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_rc(idx, H=24, W=24):\n",
    "    r = (idx // W).clamp(min=0, max=H-1)\n",
    "    c = (idx %  W).clamp(min=0, max=W-1)\n",
    "    return r, c\n",
    "\n",
    "def triple_to_map3(ids_3, H=24, W=24):\n",
    "    B = ids_3.shape[0]\n",
    "    maps = torch.zeros(B, 3, H, W, device=ids_3.device)\n",
    "    for a in range(3):\n",
    "        r, c = id_to_rc(ids_3[:, a], H, W)\n",
    "        maps[torch.arange(B), a, r, c] = 1.0\n",
    "    return maps\n",
    "def prefix_to_spike_seq_batched(prefix_ids, attn_mask, pad_id=576, cls_id=577, H=24, W=24):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "      spike_seq: [T_max, B, 3, H, W]  \n",
    "      T_lens:    [B]                  \n",
    "    \"\"\"\n",
    "    device = prefix_ids.device\n",
    "    B, L = prefix_ids.shape\n",
    "    seq_per_b = []\n",
    "    T_lens = []\n",
    "\n",
    "    for b in range(B):\n",
    "        keep_b = attn_mask[b].clone()\n",
    "        if prefix_ids[b, 0] == cls_id:\n",
    "            keep_b[0] = False  # 忽略 CLS\n",
    "\n",
    "        ids_b = prefix_ids[b, keep_b]           # [Li]\n",
    "        Li = ids_b.numel()\n",
    "        if Li % 3 != 0:\n",
    "            raise ValueError(f\"sample {b}: prefix_ids length {Li} is not a multiple of 3\")\n",
    "\n",
    "        T_i = Li // 3\n",
    "        T_lens.append(T_i)\n",
    "\n",
    "        if T_i == 0:\n",
    "            seq_per_b.append(torch.zeros(1, 3, H, W, device=device))\n",
    "            T_lens[-1] = 1\n",
    "            continue\n",
    "\n",
    "        triples = ids_b.view(T_i, 3)            # [T_i, 3]\n",
    "        maps_t = []\n",
    "        for t in range(T_i):\n",
    "            maps_t.append(triple_to_map3(triples[t].unsqueeze(0), H, W)[0])  # [3,H,W]\n",
    "        seq_per_b.append(torch.stack(maps_t, dim=0))  # [T_i,3,H,W]\n",
    "\n",
    "    T_lens = torch.tensor(T_lens, device=device, dtype=torch.long)\n",
    "    T_max = int(max(T_lens).item())\n",
    "\n",
    "    # pad 到 [T_max,B,3,H,W]\n",
    "    out = torch.zeros(T_max, B, 3, H, W, device=device)\n",
    "    for b in range(B):\n",
    "        Ti = int(T_lens[b].item())\n",
    "        out[:Ti, b] = seq_per_b[b]\n",
    "\n",
    "    return out, T_lens  # [T_max,B,3,H,W], [B]\n",
    "\n",
    "\n",
    "def last_step_query(prefix_ids, attn_mask, H=24, W=24):\n",
    "    # 取最後 3 個有效 token -> [B,3,H,W]\n",
    "    B, L = prefix_ids.shape\n",
    "    lengths = attn_mask.sum(dim=1)           \n",
    "    idx_end = lengths - 1\n",
    "    idxs = torch.stack([idx_end-2, idx_end-1, idx_end], dim=1).clamp(min=0)\n",
    "    gather = prefix_ids.gather(1, idxs)                 # [B,3]\n",
    "    return triple_to_map3(gather, H, W)\n",
    "\n",
    "def choices_to_maps(choices_ids, H=24, W=24):\n",
    "    B = choices_ids.shape[0]\n",
    "    out = []\n",
    "    for k in range(4):\n",
    "        out.append(triple_to_map3(choices_ids[:, k, :], H, W))\n",
    "    return torch.stack(out, dim=1)                      # [B,4,3,H,W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_adj(H, W, device=None):\n",
    "    n = H*W\n",
    "    A = torch.zeros(n, n, device=device)\n",
    "    for i in range(n):\n",
    "        if i-W >= 0:     A[i, i-W] = 1\n",
    "        if i+W < n:      A[i, i+W] = 1\n",
    "        if i%W != 0:     A[i, i-1] = 1\n",
    "        if (i+1)%W != 0: A[i, i+1] = 1\n",
    "    A.fill_diagonal_(1)\n",
    "    return A\n",
    "\n",
    "class SGA(nn.Module):\n",
    "    \"\"\"Spiking Graph Attention on grid nodes: Q,K,V → spk → masked attn → spk V\"\"\"\n",
    "    def __init__(self, d_in, d_out, beta=0.95):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=False)\n",
    "        sgrad = surrogate.fast_sigmoid(slope=25)\n",
    "        self.lif_q = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "        self.lif_k = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "        self.lif_v = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "\n",
    "    def forward(self, X, adj):  # X: [B, N, d_in]\n",
    "        q = self.lif_q(self.Wq(X))       # [B,N,d_out]\n",
    "        k = self.lif_k(self.Wk(X))       # [B,N,d_out]\n",
    "        v = self.lif_v(self.Wv(X))       # [B,N,d_out]\n",
    "        co = torch.bmm(q, k.transpose(1,2)) * adj   # [B,N,N]\n",
    "        attn = co / (co.sum(-1, keepdim=True) + 1e-6)\n",
    "        out = torch.bmm(attn, v)         # [B,N,d_out]\n",
    "        return out\n",
    "\n",
    "class SNN_CharNet(nn.Module):\n",
    "    def __init__(self, p: dict, phase: Literal[\"stdp\",\"finetune\"]=\"stdp\"):\n",
    "        super().__init__()\n",
    "        self.p = p; self.phase = phase\n",
    "        beta = p[\"BETA\"]; th = p[\"THRESHOLD\"]; sgrad = surrogate.atan()\n",
    "\n",
    "        C_in = p[\"TRAJECTORY_CHANNELS\"]; C = p[\"CONV_CHANNELS\"]\n",
    "        H, W = p[\"MAZE_HEIGHT\"], p[\"MAZE_WIDTH\"]\n",
    "        self.conv1 = nn.Conv2d(C_in, C, 3, padding=1, bias=False)\n",
    "        self.lif1  = snn.Leaky(beta=beta, threshold=th, spike_grad=sgrad, learn_beta=True, learn_threshold=True)\n",
    "        self.conv2 = nn.Conv2d(C, C, 3, padding=1, bias=False)\n",
    "        self.lif2  = snn.Leaky(beta=beta, threshold=th, spike_grad=sgrad, learn_beta=True, learn_threshold=True)\n",
    "        self.pool  = nn.AvgPool2d(kernel_size=(H, W))      # → [B,C,1,1]\n",
    "\n",
    "        self.slstm = snn.SLSTM(input_size=C, hidden_size=p[\"LSTM_HIDDEN_SIZE\"], spike_grad=sgrad)\n",
    "        self.fc    = nn.Linear(p[\"LSTM_HIDDEN_SIZE\"], p[\"LENGTH_E_CHAR\"], bias=False)\n",
    "        self.lif_o = snn.Leaky(beta=beta, threshold=th, spike_grad=sgrad, learn_beta=True, learn_threshold=True)\n",
    "\n",
    "        if phase == \"stdp\":\n",
    "            for p_ in list(self.slstm.parameters()) + list(self.fc.parameters()) + list(self.lif_o.parameters()):\n",
    "                p_.requires_grad_(False)\n",
    "        else:\n",
    "            for p_ in list(self.conv1.parameters()) + list(self.lif1.parameters()) + \\\n",
    "                      list(self.conv2.parameters()) + list(self.lif2.parameters()):\n",
    "                p_.requires_grad_(False)\n",
    "\n",
    "    def forward(self, spike_seq, t_lens=None):  # spike_seq: [T_max,B,3,H,W]\n",
    "        T, B, _, _, _ = spike_seq.shape\n",
    "        mem1 = mem2 = None\n",
    "        syn = mem = None\n",
    "        spk_hist = [] \n",
    "\n",
    "        for t in range(T):\n",
    "            x = spike_seq[t]\n",
    "            spk1, mem1 = self.lif1(self.conv1(x), mem1)\n",
    "            spk2, mem2 = self.lif2(self.conv2(spk1), mem2)\n",
    "            pooled = self.pool(spk2).flatten(1)             # [B,C]\n",
    "            spk_l, syn, mem = self.slstm(pooled, syn, mem)  # [B,H]\n",
    "            spk_hist.append(spk_l)\n",
    "\n",
    "        spk_hist = torch.stack(spk_hist, dim=0)             # [T,B,H]\n",
    "        if t_lens is not None:\n",
    "            # 取各自最後有效步（t_lens-1）\n",
    "            idx = (t_lens - 1).clamp_min(0)                 # [B]\n",
    "            # 轉成 [B,T,H] 再 gather\n",
    "            spk_bt = spk_hist.permute(1, 0, 2)              # [B,T,H]\n",
    "            idx_exp = idx.view(B, 1, 1).expand(B, 1, spk_bt.size(2))\n",
    "            spk_last = spk_bt.gather(1, idx_exp).squeeze(1) # [B,H]\n",
    "        else:\n",
    "            spk_last = spk_hist[-1]                         # 沒提供就取最後一帧\n",
    "\n",
    "        spk_o, _ = self.lif_o(self.fc(spk_last))            # [B,E]\n",
    "        return spk_o\n",
    "\n",
    "class SNN_PredNetChoiceScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    對「單一 choice」打分：輸入 query(3) + choice(3) + e_char_tile(E)\n",
    "    → Conv-LIF×2 → SGA×2 → Readout-Leaky → RLeaky（NUM_CLASSES=1）→ scalar logit\n",
    "    \"\"\"\n",
    "    def __init__(self, p: dict):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        beta = p[\"BETA\"]; sgrad = surrogate.fast_sigmoid(slope=25)\n",
    "        H, W = p[\"MAZE_HEIGHT\"], p[\"MAZE_WIDTH\"]\n",
    "        C_in = p[\"QUERY_STATE_CHANNELS\"] + p[\"CHOICE_CHANNELS\"] + p[\"LENGTH_E_CHAR\"]\n",
    "        C = p[\"CONV_CHANNELS\"]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(C_in, C, 3, padding=1, bias=False)\n",
    "        self.lif1  = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "        self.conv2 = nn.Conv2d(C, C, 3, padding=1, bias=False)\n",
    "        self.lif2  = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "\n",
    "        self.sga1  = SGA(C, 2*C, beta=beta)\n",
    "        self.sga2  = SGA(2*C, C, beta=beta)\n",
    "\n",
    "        self.read  = snn.Leaky(beta=beta, spike_grad=sgrad, init_hidden=True)\n",
    "        self.proj  = nn.Linear(C, 1, bias=False)      # → 單一類別\n",
    "        self.rlif  = snn.RLeaky(beta=beta, linear_features=1, spike_grad=sgrad, init_hidden=True)\n",
    "\n",
    "        self.register_buffer(\"ADJ\", grid_adj(H, W))\n",
    "\n",
    "    def reset(self): snn_utils.reset(self)\n",
    "\n",
    "    def forward(self, e_char_spk, query_spk, choice_map):\n",
    "        \"\"\"\n",
    "        e_char_spk: [B,E]，query_spk: [B,3,H,W]，choice_map: [B,3,H,W]\n",
    "        回傳 scalar logits: [B]\n",
    "        \"\"\"\n",
    "        B, _, H, W = query_spk.shape\n",
    "        e_tile = e_char_spk.unsqueeze(-1).unsqueeze(-1).expand(B, -1, H, W)  # [B,E,H,W]\n",
    "        x = torch.cat([query_spk, choice_map, e_tile], dim=1)                # [B,3+3+E,H,W]\n",
    "\n",
    "        spk1 = self.lif1(self.conv1(x))\n",
    "        spk2 = self.lif2(self.conv2(spk1))                                   # [B,C,H,W]\n",
    "\n",
    "        nodes = spk2.flatten(2).permute(0,2,1)                               # [B,N,C], N=H*W\n",
    "        out1 = self.sga1(nodes, self.ADJ)\n",
    "        out2 = self.sga2(out1,  self.ADJ)\n",
    "\n",
    "        g = out2.mean(dim=1)                                                 # [B,C]\n",
    "        g_spk = self.read(g)                                                 # [B,C]\n",
    "        I_bias = self.proj(g_spk)                                            # [B,1]\n",
    "        # 吸引子迭代幾步（小常數就夠）\n",
    "        for _ in range(self.p.get(\"ATTR_STEPS\", 6)):\n",
    "            _ = self.rlif(I_bias)\n",
    "        return self.rlif.mem.squeeze(-1)                                     # [B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "# from utils_encoding import prefix_to_spike_seq, last_step_query, choices_to_maps\n",
    "# from snn_blocks import SNN_CharNet, SNN_PredNetChoiceScorer\n",
    "from snntorch import utils as snn_utils\n",
    "\n",
    "class ToMNet2SNN(nn.Module):\n",
    "    def __init__(self, cfg, snn_params: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        defaults = dict(\n",
    "            MAZE_HEIGHT=24, MAZE_WIDTH=24,\n",
    "            TRAJECTORY_CHANNELS=3, QUERY_STATE_CHANNELS=3, CHOICE_CHANNELS=3,\n",
    "            LENGTH_E_CHAR=cfg.e_char_dim,\n",
    "            CONV_CHANNELS=64,       \n",
    "            LSTM_HIDDEN_SIZE=128,   \n",
    "            BETA=0.95, THRESHOLD=1.0,\n",
    "            ATTR_STEPS=6,\n",
    "        )\n",
    "        self.p = {**defaults, **(snn_params or {})}\n",
    "\n",
    "        self.charnet = SNN_CharNet(self.p, phase=\"finetune\")\n",
    "        self.scorer  = SNN_PredNetChoiceScorer(self.p)\n",
    "\n",
    "    def _encode_query_choice(self, prefix_ids, attn_mask, choices_ids):\n",
    "        q = last_step_query(prefix_ids, attn_mask, self.p[\"MAZE_HEIGHT\"], self.p[\"MAZE_WIDTH\"])     # [B,3,H,W]\n",
    "        cm = choices_to_maps(choices_ids, self.p[\"MAZE_HEIGHT\"], self.p[\"MAZE_WIDTH\"])              # [B,4,3,H,W]\n",
    "        return q, cm\n",
    "\n",
    "    def forward(self, prefix_ids, attn_mask, choices_ids):\n",
    "        device = prefix_ids.device\n",
    "        # 1) prefix → spike seq（CharNet）\n",
    "        spike_seq, t_lens = prefix_to_spike_seq_batched(\n",
    "            prefix_ids, attn_mask,\n",
    "            pad_id=self.cfg.pad_id, cls_id=self.cfg.cls_id,\n",
    "            H=self.p[\"MAZE_HEIGHT\"], W=self.p[\"MAZE_WIDTH\"]\n",
    "        )  # [T_max,B,3,H,W], [B]\n",
    "\n",
    "        e_char_spk = self.charnet(spike_seq, t_lens=t_lens)                                                # [B,E]\n",
    "        # 2) query/choices → logits\n",
    "        q, cm = self._encode_query_choice(prefix_ids, attn_mask, choices_ids)                       # q:[B,3,H,W], cm:[B,4,3,H,W]\n",
    "        B = prefix_ids.size(0)\n",
    "        # 將四個 choice 在 batch 維度展開，平行算分數\n",
    "        q_rep   = q.repeat_interleave(4, dim=0)                                                      # [4B,3,H,W]\n",
    "        e_rep   = e_char_spk.repeat_interleave(4, dim=0)                                             # [4B,E]\n",
    "        c_flat  = cm.view(B*4, 3, self.p[\"MAZE_HEIGHT\"], self.p[\"MAZE_WIDTH\"])                       # [4B,3,H,W]\n",
    "        # reset scorer 的狀態（避免跨 batch 狀態污染）\n",
    "        self.scorer.reset()\n",
    "        logits_flat = self.scorer(e_rep, q_rep, c_flat)                                              # [4B]\n",
    "        logits = logits_flat.view(B, 4)                                                              # [B,4]\n",
    "        return logits\n",
    "\n",
    "# ---- 訓練流程重點 ----\n",
    "def train_epoch_snn(model, loader, device, optim, spike_reg_lambda=1e-4):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch in loader:\n",
    "        x = {k:(v.to(device) if torch.is_tensor(v) else v) for k,v in batch.items()}\n",
    "        # snn 模組可能保留隱狀態，保險起見每批 reset\n",
    "        snn_utils.reset(model)\n",
    "        logits = model(x[\"prefix_ids\"], x[\"attn_mask\"], x[\"choices_ids\"])   # [B,4]\n",
    "        loss_ce = F.cross_entropy(logits, x[\"labels\"])\n",
    "        # 可選：脈衝稀疏正則（以 logits 的 L2 或在 SNN 模組內記錄總 spikes 做 L1）\n",
    "        loss = loss_ce\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == x[\"labels\"]).sum().item()\n",
    "        total += logits.size(0)\n",
    "        loss_sum += loss.item() * logits.size(0)\n",
    "    return correct/max(total,1), loss_sum/max(total,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_snn(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch in loader:\n",
    "        x = {k:(v.to(device) if torch.is_tensor(v) else v) for k,v in batch.items()}\n",
    "        snn_utils.reset(model)\n",
    "        logits = model(x[\"prefix_ids\"], x[\"attn_mask\"], x[\"choices_ids\"])\n",
    "        loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == x[\"labels\"]).sum().item()\n",
    "        total += logits.size(0)\n",
    "        loss_sum += loss.item() * logits.size(0)\n",
    "    return correct/max(total,1), loss_sum/max(total,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45862431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "_MODE_ALIASES = {\n",
    "    \"rulemap\": \"rulemap\", \"random\": \"random\", \"logic\": \"logic\",\n",
    "    \"intermediate_case1\": \"intermediate_case1\", \"intermediate_case2\": \"intermediate_case2\",\n",
    "    # 常見拼法\n",
    "    \"intermediate_1\": \"intermediate_case1\", \"intermediate_2\": \"intermediate_case2\",\n",
    "    \"intemediete_1\": \"intermediate_case1\", \"intemediete_2\": \"intermediate_case2\",\n",
    "    \"intermediat_case1\": \"intermediate_case1\", \"intermediat_case2\": \"intermediate_case2\",\n",
    "    \"all\": \"all\",\n",
    "}\n",
    "_SPLIT_DIR = {\"train\":\"training_data\",\"val\":\"validation_data\",\"test\":\"testing_data\"}\n",
    "_ALL_MODES = [\"rulemap\",\"random\",\"logic\",\"intermediate_case1\",\"intermediate_case2\"]\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    data_root: Path                \n",
    "    sim_name: str = \"sim1\"\n",
    "    mode: str = \"all\"              \n",
    "    pad_id: int = 576              \n",
    "    cls_id: int = 577              \n",
    "    add_cls: bool = True\n",
    "\n",
    "    # model\n",
    "    vocab_size: int = 578          \n",
    "    d_model: int = 128\n",
    "    nhead: int = 8\n",
    "    n_layers: int = 4\n",
    "    dim_ff: int = 1024\n",
    "    dropout: float = 0.4\n",
    "    e_char_dim: int = 64           \n",
    "\n",
    "    # train\n",
    "    batch_size: int = 64\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    max_epochs: int = 30\n",
    "    seed: int = 42\n",
    "    num_workers: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def canonical_mode(self) -> str:\n",
    "        key = self.mode.strip().lower()\n",
    "        if key not in _MODE_ALIASES:\n",
    "            raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "        return _MODE_ALIASES[key]\n",
    "    \n",
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"/Users/Jer_ry/Desktop/scripts\"), ###\n",
    "    sim_name=\"sim1\",\n",
    "    mode=\"logic\",                 #  'rulemap' / 'random' / 'logic' / 'intermediate_case1' / 'intermediate_case2' / 'all'\n",
    "    batch_size=64,\n",
    "    max_epochs=30,\n",
    "    d_model=256, nhead=8, n_layers=4, dim_ff=1024,\n",
    "    e_char_dim=64,               \n",
    "    add_cls=True, pad_id=576, cls_id=577,\n",
    ")\n",
    "\n",
    "print(\"device:\", cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b60bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_params = dict(\n",
    "    CONV_CHANNELS=64,        \n",
    "    LSTM_HIDDEN_SIZE=128,    \n",
    "    BETA=0.95, THRESHOLD=1.0\n",
    "    ATTR_STEPS=6\n",
    ")\n",
    "model = ToMNet2SNN(cfg, snn_params).to(cfg.device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26319e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2) 準備資料載入器（用你現有的 build_loaders）----\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import functools\n",
    "from functools import partial\n",
    "import json, math, random\n",
    "\n",
    "def csv_paths(cfg: TrainConfig, split: str, mode: str) -> Tuple[Path, Path]:\n",
    "    base = cfg.data_root / _SPLIT_DIR[split] / cfg.sim_name / \"csv\"\n",
    "    return base / f\"{mode}_train.csv\", base / f\"{mode}_answers.csv\"\n",
    "\n",
    "def build_loaders(cfg: TrainConfig):\n",
    "    def _mk(split):\n",
    "        ds = MazeMCQDataset.load(cfg, split)\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=(split == \"train\"),\n",
    "            num_workers=0,  \n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(collate_maze, pad_id=cfg.pad_id)  \n",
    "        )\n",
    "    return _mk(\"train\"), _mk(\"val\"), _mk(\"test\")\n",
    "\n",
    "class MazeMCQDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, pad_id: int, cls_id: int, add_cls: bool):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.pad_id, self.cls_id, self.add_cls = pad_id, cls_id, add_cls\n",
    "        self.letter2idx = {\"A\":0,\"B\":1,\"C\":2,\"D\":3}\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_one(cfg: TrainConfig, split: str, mode: str) -> \"MazeMCQDataset\":\n",
    "        t_csv, a_csv = csv_paths(cfg, split, mode)\n",
    "        if not t_csv.exists() or not a_csv.exists():\n",
    "            raise FileNotFoundError(f\"Missing: {t_csv} / {a_csv}\")\n",
    "        t = pd.read_csv(t_csv)\n",
    "        a = pd.read_csv(a_csv)\n",
    "        common = sorted(set(t[\"trial_id\"]).intersection(set(a[\"trial_id\"])))\n",
    "        df = t[t[\"trial_id\"].isin(common)].merge(a[a[\"trial_id\"].isin(common)], on=[\"trial_id\",\"mode\"], how=\"inner\")\n",
    "        return MazeMCQDataset(df, cfg.pad_id, cfg.cls_id, cfg.add_cls)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(cfg: TrainConfig, split: str) -> \"MazeMCQDataset\":\n",
    "        cmode = cfg.canonical_mode()\n",
    "        if cmode != \"all\":\n",
    "            return MazeMCQDataset._load_one(cfg, split, cmode)\n",
    "        parts = []\n",
    "        for m in _ALL_MODES:\n",
    "            ds_m = MazeMCQDataset._load_one(cfg, split, m)\n",
    "            parts.append(ds_m.df)\n",
    "        df_all = pd.concat(parts, axis=0, ignore_index=True)\n",
    "        return MazeMCQDataset(df_all, cfg.pad_id, cfg.cls_id, cfg.add_cls)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _parse_cells(self, s: str) -> List[int]:\n",
    "        arr = json.loads(s)\n",
    "        assert all(isinstance(x,int) and 0<=x<=575 for x in arr), \"cid out of range\"\n",
    "        return arr\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict:\n",
    "        r = self.df.iloc[i]\n",
    "        # prefix\n",
    "        prefix = self._parse_cells(r[\"cell_json\"])\n",
    "        if self.add_cls:\n",
    "            prefix = [self.cls_id] + prefix\n",
    "\n",
    "        # choices\n",
    "        ch = []\n",
    "        for c in \"ABCD\":\n",
    "            vec = self._parse_cells(r[f\"choice{c}_cell\"])\n",
    "            assert len(vec) == 3\n",
    "            ch.append(torch.tensor(vec, dtype=torch.long))\n",
    "        choices = torch.stack(ch, dim=0)  # [4,3]\n",
    "\n",
    "        label = torch.tensor({\"A\":0,\"B\":1,\"C\":2,\"D\":3}[r[\"correct\"]], dtype=torch.long)\n",
    "        meta = {\n",
    "            \"trial_id\": r[\"trial_id\"], \"mode\": r[\"mode\"],\n",
    "            \"seq_len_T\": int(r[\"seq_len_T\"]),\n",
    "            \"case_type\": \"\" if pd.isna(r.get(\"case_type\",\"\")) else str(r.get(\"case_type\",\"\")),\n",
    "        }\n",
    "        return {\n",
    "            \"prefix_ids\": torch.tensor(prefix, dtype=torch.long),  \n",
    "            \"choices_ids\": choices,                                \n",
    "            \"label\": label, \"meta\": meta,\n",
    "        }\n",
    "    \n",
    "def collate_maze(batch: List[Dict], pad_id: int) -> Dict[str, torch.Tensor]:\n",
    "    B = len(batch)\n",
    "    lengths = [len(b[\"prefix_ids\"]) for b in batch]\n",
    "    L = max(lengths)\n",
    "    prefix = torch.full((B,L), pad_id, dtype=torch.long)\n",
    "    mask   = torch.zeros((B,L), dtype=torch.bool)  # True=keep\n",
    "    for i,b in enumerate(batch):\n",
    "        li = len(b[\"prefix_ids\"])\n",
    "        prefix[i,:li] = b[\"prefix_ids\"]\n",
    "        mask[i,:li] = True\n",
    "    choices = torch.stack([b[\"choices_ids\"] for b in batch], dim=0)  # [B,4,3]\n",
    "    labels  = torch.stack([b[\"label\"] for b in batch], dim=0)\n",
    "    meta = {k:[b[\"meta\"][k] for b in batch] for k in batch[0][\"meta\"].keys()}\n",
    "    return {\"prefix_ids\":prefix, \"attn_mask\":mask, \"choices_ids\":choices, \"labels\":labels, \"meta\":meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e4e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] train acc=0.264 loss=1.386 | val acc=0.208 loss=1.386\n",
      "[Epoch 02] train acc=0.264 loss=1.386 | val acc=0.208 loss=1.386\n",
      "[Epoch 03] train acc=0.264 loss=1.386 | val acc=0.208 loss=1.386\n",
      "[TEST] acc=0.234 loss=1.386\n"
     ]
    }
   ],
   "source": [
    "for p_ in list(model.charnet.conv1.parameters()) + list(model.charnet.lif1.parameters()) + \\\n",
    "          list(model.charnet.conv2.parameters()) + list(model.charnet.lif2.parameters()):\n",
    "    p_.requires_grad_(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = build_loaders(cfg)\n",
    "\n",
    "EPOCHS = 3\n",
    "best_va, best_state = 0.0, None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_acc, tr_loss = train_epoch_snn(model, train_loader, cfg.device, optim)\n",
    "    va_acc, va_loss = eval_snn(model, val_loader, cfg.device)\n",
    "    print(f\"[Epoch {ep:02d}] train acc={tr_acc:.3f} loss={tr_loss:.3f} | val acc={va_acc:.3f} loss={va_loss:.3f}\")\n",
    "    if va_acc > best_va:\n",
    "        best_va, best_state = va_acc, {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "te_acc, te_loss = eval_snn(model, test_loader, cfg.device)\n",
    "print(f\"[TEST] acc={te_acc:.3f} loss={te_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ec991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee58223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
