{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c00e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# ============================================================\n",
    "# Imports & Constants\n",
    "# ============================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from grid_dataset import GridSeqDataset\n",
    "from collate_grid import collate_grid\n",
    "\n",
    "try:\n",
    "    import snntorch as snn\n",
    "    from snntorch import surrogate\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"    pip install snntorch\\n\"\n",
    "    ) from e\n",
    "\n",
    "# === (CHANGE IT) sim_name ===\n",
    "PROJECT_ROOT = Path(\"project_root/scripts\")\n",
    "SIM_NAME = \"5_24\"\n",
    "\n",
    "_ALL_MODES = [\"logic\", \"rulemap\", \"random\", \"intermediate_case1\", \"intermediate_case2\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainConfigSNN:\n",
    "    \"\"\"\n",
    "    SNN training configuration\n",
    "    data_root: project root directory (contains data/)\n",
    "    sim_name:  corresponds to data/<sim_name>/ directory\n",
    "    mode:      'all' or a single mode (e.g., 'random')\n",
    "    \"\"\"\n",
    "    data_root: Path = PROJECT_ROOT\n",
    "    sim_name: str = SIM_NAME\n",
    "    mode: str = \"all\"    # or a single mode (e.g., 'random')\n",
    "    # model\n",
    "    hidden_dim: int = 256\n",
    "    e_char_dim: int = 64\n",
    "    d_model: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # train\n",
    "    batch_size: int = 32\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    max_epochs: int = 20\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    save_dir: Path = Path(\"../checkpoints\")\n",
    "\n",
    "    # transfer\n",
    "    # e.g., pretrain_mode='random' means pretrain on random mode, then fine-tune on other modes\n",
    "    pretrain_mode: Optional[str] = None\n",
    "    freeze_encoder_on_ft: bool = True    # Freeze front-end conv+LIF during fine-tuning, only train head\n",
    "\n",
    "    # checkpoint save location\n",
    "    save_dir: Path = PROJECT_ROOT / \"checkpoints_snn\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Spiking Version:  Character Net + Prediction Head\n",
    "# ============================================================\n",
    "\n",
    "class SpikingCharNet(nn.Module):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      grid_seq: [B,T,H,W,C]\n",
    "      tmask:    [B,T]  True=valid time step\n",
    "\n",
    "    per-frame:\n",
    "      Conv2d( stride=2 ) x2:\n",
    "        H0×W0 → H1×W1 → H2×W2\n",
    "        Here, use a formula to automatically infer H2=W2 from input_hw, supporting different sizes like 12x12, 24x24, etc.\n",
    "\n",
    "    temporal:\n",
    "      One layer of Leaky-Integrate-and-Fire (LIF) accumulates spikes\n",
    "      Use time-avg spike rate as representation -> map to e_char_dim\n",
    "    output:\n",
    "      e_char: [B, e_char_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, hidden_dim: int = 256,\n",
    "                 e_char_dim: int = 64, input_hw: int = 24):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Spatial encoding: two stride=2 conv layers\n",
    "        self.conv1 = nn.Conv2d(in_ch, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Infer conv2 output H2=W2 from input_hw\n",
    "        def _out_size(size, k=3, s=2, p=1):\n",
    "            return (size + 2 * p - k) // s + 1\n",
    "\n",
    "        h1 = _out_size(input_hw)   # after first conv\n",
    "        h2 = _out_size(h1)         # after second conv\n",
    "        self.h2 = h2\n",
    "        self.flatten_dim = 64 * h2 * h2\n",
    "\n",
    "        # flatten → hidden_dim\n",
    "        self.fc = nn.Linear(self.flatten_dim, hidden_dim)\n",
    "\n",
    "        # Temporal SNN: one layer of LIF hidden\n",
    "        self.lif_hidden = snn.Leaky(beta=0.9, spike_grad=surrogate.atan())\n",
    "\n",
    "        # time-avg spike-rate → e_char\n",
    "        self.to_e = nn.Linear(hidden_dim, e_char_dim)\n",
    "        self.ln = nn.LayerNorm(e_char_dim)\n",
    "\n",
    "    def forward(self, grid_seq: torch.Tensor, tmask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        grid_seq: [B,T,H,W,C]\n",
    "        tmask:    [B,T] (bool) True=valid time step\n",
    "        return:   e_char [B, e_char_dim]\n",
    "        \"\"\"\n",
    "        B, T, H, W, C = grid_seq.shape\n",
    "\n",
    "        # Convert to conv2d format [B,T,C,H,W]\n",
    "        x = grid_seq.permute(0, 1, 4, 2, 3).contiguous()\n",
    "\n",
    "        # Initialize LIF state and spike accumulation\n",
    "        mem_h = torch.zeros(B, self.fc.out_features, device=grid_seq.device)\n",
    "        spike_sum = torch.zeros(B, self.fc.out_features, device=grid_seq.device)\n",
    "\n",
    "        # Valid length for time-average\n",
    "        valid_len = (\n",
    "            tmask.sum(dim=1)            # [B]\n",
    "            .clamp_min(1)\n",
    "            .to(grid_seq.device)\n",
    "            .unsqueeze(1)               # [B,1]\n",
    "            .float()\n",
    "        )\n",
    "\n",
    "        for t in range(T):\n",
    "            # Take the frame at time step t [B,C,H,W]\n",
    "            xt = x[:, t]\n",
    "            m = tmask[:, t].float().view(B, 1, 1, 1)  # [B,1,1,1]\n",
    "            # Zero out PAD steps (tmask=False) to avoid contaminating membrane potential\n",
    "            xt = xt * m\n",
    "\n",
    "            # Spatial conv\n",
    "            h = F.relu(self.conv1(xt))\n",
    "            h = F.relu(self.conv2(h))\n",
    "            h = h.view(B, -1)          # [B, flatten_dim]\n",
    "            # Here flatten_dim equals self.flatten_dim\n",
    "            h = F.relu(self.fc(h))     # [B, hidden_dim]\n",
    "\n",
    "            # Temporal LIF (hidden spikes)\n",
    "            spk_h, mem_h = self.lif_hidden(h, mem_h)  # [B, hidden_dim], [B, hidden_dim]\n",
    "\n",
    "            # Accumulate spikes only at valid steps\n",
    "            spike_sum = spike_sum + spk_h * m.view(B, 1)\n",
    "\n",
    "        # time-average spike rate\n",
    "        rate = spike_sum / valid_len                 # [B, hidden_dim]\n",
    "        e_char = self.to_e(rate)                     # [B, e_char_dim]\n",
    "        e_char = self.ln(e_char)\n",
    "        return e_char\n",
    "\n",
    "class PredHeadSNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction head：\n",
    "      e_char + choice embeddings → MLP → 4 logits\n",
    "\n",
    "    - choices_ids: [B,4,3] (each choice consists of 3 cell ids)\n",
    "      First do nn.Embedding, then average over the 3 positions\n",
    "    - e_char is linearly projected to d_model, followed by LayerNorm\n",
    "    - For each choice:\n",
    "        feat = concat(pf, ch, |pf-ch|, pf*ch)  → MLP → scalar logit\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        e_char_dim: int,\n",
    "        d_model: int = 256,\n",
    "        cell_vocab: int = 24 * 24,\n",
    "        choice_emb_dim: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj_e = nn.Linear(e_char_dim, d_model)\n",
    "        self.choice_emb = nn.Embedding(cell_vocab, choice_emb_dim)\n",
    "        self.proj_choice = nn.Linear(choice_emb_dim, d_model)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4 * d_model, 2 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2 * d_model, 1),\n",
    "        )\n",
    "\n",
    "    def _embed_choice_triplet(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ids: [B,4,3]\n",
    "        return: [B,4,Dc] averaged choice embeddings\n",
    "        \"\"\"\n",
    "        emb = self.choice_emb(ids)  # [B,4,3,Dc]\n",
    "        return emb.mean(dim=2)      # [B,4,Dc]\n",
    "\n",
    "    def forward(self, e_char: torch.Tensor, choices_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        e_char:      [B, e_char_dim]\n",
    "        choices_ids: [B, 4, 3]\n",
    "        return:      logits [B, 4]\n",
    "        \"\"\"\n",
    "        B = e_char.size(0)\n",
    "\n",
    "        # prefix fused\n",
    "        e = self.proj_e(e_char)   # [B,D]\n",
    "        pf = self.ln(e)           # [B,D]\n",
    "\n",
    "        # choices\n",
    "        ch = self._embed_choice_triplet(choices_ids)  # [B,4,Dc]\n",
    "        ch = self.proj_choice(ch)                     # [B,4,D]\n",
    "\n",
    "        pf_expand = pf.unsqueeze(1).expand_as(ch)     # [B,4,D]\n",
    "        feat = torch.cat(\n",
    "            [pf_expand, ch, torch.abs(pf_expand - ch), pf_expand * ch],\n",
    "            dim=-1,\n",
    "        )  # [B,4,4D]\n",
    "\n",
    "        logits = self.mlp(feat).squeeze(-1)          # [B,4]\n",
    "        return logits\n",
    "\n",
    "class ToMNetSNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Overall SNN version of ToMNet:\n",
    "      - vision: SpikingCharNet → e_char\n",
    "      - head:   PredHeadSNN    → 4 logits\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        hidden_dim: int = 256,\n",
    "        e_char_dim: int = 64,\n",
    "        d_model: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        cell_vocab: int = 24 * 24,\n",
    "        input_hw: int = 24, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision = SpikingCharNet(\n",
    "            in_ch=in_ch,\n",
    "            hidden_dim=hidden_dim,\n",
    "            e_char_dim=e_char_dim,\n",
    "            input_hw=input_hw,\n",
    "        )\n",
    "        self.head = PredHeadSNN(\n",
    "            e_char_dim=e_char_dim,\n",
    "            d_model=d_model,\n",
    "            cell_vocab=cell_vocab,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def num_params(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        grid_seq: torch.Tensor,\n",
    "        tmask: torch.Tensor,\n",
    "        choices_ids: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        e_char = self.vision(grid_seq, tmask)   # [B,e_char_dim]\n",
    "        logits = self.head(e_char, choices_ids) # [B,4]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training Utilities\n",
    "# ============================================================\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_loader(cfg: TrainConfigSNN, split: str, mode: str) -> Tuple[DataLoader, int, int]:\n",
    "    \"\"\"\n",
    "    create DataLoader, and also return\n",
    "      - in_ch(last dimension C)\n",
    "      - input_hw (here use H, assuming H=W)\n",
    "    \"\"\"\n",
    "    ds = GridSeqDataset(cfg.data_root, cfg.sim_name, split, mode)\n",
    "    one = ds[0][\"grid_seq\"]  # [T,H,W,C]\n",
    "    _, H, W, C = one.shape\n",
    "    in_ch = int(C)\n",
    "\n",
    "    ld = DataLoader(\n",
    "        ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=(split == \"train\"),\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_grid,\n",
    "    )\n",
    "    assert H == W, f\"Currently assume inner grid is square, but got H={H}, W={W}\"\n",
    "    return ld, in_ch, H\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate on a given DataLoader:\n",
    "      return: (accuracy, avg_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n, corr, tot_loss = 0, 0, 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = {\n",
    "            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        logits = model(x[\"grid_seq\"], x[\"tmask\"], x[\"choices_ids\"])\n",
    "        loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        corr += (pred == x[\"labels\"]).sum().item()\n",
    "        tot_loss += loss.item() * logits.size(0)\n",
    "        n += logits.size(0)\n",
    "\n",
    "    if n == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return corr / n, tot_loss / n\n",
    "\n",
    "def build_model(in_ch: int, cfg: TrainConfigSNN, input_hw: int) -> ToMNetSNN:\n",
    "    \"\"\"\n",
    "    Build a ToMNetSNN based on in_ch and input_hw.\n",
    "    \"\"\"\n",
    "    model = ToMNetSNN(\n",
    "        in_ch=in_ch,\n",
    "        hidden_dim=cfg.hidden_dim,\n",
    "        e_char_dim=cfg.e_char_dim,\n",
    "        d_model=cfg.d_model,\n",
    "        dropout=cfg.dropout,\n",
    "        cell_vocab=24 * 24,   # vocab 開大一點沒關係，只要 id < vocab\n",
    "        input_hw=input_hw,    # ★ 傳進去\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder(model: ToMNetSNN, flag: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    freeze/unfreeze vision encoder (SpikingCharNet).\n",
    "    \"\"\"\n",
    "    for p in model.vision.parameters():\n",
    "        p.requires_grad = not flag\n",
    "\n",
    "def train_one_mode(\n",
    "    cfg: TrainConfigSNN,\n",
    "    mode: str,\n",
    "    model: Optional[nn.Module] = None,\n",
    "    *,\n",
    "    load_ckpt: Optional[Path] = None,\n",
    "    freeze_encoder_flag: bool = False,\n",
    "):\n",
    "    print(f\"\\n=== [SNN] Training mode = {mode} ===\")\n",
    "    cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Note: now also return input_hw\n",
    "    train_ld, in_ch, input_hw = make_loader(cfg, \"train\", mode)\n",
    "    val_ld, _, _ = make_loader(cfg, \"val\", mode)\n",
    "    test_ld, _, _ = make_loader(cfg, \"test\", mode)\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model(in_ch, cfg, input_hw=input_hw).to(cfg.device)\n",
    "\n",
    "    if load_ckpt is not None:\n",
    "        print(f\"[SNN] Load checkpoint: {load_ckpt}\")\n",
    "        state = torch.load(load_ckpt, map_location=\"cpu\")\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "    if freeze_encoder_flag:\n",
    "        print(\"[SNN] Freeze encoder (vision) for fine-tuning.\")\n",
    "        freeze_encoder(model, True)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "    )\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.max_epochs)\n",
    "\n",
    "    best_va, best_state = 0.0, None\n",
    "\n",
    "    for ep in range(1, cfg.max_epochs + 1):\n",
    "        model.train()\n",
    "        n = corr = 0\n",
    "        tot_loss = 0.0\n",
    "\n",
    "        for batch in train_ld:\n",
    "            x = {\n",
    "                k: (v.to(cfg.device) if isinstance(v, torch.Tensor) else v)\n",
    "                for k, v in batch.items()\n",
    "            }\n",
    "            logits = model(x[\"grid_seq\"], x[\"tmask\"], x[\"choices_ids\"])\n",
    "            loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            corr += (pred == x[\"labels\"]).sum().item()\n",
    "            tot_loss += loss.item() * logits.size(0)\n",
    "            n += logits.size(0)\n",
    "\n",
    "        tr_acc = corr / max(n, 1)\n",
    "        tr_loss = tot_loss / max(n, 1)\n",
    "        va_acc, va_loss = evaluate(model, val_ld, cfg.device)\n",
    "        sch.step()\n",
    "\n",
    "        print(\n",
    "            f\"[{mode}][Ep {ep:02d}] \"\n",
    "            f\"train acc={tr_acc:.3f} loss={tr_loss:.3f} | \"\n",
    "            f\"val acc={va_acc:.3f} loss={va_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "        if va_acc > best_va:\n",
    "            best_va = va_acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    ckpt_path = cfg.save_dir / f\"snn_{cfg.sim_name}_{mode}.pt\"\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "    te_acc, te_loss = evaluate(model, test_ld, cfg.device)\n",
    "    print(\n",
    "        f\"[{mode}][TEST] acc={te_acc:.3f} loss={te_loss:.3f} | \"\n",
    "        f\"saved: {ckpt_path}\"\n",
    "    )\n",
    "    return model, {\"val_acc\": best_va, \"test_acc\": te_acc, \"ckpt\": ckpt_path}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ed0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ALL_MODES = [\"random\", \"rulemap\", \"logic\", \"intermediate_case1\", \"intermediate_case2\"]\n",
    "\n",
    "def _model_name(sim_name: str, mode: str, kind: str = \"ToMNet\") -> str:\n",
    "    # kind 可以填 \"ToMNet\", \"ToMNetSNN\", \"ToMNetConv\"\n",
    "    return f\"{kind}_{sim_name}_{mode}\"\n",
    "\n",
    "def _save_model(model: nn.Module, cfg: TrainConfigSNN, mode: str, kind: str = \"ToMNet\") -> Path:\n",
    "    cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    name = _model_name(cfg.sim_name, mode, kind)\n",
    "    path = cfg.save_dir / f\"{name}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"[SAVE] {name} -> {path}\")\n",
    "    return path\n",
    "\n",
    "def run(cfg: TrainConfigSNN, kind: str = \"ToMNet\") -> Dict[str, nn.Module]:\n",
    "    \"\"\"\n",
    "    kind: \"ToMNet\" / \"ToMNetSNN\" / \"ToMNetConv\", for naming purpose\n",
    "    Return {model_name: model_instance}\n",
    "    \"\"\"\n",
    "    models: Dict[str, nn.Module] = {}\n",
    "\n",
    "    if cfg.mode == \"all\":\n",
    "        modes = _ALL_MODES\n",
    "    else:\n",
    "        modes = [cfg.mode]\n",
    "\n",
    "    for mode in modes:\n",
    "        print(f\"[TRAIN] sim={cfg.sim_name} mode={mode}\")\n",
    "        model, best_acc = train_one_mode(cfg, mode)\n",
    "\n",
    "        name = _model_name(cfg.sim_name, mode, kind)\n",
    "        _save_model(model, cfg, mode, kind=kind)\n",
    "        models[name] = model\n",
    "        print(f\"[DONE] {name} (best_acc={best_acc})\")\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288fd75",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d1089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=random\n",
      "\n",
      "=== [SNN] Training mode = random ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random][Ep 01] train acc=0.307 loss=1.326 | val acc=0.320 loss=1.259\n",
      "[random][Ep 02] train acc=0.385 loss=1.219 | val acc=0.370 loss=1.214\n",
      "[random][Ep 03] train acc=0.403 loss=1.169 | val acc=0.380 loss=1.197\n",
      "[random][Ep 04] train acc=0.438 loss=1.147 | val acc=0.420 loss=1.190\n",
      "[random][Ep 05] train acc=0.439 loss=1.133 | val acc=0.420 loss=1.186\n",
      "[random][Ep 06] train acc=0.455 loss=1.116 | val acc=0.390 loss=1.180\n",
      "[random][Ep 07] train acc=0.459 loss=1.112 | val acc=0.410 loss=1.185\n",
      "[random][Ep 08] train acc=0.470 loss=1.096 | val acc=0.400 loss=1.187\n",
      "[random][Ep 09] train acc=0.494 loss=1.082 | val acc=0.410 loss=1.190\n",
      "[random][Ep 10] train acc=0.515 loss=1.069 | val acc=0.390 loss=1.199\n",
      "[random][Ep 11] train acc=0.509 loss=1.063 | val acc=0.360 loss=1.203\n",
      "[random][Ep 12] train acc=0.507 loss=1.049 | val acc=0.380 loss=1.212\n",
      "[random][Ep 13] train acc=0.529 loss=1.043 | val acc=0.380 loss=1.217\n",
      "[random][Ep 14] train acc=0.526 loss=1.034 | val acc=0.370 loss=1.223\n",
      "[random][Ep 15] train acc=0.530 loss=1.022 | val acc=0.370 loss=1.222\n",
      "[random][Ep 16] train acc=0.540 loss=1.018 | val acc=0.360 loss=1.228\n",
      "[random][Ep 17] train acc=0.542 loss=1.009 | val acc=0.350 loss=1.235\n",
      "[random][Ep 18] train acc=0.566 loss=0.997 | val acc=0.360 loss=1.235\n",
      "[random][Ep 19] train acc=0.571 loss=0.984 | val acc=0.370 loss=1.237\n",
      "[random][Ep 20] train acc=0.571 loss=0.975 | val acc=0.360 loss=1.237\n",
      "[random][Ep 21] train acc=0.584 loss=0.959 | val acc=0.330 loss=1.250\n",
      "[random][Ep 22] train acc=0.604 loss=0.944 | val acc=0.360 loss=1.246\n",
      "[random][Ep 23] train acc=0.605 loss=0.926 | val acc=0.320 loss=1.266\n",
      "[random][Ep 24] train acc=0.630 loss=0.906 | val acc=0.380 loss=1.253\n",
      "[random][Ep 25] train acc=0.650 loss=0.881 | val acc=0.380 loss=1.261\n",
      "[random][Ep 26] train acc=0.659 loss=0.863 | val acc=0.370 loss=1.258\n",
      "[random][Ep 27] train acc=0.691 loss=0.824 | val acc=0.350 loss=1.276\n",
      "[random][Ep 28] train acc=0.721 loss=0.795 | val acc=0.390 loss=1.287\n",
      "[random][Ep 29] train acc=0.730 loss=0.771 | val acc=0.330 loss=1.310\n",
      "[random][Ep 30] train acc=0.756 loss=0.739 | val acc=0.330 loss=1.323\n",
      "[random][Ep 31] train acc=0.786 loss=0.696 | val acc=0.350 loss=1.317\n",
      "[random][Ep 32] train acc=0.787 loss=0.672 | val acc=0.360 loss=1.334\n",
      "[random][Ep 33] train acc=0.796 loss=0.645 | val acc=0.360 loss=1.338\n",
      "[random][Ep 34] train acc=0.830 loss=0.603 | val acc=0.390 loss=1.321\n",
      "[random][Ep 35] train acc=0.830 loss=0.581 | val acc=0.340 loss=1.391\n",
      "[random][Ep 36] train acc=0.853 loss=0.555 | val acc=0.380 loss=1.410\n",
      "[random][Ep 37] train acc=0.851 loss=0.524 | val acc=0.360 loss=1.410\n",
      "[random][Ep 38] train acc=0.882 loss=0.486 | val acc=0.340 loss=1.470\n",
      "[random][Ep 39] train acc=0.877 loss=0.459 | val acc=0.340 loss=1.467\n",
      "[random][Ep 40] train acc=0.891 loss=0.440 | val acc=0.360 loss=1.472\n",
      "[random][Ep 41] train acc=0.911 loss=0.404 | val acc=0.300 loss=1.542\n",
      "[random][Ep 42] train acc=0.911 loss=0.389 | val acc=0.360 loss=1.519\n",
      "[random][Ep 43] train acc=0.917 loss=0.359 | val acc=0.340 loss=1.574\n",
      "[random][Ep 44] train acc=0.917 loss=0.348 | val acc=0.340 loss=1.539\n",
      "[random][Ep 45] train acc=0.935 loss=0.320 | val acc=0.360 loss=1.568\n",
      "[random][Ep 46] train acc=0.939 loss=0.309 | val acc=0.370 loss=1.583\n",
      "[random][Ep 47] train acc=0.956 loss=0.286 | val acc=0.310 loss=1.701\n",
      "[random][Ep 48] train acc=0.963 loss=0.266 | val acc=0.380 loss=1.633\n",
      "[random][Ep 49] train acc=0.959 loss=0.247 | val acc=0.280 loss=1.659\n",
      "[random][Ep 50] train acc=0.959 loss=0.243 | val acc=0.330 loss=1.749\n",
      "[random][Ep 51] train acc=0.965 loss=0.234 | val acc=0.320 loss=1.748\n",
      "[random][Ep 52] train acc=0.981 loss=0.211 | val acc=0.320 loss=1.756\n",
      "[random][Ep 53] train acc=0.979 loss=0.197 | val acc=0.320 loss=1.795\n",
      "[random][Ep 54] train acc=0.971 loss=0.202 | val acc=0.330 loss=1.773\n",
      "[random][Ep 55] train acc=0.978 loss=0.187 | val acc=0.290 loss=1.810\n",
      "[random][Ep 56] train acc=0.985 loss=0.171 | val acc=0.290 loss=1.793\n",
      "[random][Ep 57] train acc=0.980 loss=0.171 | val acc=0.300 loss=1.858\n",
      "[random][Ep 58] train acc=0.986 loss=0.165 | val acc=0.330 loss=1.812\n",
      "[random][Ep 59] train acc=0.988 loss=0.155 | val acc=0.310 loss=1.789\n",
      "[random][Ep 60] train acc=0.986 loss=0.154 | val acc=0.310 loss=1.813\n",
      "[random][Ep 61] train acc=0.990 loss=0.148 | val acc=0.310 loss=1.810\n",
      "[random][Ep 62] train acc=0.989 loss=0.143 | val acc=0.340 loss=1.815\n",
      "[random][Ep 63] train acc=0.993 loss=0.137 | val acc=0.330 loss=1.798\n",
      "[random][Ep 64] train acc=0.996 loss=0.133 | val acc=0.350 loss=1.831\n",
      "[random][Ep 65] train acc=0.991 loss=0.132 | val acc=0.350 loss=1.841\n",
      "[random][Ep 66] train acc=0.994 loss=0.126 | val acc=0.350 loss=1.853\n",
      "[random][Ep 67] train acc=0.989 loss=0.127 | val acc=0.340 loss=1.876\n",
      "[random][Ep 68] train acc=0.995 loss=0.119 | val acc=0.330 loss=1.849\n",
      "[random][Ep 69] train acc=0.990 loss=0.119 | val acc=0.370 loss=1.886\n",
      "[random][Ep 70] train acc=0.991 loss=0.119 | val acc=0.350 loss=1.904\n",
      "[random][Ep 71] train acc=0.994 loss=0.114 | val acc=0.340 loss=1.917\n",
      "[random][Ep 72] train acc=0.993 loss=0.114 | val acc=0.310 loss=1.933\n",
      "[random][Ep 73] train acc=0.996 loss=0.114 | val acc=0.340 loss=1.909\n",
      "[random][Ep 74] train acc=0.996 loss=0.112 | val acc=0.350 loss=1.885\n",
      "[random][Ep 75] train acc=0.998 loss=0.110 | val acc=0.360 loss=1.905\n",
      "[random][Ep 76] train acc=0.994 loss=0.111 | val acc=0.360 loss=1.910\n",
      "[random][Ep 77] train acc=0.995 loss=0.113 | val acc=0.360 loss=1.914\n",
      "[random][Ep 78] train acc=0.993 loss=0.116 | val acc=0.360 loss=1.928\n",
      "[random][Ep 79] train acc=0.994 loss=0.111 | val acc=0.360 loss=1.917\n",
      "[random][Ep 80] train acc=0.995 loss=0.110 | val acc=0.360 loss=1.922\n",
      "[random][TEST] acc=0.380 loss=1.694 | saved: project_root/scripts/checkpoints_snn/snn_3_12_random.pt\n",
      "[SAVE] ToMNetSNN_3_12_random -> project_root/scripts/checkpoints_snn/ToMNetSNN_3_12_random.pt\n",
      "[DONE] ToMNetSNN_3_12_random (best_acc={'val_acc': 0.42, 'test_acc': 0.38, 'ckpt': PosixPath('project_root/scripts/checkpoints_snn/snn_3_12_random.pt')})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfigSNN(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"random\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "\n",
    "models = run(cfg, kind=\"ToMNetSNN\")\n",
    "TomNet_Random_Model = models[\"ToMNetSNN_3_12_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cda14906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=rulemap\n",
      "\n",
      "=== [SNN] Training mode = rulemap ===\n",
      "[rulemap][Ep 01] train acc=0.639 loss=1.151 | val acc=0.780 loss=0.918\n",
      "[rulemap][Ep 02] train acc=0.800 loss=0.762 | val acc=0.820 loss=0.609\n",
      "[rulemap][Ep 03] train acc=0.820 loss=0.501 | val acc=0.840 loss=0.443\n",
      "[rulemap][Ep 04] train acc=0.850 loss=0.369 | val acc=0.880 loss=0.359\n",
      "[rulemap][Ep 05] train acc=0.891 loss=0.297 | val acc=0.920 loss=0.306\n",
      "[rulemap][Ep 06] train acc=0.916 loss=0.239 | val acc=0.940 loss=0.257\n",
      "[rulemap][Ep 07] train acc=0.935 loss=0.199 | val acc=0.940 loss=0.223\n",
      "[rulemap][Ep 08] train acc=0.949 loss=0.168 | val acc=0.950 loss=0.188\n",
      "[rulemap][Ep 09] train acc=0.956 loss=0.146 | val acc=0.950 loss=0.168\n",
      "[rulemap][Ep 10] train acc=0.956 loss=0.127 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 11] train acc=0.965 loss=0.113 | val acc=0.950 loss=0.142\n",
      "[rulemap][Ep 12] train acc=0.970 loss=0.097 | val acc=0.950 loss=0.134\n",
      "[rulemap][Ep 13] train acc=0.973 loss=0.091 | val acc=0.950 loss=0.132\n",
      "[rulemap][Ep 14] train acc=0.969 loss=0.085 | val acc=0.960 loss=0.124\n",
      "[rulemap][Ep 15] train acc=0.981 loss=0.079 | val acc=0.960 loss=0.120\n",
      "[rulemap][Ep 16] train acc=0.975 loss=0.070 | val acc=0.960 loss=0.115\n",
      "[rulemap][Ep 17] train acc=0.980 loss=0.066 | val acc=0.960 loss=0.117\n",
      "[rulemap][Ep 18] train acc=0.985 loss=0.062 | val acc=0.960 loss=0.118\n",
      "[rulemap][Ep 19] train acc=0.986 loss=0.057 | val acc=0.960 loss=0.114\n",
      "[rulemap][Ep 20] train acc=0.988 loss=0.054 | val acc=0.960 loss=0.121\n",
      "[rulemap][Ep 21] train acc=0.986 loss=0.053 | val acc=0.960 loss=0.116\n",
      "[rulemap][Ep 22] train acc=0.985 loss=0.053 | val acc=0.960 loss=0.117\n",
      "[rulemap][Ep 23] train acc=0.985 loss=0.049 | val acc=0.960 loss=0.115\n",
      "[rulemap][Ep 24] train acc=0.984 loss=0.045 | val acc=0.960 loss=0.123\n",
      "[rulemap][Ep 25] train acc=0.990 loss=0.042 | val acc=0.960 loss=0.125\n",
      "[rulemap][Ep 26] train acc=0.990 loss=0.039 | val acc=0.960 loss=0.124\n",
      "[rulemap][Ep 27] train acc=0.993 loss=0.036 | val acc=0.960 loss=0.128\n",
      "[rulemap][Ep 28] train acc=0.989 loss=0.037 | val acc=0.960 loss=0.125\n",
      "[rulemap][Ep 29] train acc=0.990 loss=0.031 | val acc=0.960 loss=0.129\n",
      "[rulemap][Ep 30] train acc=0.994 loss=0.029 | val acc=0.960 loss=0.121\n",
      "[rulemap][Ep 31] train acc=0.990 loss=0.031 | val acc=0.950 loss=0.124\n",
      "[rulemap][Ep 32] train acc=0.994 loss=0.026 | val acc=0.950 loss=0.131\n",
      "[rulemap][Ep 33] train acc=0.994 loss=0.027 | val acc=0.950 loss=0.126\n",
      "[rulemap][Ep 34] train acc=0.996 loss=0.023 | val acc=0.950 loss=0.126\n",
      "[rulemap][Ep 35] train acc=0.999 loss=0.020 | val acc=0.950 loss=0.137\n",
      "[rulemap][Ep 36] train acc=0.994 loss=0.021 | val acc=0.950 loss=0.141\n",
      "[rulemap][Ep 37] train acc=0.995 loss=0.020 | val acc=0.950 loss=0.140\n",
      "[rulemap][Ep 38] train acc=0.993 loss=0.021 | val acc=0.950 loss=0.134\n",
      "[rulemap][Ep 39] train acc=0.999 loss=0.018 | val acc=0.950 loss=0.140\n",
      "[rulemap][Ep 40] train acc=0.996 loss=0.017 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 41] train acc=0.998 loss=0.015 | val acc=0.950 loss=0.155\n",
      "[rulemap][Ep 42] train acc=0.998 loss=0.016 | val acc=0.950 loss=0.144\n",
      "[rulemap][Ep 43] train acc=1.000 loss=0.014 | val acc=0.950 loss=0.148\n",
      "[rulemap][Ep 44] train acc=1.000 loss=0.014 | val acc=0.950 loss=0.154\n",
      "[rulemap][Ep 45] train acc=1.000 loss=0.012 | val acc=0.950 loss=0.150\n",
      "[rulemap][Ep 46] train acc=0.999 loss=0.013 | val acc=0.950 loss=0.149\n",
      "[rulemap][Ep 47] train acc=1.000 loss=0.012 | val acc=0.950 loss=0.147\n",
      "[rulemap][Ep 48] train acc=1.000 loss=0.010 | val acc=0.950 loss=0.151\n",
      "[rulemap][Ep 49] train acc=1.000 loss=0.009 | val acc=0.950 loss=0.156\n",
      "[rulemap][Ep 50] train acc=1.000 loss=0.009 | val acc=0.950 loss=0.163\n",
      "[rulemap][Ep 51] train acc=1.000 loss=0.008 | val acc=0.950 loss=0.140\n",
      "[rulemap][Ep 52] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.146\n",
      "[rulemap][Ep 53] train acc=1.000 loss=0.008 | val acc=0.950 loss=0.157\n",
      "[rulemap][Ep 54] train acc=1.000 loss=0.008 | val acc=0.950 loss=0.153\n",
      "[rulemap][Ep 55] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.150\n",
      "[rulemap][Ep 56] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.160\n",
      "[rulemap][Ep 57] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.159\n",
      "[rulemap][Ep 58] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.156\n",
      "[rulemap][Ep 59] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.153\n",
      "[rulemap][Ep 60] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.154\n",
      "[rulemap][Ep 61] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.148\n",
      "[rulemap][Ep 62] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.157\n",
      "[rulemap][Ep 63] train acc=1.000 loss=0.007 | val acc=0.950 loss=0.153\n",
      "[rulemap][Ep 64] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.151\n",
      "[rulemap][Ep 65] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.155\n",
      "[rulemap][Ep 66] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.153\n",
      "[rulemap][Ep 67] train acc=0.999 loss=0.006 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 68] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 69] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 70] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.153\n",
      "[rulemap][Ep 71] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 72] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 73] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 74] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 75] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 76] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 77] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.152\n",
      "[rulemap][Ep 78] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.156\n",
      "[rulemap][Ep 79] train acc=1.000 loss=0.005 | val acc=0.950 loss=0.156\n",
      "[rulemap][Ep 80] train acc=1.000 loss=0.006 | val acc=0.950 loss=0.156\n",
      "[rulemap][TEST] acc=0.940 loss=0.340 | saved: project_root/scripts/checkpoints_snn/snn_3_12_rulemap.pt\n",
      "[SAVE] ToMNetSNN_3_12_rulemap -> project_root/scripts/checkpoints_snn/ToMNetSNN_3_12_rulemap.pt\n",
      "[DONE] ToMNetSNN_3_12_rulemap (best_acc={'val_acc': 0.96, 'test_acc': 0.94, 'ckpt': PosixPath('project_root/scripts/checkpoints_snn/snn_3_12_rulemap.pt')})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfigSNN(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"rulemap\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNetSNN\") \n",
    "TomNet_Rulemap_Model = models[\"ToMNetSNN_3_12_rulemap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25dd0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=logic\n",
      "\n",
      "=== [SNN] Training mode = logic ===\n",
      "[logic][Ep 01] train acc=0.601 loss=1.192 | val acc=0.770 loss=0.954\n",
      "[logic][Ep 02] train acc=0.756 loss=0.806 | val acc=0.780 loss=0.631\n",
      "[logic][Ep 03] train acc=0.806 loss=0.541 | val acc=0.790 loss=0.447\n",
      "[logic][Ep 04] train acc=0.835 loss=0.402 | val acc=0.830 loss=0.369\n",
      "[logic][Ep 05] train acc=0.864 loss=0.333 | val acc=0.840 loss=0.327\n",
      "[logic][Ep 06] train acc=0.882 loss=0.285 | val acc=0.880 loss=0.295\n",
      "[logic][Ep 07] train acc=0.897 loss=0.248 | val acc=0.900 loss=0.267\n",
      "[logic][Ep 08] train acc=0.914 loss=0.220 | val acc=0.920 loss=0.239\n",
      "[logic][Ep 09] train acc=0.938 loss=0.192 | val acc=0.930 loss=0.212\n",
      "[logic][Ep 10] train acc=0.960 loss=0.167 | val acc=0.960 loss=0.185\n",
      "[logic][Ep 11] train acc=0.974 loss=0.145 | val acc=0.960 loss=0.166\n",
      "[logic][Ep 12] train acc=0.975 loss=0.128 | val acc=0.970 loss=0.150\n",
      "[logic][Ep 13] train acc=0.979 loss=0.109 | val acc=0.970 loss=0.130\n",
      "[logic][Ep 14] train acc=0.985 loss=0.097 | val acc=0.980 loss=0.119\n",
      "[logic][Ep 15] train acc=0.988 loss=0.086 | val acc=0.980 loss=0.110\n",
      "[logic][Ep 16] train acc=0.988 loss=0.078 | val acc=0.980 loss=0.106\n",
      "[logic][Ep 17] train acc=0.988 loss=0.070 | val acc=0.980 loss=0.098\n",
      "[logic][Ep 18] train acc=0.990 loss=0.061 | val acc=0.980 loss=0.095\n",
      "[logic][Ep 19] train acc=0.990 loss=0.056 | val acc=0.980 loss=0.089\n",
      "[logic][Ep 20] train acc=0.994 loss=0.048 | val acc=0.980 loss=0.086\n",
      "[logic][Ep 21] train acc=0.996 loss=0.045 | val acc=0.980 loss=0.087\n",
      "[logic][Ep 22] train acc=0.993 loss=0.044 | val acc=0.980 loss=0.082\n",
      "[logic][Ep 23] train acc=0.995 loss=0.038 | val acc=0.980 loss=0.081\n",
      "[logic][Ep 24] train acc=0.994 loss=0.036 | val acc=0.980 loss=0.074\n",
      "[logic][Ep 25] train acc=0.994 loss=0.031 | val acc=0.980 loss=0.070\n",
      "[logic][Ep 26] train acc=0.996 loss=0.028 | val acc=0.980 loss=0.072\n",
      "[logic][Ep 27] train acc=0.995 loss=0.026 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 28] train acc=0.998 loss=0.023 | val acc=0.980 loss=0.062\n",
      "[logic][Ep 29] train acc=1.000 loss=0.020 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 30] train acc=0.999 loss=0.018 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 31] train acc=1.000 loss=0.017 | val acc=0.980 loss=0.060\n",
      "[logic][Ep 32] train acc=1.000 loss=0.014 | val acc=0.980 loss=0.059\n",
      "[logic][Ep 33] train acc=0.999 loss=0.014 | val acc=0.980 loss=0.061\n",
      "[logic][Ep 34] train acc=1.000 loss=0.012 | val acc=0.980 loss=0.061\n",
      "[logic][Ep 35] train acc=1.000 loss=0.011 | val acc=0.980 loss=0.059\n",
      "[logic][Ep 36] train acc=1.000 loss=0.012 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 37] train acc=1.000 loss=0.010 | val acc=0.980 loss=0.056\n",
      "[logic][Ep 38] train acc=1.000 loss=0.009 | val acc=0.980 loss=0.056\n",
      "[logic][Ep 39] train acc=0.999 loss=0.009 | val acc=0.980 loss=0.058\n",
      "[logic][Ep 40] train acc=1.000 loss=0.008 | val acc=0.980 loss=0.058\n",
      "[logic][Ep 41] train acc=1.000 loss=0.007 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 42] train acc=1.000 loss=0.007 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 43] train acc=1.000 loss=0.007 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 44] train acc=1.000 loss=0.007 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 45] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 46] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.058\n",
      "[logic][Ep 47] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.054\n",
      "[logic][Ep 48] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 49] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 50] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 51] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 52] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.056\n",
      "[logic][Ep 53] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.056\n",
      "[logic][Ep 54] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.054\n",
      "[logic][Ep 55] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.054\n",
      "[logic][Ep 56] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 57] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 58] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.058\n",
      "[logic][Ep 59] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.056\n",
      "[logic][Ep 60] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.057\n",
      "[logic][Ep 61] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.065\n",
      "[logic][Ep 62] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.065\n",
      "[logic][Ep 63] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.065\n",
      "[logic][Ep 64] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.064\n",
      "[logic][Ep 65] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.064\n",
      "[logic][Ep 66] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.062\n",
      "[logic][Ep 67] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 68] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 69] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 70] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 71] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 72] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 73] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 74] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 75] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 76] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 77] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 78] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 79] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][Ep 80] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.064\n",
      "[logic][TEST] acc=0.960 loss=0.087 | saved: project_root/scripts/checkpoints_snn/snn_3_12_logic.pt\n",
      "[SAVE] ToMNetSNN_3_12_logic -> project_root/scripts/checkpoints_snn/ToMNetSNN_3_12_logic.pt\n",
      "[DONE] ToMNetSNN_3_12_logic (best_acc={'val_acc': 0.98, 'test_acc': 0.96, 'ckpt': PosixPath('project_root/scripts/checkpoints_snn/snn_3_12_logic.pt')})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfigSNN(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"logic\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNetSNN\") \n",
    "TomNet_Logic_Model = models[\"ToMNetSNN_3_12_logic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90de34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case1\n",
      "\n",
      "=== [SNN] Training mode = intermediate_case1 ===\n",
      "[intermediate_case1][Ep 01] train acc=0.319 loss=1.334 | val acc=0.410 loss=1.268\n",
      "[intermediate_case1][Ep 02] train acc=0.364 loss=1.221 | val acc=0.430 loss=1.217\n",
      "[intermediate_case1][Ep 03] train acc=0.360 loss=1.180 | val acc=0.410 loss=1.199\n",
      "[intermediate_case1][Ep 04] train acc=0.414 loss=1.151 | val acc=0.400 loss=1.191\n",
      "[intermediate_case1][Ep 05] train acc=0.439 loss=1.135 | val acc=0.380 loss=1.186\n",
      "[intermediate_case1][Ep 06] train acc=0.456 loss=1.117 | val acc=0.410 loss=1.183\n",
      "[intermediate_case1][Ep 07] train acc=0.466 loss=1.104 | val acc=0.420 loss=1.178\n",
      "[intermediate_case1][Ep 08] train acc=0.469 loss=1.096 | val acc=0.430 loss=1.174\n",
      "[intermediate_case1][Ep 09] train acc=0.497 loss=1.078 | val acc=0.490 loss=1.173\n",
      "[intermediate_case1][Ep 10] train acc=0.511 loss=1.071 | val acc=0.450 loss=1.169\n",
      "[intermediate_case1][Ep 11] train acc=0.492 loss=1.060 | val acc=0.490 loss=1.165\n",
      "[intermediate_case1][Ep 12] train acc=0.495 loss=1.058 | val acc=0.500 loss=1.163\n",
      "[intermediate_case1][Ep 13] train acc=0.520 loss=1.040 | val acc=0.500 loss=1.164\n",
      "[intermediate_case1][Ep 14] train acc=0.525 loss=1.040 | val acc=0.490 loss=1.165\n",
      "[intermediate_case1][Ep 15] train acc=0.537 loss=1.022 | val acc=0.500 loss=1.164\n",
      "[intermediate_case1][Ep 16] train acc=0.542 loss=1.011 | val acc=0.490 loss=1.172\n",
      "[intermediate_case1][Ep 17] train acc=0.546 loss=1.007 | val acc=0.490 loss=1.169\n",
      "[intermediate_case1][Ep 18] train acc=0.560 loss=0.999 | val acc=0.450 loss=1.174\n",
      "[intermediate_case1][Ep 19] train acc=0.568 loss=0.984 | val acc=0.450 loss=1.171\n",
      "[intermediate_case1][Ep 20] train acc=0.576 loss=0.974 | val acc=0.450 loss=1.171\n",
      "[intermediate_case1][Ep 21] train acc=0.583 loss=0.964 | val acc=0.440 loss=1.177\n",
      "[intermediate_case1][Ep 22] train acc=0.586 loss=0.957 | val acc=0.420 loss=1.168\n",
      "[intermediate_case1][Ep 23] train acc=0.599 loss=0.935 | val acc=0.440 loss=1.176\n",
      "[intermediate_case1][Ep 24] train acc=0.613 loss=0.925 | val acc=0.450 loss=1.173\n",
      "[intermediate_case1][Ep 25] train acc=0.635 loss=0.900 | val acc=0.410 loss=1.179\n",
      "[intermediate_case1][Ep 26] train acc=0.649 loss=0.877 | val acc=0.440 loss=1.192\n",
      "[intermediate_case1][Ep 27] train acc=0.647 loss=0.866 | val acc=0.430 loss=1.192\n",
      "[intermediate_case1][Ep 28] train acc=0.667 loss=0.845 | val acc=0.410 loss=1.195\n",
      "[intermediate_case1][Ep 29] train acc=0.679 loss=0.823 | val acc=0.460 loss=1.204\n",
      "[intermediate_case1][Ep 30] train acc=0.706 loss=0.794 | val acc=0.410 loss=1.199\n",
      "[intermediate_case1][Ep 31] train acc=0.708 loss=0.783 | val acc=0.420 loss=1.192\n",
      "[intermediate_case1][Ep 32] train acc=0.726 loss=0.750 | val acc=0.390 loss=1.202\n",
      "[intermediate_case1][Ep 33] train acc=0.756 loss=0.718 | val acc=0.400 loss=1.214\n",
      "[intermediate_case1][Ep 34] train acc=0.752 loss=0.700 | val acc=0.410 loss=1.241\n",
      "[intermediate_case1][Ep 35] train acc=0.754 loss=0.683 | val acc=0.470 loss=1.220\n",
      "[intermediate_case1][Ep 36] train acc=0.787 loss=0.648 | val acc=0.420 loss=1.224\n",
      "[intermediate_case1][Ep 37] train acc=0.784 loss=0.636 | val acc=0.410 loss=1.239\n",
      "[intermediate_case1][Ep 38] train acc=0.790 loss=0.611 | val acc=0.470 loss=1.259\n",
      "[intermediate_case1][Ep 39] train acc=0.811 loss=0.570 | val acc=0.420 loss=1.251\n",
      "[intermediate_case1][Ep 40] train acc=0.835 loss=0.547 | val acc=0.460 loss=1.276\n",
      "[intermediate_case1][Ep 41] train acc=0.835 loss=0.530 | val acc=0.480 loss=1.274\n",
      "[intermediate_case1][Ep 42] train acc=0.834 loss=0.519 | val acc=0.460 loss=1.280\n",
      "[intermediate_case1][Ep 43] train acc=0.848 loss=0.501 | val acc=0.450 loss=1.297\n",
      "[intermediate_case1][Ep 44] train acc=0.880 loss=0.468 | val acc=0.500 loss=1.297\n",
      "[intermediate_case1][Ep 45] train acc=0.880 loss=0.453 | val acc=0.460 loss=1.298\n",
      "[intermediate_case1][Ep 46] train acc=0.887 loss=0.433 | val acc=0.470 loss=1.348\n",
      "[intermediate_case1][Ep 47] train acc=0.894 loss=0.415 | val acc=0.430 loss=1.304\n",
      "[intermediate_case1][Ep 48] train acc=0.899 loss=0.398 | val acc=0.460 loss=1.343\n",
      "[intermediate_case1][Ep 49] train acc=0.897 loss=0.383 | val acc=0.450 loss=1.347\n",
      "[intermediate_case1][Ep 50] train acc=0.921 loss=0.359 | val acc=0.500 loss=1.342\n",
      "[intermediate_case1][Ep 51] train acc=0.925 loss=0.349 | val acc=0.430 loss=1.377\n",
      "[intermediate_case1][Ep 52] train acc=0.927 loss=0.340 | val acc=0.470 loss=1.378\n",
      "[intermediate_case1][Ep 53] train acc=0.921 loss=0.328 | val acc=0.460 loss=1.421\n",
      "[intermediate_case1][Ep 54] train acc=0.924 loss=0.321 | val acc=0.450 loss=1.425\n",
      "[intermediate_case1][Ep 55] train acc=0.934 loss=0.306 | val acc=0.490 loss=1.445\n",
      "[intermediate_case1][Ep 56] train acc=0.951 loss=0.291 | val acc=0.460 loss=1.457\n",
      "[intermediate_case1][Ep 57] train acc=0.953 loss=0.287 | val acc=0.450 loss=1.455\n",
      "[intermediate_case1][Ep 58] train acc=0.948 loss=0.283 | val acc=0.450 loss=1.459\n",
      "[intermediate_case1][Ep 59] train acc=0.953 loss=0.276 | val acc=0.440 loss=1.474\n",
      "[intermediate_case1][Ep 60] train acc=0.948 loss=0.273 | val acc=0.430 loss=1.493\n",
      "[intermediate_case1][Ep 61] train acc=0.959 loss=0.265 | val acc=0.450 loss=1.492\n",
      "[intermediate_case1][Ep 62] train acc=0.958 loss=0.262 | val acc=0.460 loss=1.503\n",
      "[intermediate_case1][Ep 63] train acc=0.961 loss=0.251 | val acc=0.430 loss=1.517\n",
      "[intermediate_case1][Ep 64] train acc=0.964 loss=0.243 | val acc=0.420 loss=1.540\n",
      "[intermediate_case1][Ep 65] train acc=0.965 loss=0.242 | val acc=0.430 loss=1.533\n",
      "[intermediate_case1][Ep 66] train acc=0.959 loss=0.245 | val acc=0.440 loss=1.531\n",
      "[intermediate_case1][Ep 67] train acc=0.969 loss=0.235 | val acc=0.460 loss=1.491\n",
      "[intermediate_case1][Ep 68] train acc=0.973 loss=0.231 | val acc=0.420 loss=1.520\n",
      "[intermediate_case1][Ep 69] train acc=0.969 loss=0.235 | val acc=0.420 loss=1.506\n",
      "[intermediate_case1][Ep 70] train acc=0.976 loss=0.228 | val acc=0.430 loss=1.503\n",
      "[intermediate_case1][Ep 71] train acc=0.976 loss=0.227 | val acc=0.420 loss=1.512\n",
      "[intermediate_case1][Ep 72] train acc=0.970 loss=0.228 | val acc=0.420 loss=1.521\n",
      "[intermediate_case1][Ep 73] train acc=0.978 loss=0.221 | val acc=0.430 loss=1.526\n",
      "[intermediate_case1][Ep 74] train acc=0.981 loss=0.220 | val acc=0.440 loss=1.522\n",
      "[intermediate_case1][Ep 75] train acc=0.979 loss=0.220 | val acc=0.430 loss=1.518\n",
      "[intermediate_case1][Ep 76] train acc=0.975 loss=0.217 | val acc=0.440 loss=1.509\n",
      "[intermediate_case1][Ep 77] train acc=0.979 loss=0.221 | val acc=0.440 loss=1.512\n",
      "[intermediate_case1][Ep 78] train acc=0.971 loss=0.223 | val acc=0.430 loss=1.507\n",
      "[intermediate_case1][Ep 79] train acc=0.983 loss=0.218 | val acc=0.430 loss=1.506\n",
      "[intermediate_case1][Ep 80] train acc=0.970 loss=0.222 | val acc=0.430 loss=1.507\n",
      "[intermediate_case1][TEST] acc=0.390 loss=1.539 | saved: project_root/scripts/checkpoints_snn/snn_3_12_intermediate_case1.pt\n",
      "[SAVE] ToMNetSNN_3_12_intermediate_case1 -> project_root/scripts/checkpoints_snn/ToMNetSNN_3_12_intermediate_case1.pt\n",
      "[DONE] ToMNetSNN_3_12_intermediate_case1 (best_acc={'val_acc': 0.5, 'test_acc': 0.39, 'ckpt': PosixPath('project_root/scripts/checkpoints_snn/snn_3_12_intermediate_case1.pt')})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfigSNN(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case1\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNetSNN\") \n",
    "TomNet_Intermediate1_Model = models[\"ToMNetSNN_3_12_intermediate_case1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b3c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case2\n",
      "\n",
      "=== [SNN] Training mode = intermediate_case2 ===\n",
      "[intermediate_case2][Ep 01] train acc=0.330 loss=1.303 | val acc=0.380 loss=1.205\n",
      "[intermediate_case2][Ep 02] train acc=0.389 loss=1.196 | val acc=0.390 loss=1.150\n",
      "[intermediate_case2][Ep 03] train acc=0.415 loss=1.160 | val acc=0.400 loss=1.143\n",
      "[intermediate_case2][Ep 04] train acc=0.439 loss=1.150 | val acc=0.390 loss=1.145\n",
      "[intermediate_case2][Ep 05] train acc=0.451 loss=1.120 | val acc=0.420 loss=1.143\n",
      "[intermediate_case2][Ep 06] train acc=0.479 loss=1.111 | val acc=0.410 loss=1.148\n",
      "[intermediate_case2][Ep 07] train acc=0.476 loss=1.100 | val acc=0.430 loss=1.153\n",
      "[intermediate_case2][Ep 08] train acc=0.491 loss=1.088 | val acc=0.440 loss=1.160\n",
      "[intermediate_case2][Ep 09] train acc=0.486 loss=1.077 | val acc=0.410 loss=1.158\n",
      "[intermediate_case2][Ep 10] train acc=0.506 loss=1.064 | val acc=0.400 loss=1.168\n",
      "[intermediate_case2][Ep 11] train acc=0.497 loss=1.057 | val acc=0.420 loss=1.173\n",
      "[intermediate_case2][Ep 12] train acc=0.515 loss=1.054 | val acc=0.420 loss=1.177\n",
      "[intermediate_case2][Ep 13] train acc=0.507 loss=1.048 | val acc=0.420 loss=1.179\n",
      "[intermediate_case2][Ep 14] train acc=0.514 loss=1.039 | val acc=0.400 loss=1.185\n",
      "[intermediate_case2][Ep 15] train acc=0.521 loss=1.029 | val acc=0.390 loss=1.195\n",
      "[intermediate_case2][Ep 16] train acc=0.524 loss=1.018 | val acc=0.410 loss=1.190\n",
      "[intermediate_case2][Ep 17] train acc=0.527 loss=1.014 | val acc=0.410 loss=1.192\n",
      "[intermediate_case2][Ep 18] train acc=0.546 loss=1.002 | val acc=0.400 loss=1.186\n",
      "[intermediate_case2][Ep 19] train acc=0.537 loss=1.004 | val acc=0.410 loss=1.193\n",
      "[intermediate_case2][Ep 20] train acc=0.560 loss=0.982 | val acc=0.390 loss=1.196\n",
      "[intermediate_case2][Ep 21] train acc=0.580 loss=0.970 | val acc=0.420 loss=1.196\n",
      "[intermediate_case2][Ep 22] train acc=0.576 loss=0.968 | val acc=0.420 loss=1.191\n",
      "[intermediate_case2][Ep 23] train acc=0.594 loss=0.945 | val acc=0.420 loss=1.197\n",
      "[intermediate_case2][Ep 24] train acc=0.616 loss=0.927 | val acc=0.420 loss=1.194\n",
      "[intermediate_case2][Ep 25] train acc=0.627 loss=0.915 | val acc=0.430 loss=1.199\n",
      "[intermediate_case2][Ep 26] train acc=0.650 loss=0.885 | val acc=0.430 loss=1.188\n",
      "[intermediate_case2][Ep 27] train acc=0.646 loss=0.873 | val acc=0.410 loss=1.190\n",
      "[intermediate_case2][Ep 28] train acc=0.662 loss=0.846 | val acc=0.420 loss=1.196\n",
      "[intermediate_case2][Ep 29] train acc=0.691 loss=0.820 | val acc=0.400 loss=1.202\n",
      "[intermediate_case2][Ep 30] train acc=0.714 loss=0.794 | val acc=0.410 loss=1.206\n",
      "[intermediate_case2][Ep 31] train acc=0.720 loss=0.768 | val acc=0.390 loss=1.216\n",
      "[intermediate_case2][Ep 32] train acc=0.735 loss=0.739 | val acc=0.420 loss=1.225\n",
      "[intermediate_case2][Ep 33] train acc=0.744 loss=0.710 | val acc=0.440 loss=1.217\n",
      "[intermediate_case2][Ep 34] train acc=0.765 loss=0.681 | val acc=0.440 loss=1.252\n",
      "[intermediate_case2][Ep 35] train acc=0.786 loss=0.645 | val acc=0.450 loss=1.228\n",
      "[intermediate_case2][Ep 36] train acc=0.800 loss=0.615 | val acc=0.460 loss=1.226\n",
      "[intermediate_case2][Ep 37] train acc=0.802 loss=0.600 | val acc=0.410 loss=1.275\n",
      "[intermediate_case2][Ep 38] train acc=0.819 loss=0.571 | val acc=0.370 loss=1.255\n",
      "[intermediate_case2][Ep 39] train acc=0.824 loss=0.547 | val acc=0.430 loss=1.259\n",
      "[intermediate_case2][Ep 40] train acc=0.838 loss=0.527 | val acc=0.420 loss=1.281\n",
      "[intermediate_case2][Ep 41] train acc=0.859 loss=0.486 | val acc=0.410 loss=1.257\n",
      "[intermediate_case2][Ep 42] train acc=0.869 loss=0.465 | val acc=0.410 loss=1.327\n",
      "[intermediate_case2][Ep 43] train acc=0.875 loss=0.444 | val acc=0.420 loss=1.279\n",
      "[intermediate_case2][Ep 44] train acc=0.882 loss=0.435 | val acc=0.360 loss=1.354\n",
      "[intermediate_case2][Ep 45] train acc=0.899 loss=0.397 | val acc=0.370 loss=1.342\n",
      "[intermediate_case2][Ep 46] train acc=0.900 loss=0.387 | val acc=0.410 loss=1.347\n",
      "[intermediate_case2][Ep 47] train acc=0.916 loss=0.369 | val acc=0.380 loss=1.352\n",
      "[intermediate_case2][Ep 48] train acc=0.919 loss=0.348 | val acc=0.380 loss=1.431\n",
      "[intermediate_case2][Ep 49] train acc=0.930 loss=0.339 | val acc=0.390 loss=1.363\n",
      "[intermediate_case2][Ep 50] train acc=0.934 loss=0.319 | val acc=0.370 loss=1.464\n",
      "[intermediate_case2][Ep 51] train acc=0.925 loss=0.310 | val acc=0.400 loss=1.455\n",
      "[intermediate_case2][Ep 52] train acc=0.945 loss=0.290 | val acc=0.410 loss=1.490\n",
      "[intermediate_case2][Ep 53] train acc=0.946 loss=0.278 | val acc=0.420 loss=1.495\n",
      "[intermediate_case2][Ep 54] train acc=0.946 loss=0.271 | val acc=0.400 loss=1.489\n",
      "[intermediate_case2][Ep 55] train acc=0.958 loss=0.262 | val acc=0.370 loss=1.504\n",
      "[intermediate_case2][Ep 56] train acc=0.959 loss=0.252 | val acc=0.390 loss=1.505\n",
      "[intermediate_case2][Ep 57] train acc=0.960 loss=0.245 | val acc=0.420 loss=1.504\n",
      "[intermediate_case2][Ep 58] train acc=0.964 loss=0.237 | val acc=0.410 loss=1.497\n",
      "[intermediate_case2][Ep 59] train acc=0.956 loss=0.236 | val acc=0.450 loss=1.476\n",
      "[intermediate_case2][Ep 60] train acc=0.966 loss=0.225 | val acc=0.460 loss=1.499\n",
      "[intermediate_case2][Ep 61] train acc=0.975 loss=0.219 | val acc=0.430 loss=1.521\n",
      "[intermediate_case2][Ep 62] train acc=0.961 loss=0.212 | val acc=0.440 loss=1.526\n",
      "[intermediate_case2][Ep 63] train acc=0.969 loss=0.206 | val acc=0.410 loss=1.531\n",
      "[intermediate_case2][Ep 64] train acc=0.968 loss=0.198 | val acc=0.420 loss=1.525\n",
      "[intermediate_case2][Ep 65] train acc=0.974 loss=0.191 | val acc=0.410 loss=1.548\n",
      "[intermediate_case2][Ep 66] train acc=0.970 loss=0.193 | val acc=0.430 loss=1.555\n",
      "[intermediate_case2][Ep 67] train acc=0.979 loss=0.190 | val acc=0.390 loss=1.585\n",
      "[intermediate_case2][Ep 68] train acc=0.978 loss=0.191 | val acc=0.400 loss=1.581\n",
      "[intermediate_case2][Ep 69] train acc=0.974 loss=0.188 | val acc=0.430 loss=1.543\n",
      "[intermediate_case2][Ep 70] train acc=0.975 loss=0.185 | val acc=0.420 loss=1.577\n",
      "[intermediate_case2][Ep 71] train acc=0.980 loss=0.177 | val acc=0.410 loss=1.567\n",
      "[intermediate_case2][Ep 72] train acc=0.976 loss=0.182 | val acc=0.400 loss=1.543\n",
      "[intermediate_case2][Ep 73] train acc=0.981 loss=0.178 | val acc=0.420 loss=1.556\n",
      "[intermediate_case2][Ep 74] train acc=0.979 loss=0.177 | val acc=0.420 loss=1.568\n",
      "[intermediate_case2][Ep 75] train acc=0.983 loss=0.177 | val acc=0.410 loss=1.572\n",
      "[intermediate_case2][Ep 76] train acc=0.981 loss=0.178 | val acc=0.420 loss=1.563\n",
      "[intermediate_case2][Ep 77] train acc=0.979 loss=0.178 | val acc=0.420 loss=1.560\n",
      "[intermediate_case2][Ep 78] train acc=0.979 loss=0.179 | val acc=0.420 loss=1.565\n",
      "[intermediate_case2][Ep 79] train acc=0.975 loss=0.181 | val acc=0.410 loss=1.562\n",
      "[intermediate_case2][Ep 80] train acc=0.973 loss=0.180 | val acc=0.420 loss=1.575\n",
      "[intermediate_case2][TEST] acc=0.380 loss=1.711 | saved: project_root/scripts/checkpoints_snn/snn_3_12_intermediate_case2.pt\n",
      "[SAVE] ToMNetSNN_3_12_intermediate_case2 -> project_root/scripts/checkpoints_snn/ToMNetSNN_3_12_intermediate_case2.pt\n",
      "[DONE] ToMNetSNN_3_12_intermediate_case2 (best_acc={'val_acc': 0.46, 'test_acc': 0.38, 'ckpt': PosixPath('project_root/scripts/checkpoints_snn/snn_3_12_intermediate_case2.pt')})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfigSNN(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case2\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNetSNN\") \n",
    "TomNet_Intermediate2_Model = models[\"ToMNetSNN_3_12_intermediate_case2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94a136",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a97334fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluate import evaluate_model_performance\n",
    "\n",
    "# 根據實際的批次鍵名定義forward函數\n",
    "def forward_tomnet_SNN(m, b):\n",
    "    return m(b[\"grid_seq\"], b[\"tmask\"], b[\"choices_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_random model evaluation ===\n",
      "accuracy: 0.3600\n",
      "precision: 0.5030\n",
      "recall rate: 0.3600\n",
      "F1 score: 0.3683\n",
      "ROC AUC: 0.6452\n",
      "R2 Score: -0.7594\n",
      "Prediction Entropy: 0.7121\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 7,393,792\n",
      "Energy per sample: 3.40e-05 J\n",
      "Energy per accuracy unit: 9.45e-05 J/acc\n",
      "MACs per accuracy unit: 20,538,311 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "val_loader, in_ch, input_hw = make_loader(cfg, split=\"val\", mode=\"random\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"mps\",\n",
    "    forward_fn=forward_tomnet_SNN,\n",
    "    energy_forward_fn=forward_tomnet_SNN\n",
    ")\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_random model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5516b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_rulemap model evaluation ===\n",
      "accuracy: 0.9500\n",
      "precision: 0.9506\n",
      "recall rate: 0.9500\n",
      "F1 score: 0.9501\n",
      "ROC AUC: 0.9974\n",
      "R2 Score: 0.8558\n",
      "Prediction Entropy: 0.0717\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 7,393,792\n",
      "Energy per sample: 3.40e-05 J\n",
      "Energy per accuracy unit: 3.58e-05 J/acc\n",
      "MACs per accuracy unit: 7,782,939 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# rulemap\n",
    "val_loader, in_ch, input_hw = make_loader(cfg, split=\"val\", mode=\"rulemap\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Rulemap_Model,\n",
    "    val_loader,\n",
    "    device=\"mps\",\n",
    "    forward_fn=forward_tomnet_SNN,\n",
    "    energy_forward_fn=forward_tomnet_SNN\n",
    ")\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_rulemap model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e2d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_logic model evaluation ===\n",
      "accuracy: 0.9800\n",
      "precision: 0.9803\n",
      "recall rate: 0.9800\n",
      "F1 score: 0.9799\n",
      "ROC AUC: 0.9992\n",
      "R2 Score: 0.9798\n",
      "Prediction Entropy: 0.0453\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 7,393,792\n",
      "Energy per sample: 3.40e-05 J\n",
      "Energy per accuracy unit: 3.47e-05 J/acc\n",
      "MACs per accuracy unit: 7,544,686 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# logic\n",
    "val_loader, in_ch, input_hw = make_loader(cfg, split=\"val\", mode=\"logic\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Logic_Model,\n",
    "    val_loader,\n",
    "    device=\"mps\",\n",
    "    forward_fn=forward_tomnet_SNN,\n",
    "    energy_forward_fn=forward_tomnet_SNN\n",
    ")   \n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_logic model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "884fbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_intermediate_case1 model evaluation ===\n",
      "accuracy: 0.4300\n",
      "precision: 0.5161\n",
      "recall rate: 0.4300\n",
      "F1 score: 0.4507\n",
      "ROC AUC: 0.7312\n",
      "R2 Score: -0.3609\n",
      "Prediction Entropy: 0.7282\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 7,393,792\n",
      "Energy per sample: 3.40e-05 J\n",
      "Energy per accuracy unit: 7.91e-05 J/acc\n",
      "MACs per accuracy unit: 17,194,865 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# intermediate_case1\n",
    "val_loader, in_ch, input_hw = make_loader(cfg, split=\"val\", mode=\"intermediate_case1\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Intermediate1_Model,\n",
    "    val_loader,\n",
    "    device=\"mps\",\n",
    "    forward_fn=forward_tomnet_SNN,\n",
    "    energy_forward_fn=forward_tomnet_SNN\n",
    ")   \n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_intermediate_case1 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edd4949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_intermediate_case2 model evaluation ===\n",
      "accuracy: 0.4200\n",
      "precision: 0.5368\n",
      "recall rate: 0.4200\n",
      "F1 score: 0.4356\n",
      "ROC AUC: 0.7078\n",
      "R2 Score: -0.6316\n",
      "Prediction Entropy: 0.7243\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 7,393,792\n",
      "Energy per sample: 3.40e-05 J\n",
      "Energy per accuracy unit: 8.10e-05 J/acc\n",
      "MACs per accuracy unit: 17,604,267 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# intermediate_case2\n",
    "val_loader, in_ch, input_hw = make_loader(cfg, split=\"val\", mode=\"intermediate_case2\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Intermediate2_Model,\n",
    "    val_loader,\n",
    "    device=\"mps\",\n",
    "    forward_fn=forward_tomnet_SNN,\n",
    "    energy_forward_fn=forward_tomnet_SNN\n",
    ")   \n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_intermediate_case2 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63af9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
