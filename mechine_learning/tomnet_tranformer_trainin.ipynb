{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json, math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from grid_dataset import GridSeqDataset\n",
    "from collate_grid import collate_grid\n",
    "\n",
    "# === Unify root / sim_name ===\n",
    "PROJECT_ROOT = Path(\"/Users/Jer_ry/Desktop/script_tom\")\n",
    "SIM_NAME = \"3_12\"\n",
    "N_AGENTS = 3\n",
    "\n",
    "_MODE_ALIASES = {\n",
    "    \"rulemap\":\"rulemap\", \"random\":\"random\", \"logic\":\"logic\",\n",
    "    \"intermediate_case1\":\"intermediate_case1\", \"intermediate_case2\":\"intermediate_case2\",\n",
    "    \"intermediate_1\":\"intermediate_case1\", \"intermediate_2\":\"intermediate_case2\",\n",
    "    \"intemediete_1\":\"intermediate_case1\", \"intemediete_2\":\"intermediate_case2\",\n",
    "    \"intermediat_case1\":\"intermediate_case1\", \"intermediat_case2\":\"intermediate_case2\",\n",
    "    \"all\":\"all\",\n",
    "}\n",
    "_ALL_MODES = [\"rulemap\",\"random\",\"intermediate_case1\",\"intermediate_case2\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    data_root: Path = PROJECT_ROOT\n",
    "    sim_name: str = SIM_NAME\n",
    "    mode: str = \"all\"    # or single mode\n",
    "    # model\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    n_layers: int = 2\n",
    "    stem_ch: int = 32\n",
    "    n_blocks: int = 4\n",
    "    e_char_dim: int = 64\n",
    "    dropout: float = 0.1\n",
    "    last_k: int = 3\n",
    "    # train\n",
    "    batch_size: int = 32\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    max_epochs: int = 20\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c810aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision_tomnet_transformer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, ch: int):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.c2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn1(self.c1(x)))\n",
    "        h = self.bn2(self.c2(h))\n",
    "        return self.act(x + h)\n",
    "\n",
    "class VisionCharNet(nn.Module):\n",
    "    \"\"\"\n",
    "    input: grid_seq [B, T, H, W, C]  (C 可為 18/30 ...)\n",
    "    steps:\n",
    "      1) per-frame 1x1 conv 把 C -> stem_ch\n",
    "      2) 3x3 conv + n 個 residual blocks\n",
    "      3) global average pool => [B, T, d_vis]\n",
    "      4) TransformerEncoder over time => [B, T, d_model]\n",
    "      5) 取 CLS（可選）或 masked mean => e_char [B, e_char_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, stem_ch: int = 32, n_blocks: int = 4,\n",
    "                 d_model: int = 256, nhead: int = 8, n_layers: int = 2,\n",
    "                 e_char_dim: int = 64, use_cls_token: bool = True, last_k: int = 3):\n",
    "        super().__init__()\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.last_k = last_k\n",
    "        self.stem1x1 = nn.Conv2d(in_ch, stem_ch, kernel_size=1)\n",
    "        self.c3 = nn.Conv2d(stem_ch, stem_ch, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(stem_ch)\n",
    "        self.blocks = nn.Sequential(*[BasicBlock(stem_ch) for _ in range(n_blocks)])\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)                 # [B*T, ch,1,1]\n",
    "        # 時序 Transformer\n",
    "        self.in_proj = nn.Linear(stem_ch, d_model)         # 視覺特徵 -> d_model\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=4*d_model, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.to_e = nn.Linear(d_model, e_char_dim)\n",
    "\n",
    "    def forward(self, grid_seq: torch.Tensor, tmask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        grid_seq: [B,T,H,W,C]; tmask: [B,T] True=有效\n",
    "        return:\n",
    "          e_char [B,e_char_dim],   # 角色 embedding\n",
    "          h_time [B,T,d_model]     # 每步時序特徵（給 q_vec 用）\n",
    "        \"\"\"\n",
    "        B,T,H,W,C = grid_seq.shape\n",
    "        x = grid_seq.permute(0,1,4,2,3).contiguous()     # [B,T,C,H,W]\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        x = self.stem1x1(x)\n",
    "        x = self.act(self.bn3(self.c3(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze(-1)                      # [B*T, stem_ch]\n",
    "        x = x.view(B, T, -1)                             # [B,T,stem_ch]\n",
    "        h = self.in_proj(x)                              # [B,T,d_model]\n",
    "        key_pad = ~tmask                                 # True=PAD\n",
    "        h = self.encoder(h, src_key_padding_mask=key_pad)\n",
    "        h = self.ln(h)                                   # [B,T,d_model]\n",
    "\n",
    "        if self.use_cls_token:\n",
    "            # 取最後一個有效步當「CLS」\n",
    "            last = tmask.sum(dim=1) - 1                  # [B]\n",
    "            e_src = h.gather(1, last.view(B,1,1).expand(B,1,h.size(-1))).squeeze(1)\n",
    "        else:\n",
    "            m = tmask.unsqueeze(-1)                      # [B,T,1]\n",
    "            e_src = (h * m).sum(dim=1) / m.sum(dim=1).clamp_min(1.0)\n",
    "        e_char = self.to_e(e_src)                        # [B,e_char_dim]\n",
    "        return e_char, h\n",
    "\n",
    "class PredHead(nn.Module):\n",
    "    \"\"\"\n",
    "    與你原本 PredNet 類似，但不再用 token 序列的 q_vec，\n",
    "    而是用時序特徵 h_time 的最後 K 步平均作為 query。\n",
    "    choices 仍用 cell-id 嵌入。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, e_char_dim: int, dropout: float = 0.1,\n",
    "                 cell_vocab: int = 24*24, choice_emb_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.proj_e = nn.Linear(e_char_dim, d_model)\n",
    "        self.choice_emb = nn.Embedding(cell_vocab, choice_emb_dim)\n",
    "        self.proj_choice = nn.Linear(choice_emb_dim, d_model)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4*d_model, 2*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*d_model, 1)\n",
    "        )\n",
    "\n",
    "    def _embed_choice_triplet(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        # ids: [B,4,3]\n",
    "        emb = self.choice_emb(ids)         # [B,4,3,Dc]\n",
    "        return emb.mean(dim=2)             # [B,4,Dc]\n",
    "\n",
    "    def forward(self, e_char: torch.Tensor, h_time: torch.Tensor, tmask: torch.Tensor,\n",
    "                choices_ids: torch.Tensor, last_k: int = 3) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        e_char:   [B,e]\n",
    "        h_time:   [B,T,d_model]\n",
    "        tmask:    [B,T]\n",
    "        choices:  [B,4,3]\n",
    "        return logits: [B,4]\n",
    "        \"\"\"\n",
    "        B, T, D = h_time.shape\n",
    "        L = tmask.sum(dim=1)                     # [B]，每個樣本有效長度\n",
    "        k = max(int(last_k), 1)                  # ← 修正：用 Python int 取最大至少 1\n",
    "\n",
    "        # 取最後 k 步（不足就重複最後一個有效步；如果 T==0 就用 0）\n",
    "        # 這段做法向量化、避免 Python for 迴圈慢：\n",
    "        # ex: L=[14,10] & k=3 → 索引 [[11,12,13],[7,8,9]]（0-based）\n",
    "        base = (L - 1).clamp(min=0)              # [B]\n",
    "        offs = torch.arange(k, device=h_time.device)  # [k] = [0,1,...,k-1]\n",
    "        idxs = (base.unsqueeze(1) - (k - 1 - offs)).clamp(min=0)  # [B,k]\n",
    "        gather = h_time.gather(1, idxs.unsqueeze(-1).expand(B, k, D))  # [B,k,D]\n",
    "        q_vec = gather.mean(dim=1)               # [B,D]\n",
    "\n",
    "        e_proj = self.proj_e(e_char)             # [B,D]\n",
    "        prefix_fused = self.ln(e_proj + q_vec)   # [B,D]\n",
    "\n",
    "        # choices：三元組 cell-id → 平均後投影到 D\n",
    "        ch = self._embed_choice_triplet(choices_ids)   # [B,4,Dc]\n",
    "        ch = self.proj_choice(ch)                      # [B,4,D]\n",
    "\n",
    "        pf = prefix_fused.unsqueeze(1).expand_as(ch)   # [B,4,D]\n",
    "        feat = torch.cat([pf, ch, torch.abs(pf - ch), pf * ch], dim=-1)  # [B,4,4D]\n",
    "        logits = self.mlp(feat).squeeze(-1)           # [B,4]\n",
    "        return logits\n",
    "\n",
    "class ToMNetVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    整合：VisionCharNet（ToMNet 風格特徵抽取 + Transformer 時序） + PredHead\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, d_model: int=256, nhead: int=8, n_layers: int=2,\n",
    "                 stem_ch: int=32, n_blocks: int=4, e_char_dim: int=64, dropout: float=0.1,\n",
    "                 cell_vocab: int = 24*24, use_cls_token: bool=True, last_k: int=3):\n",
    "        super().__init__()\n",
    "        self.vision = VisionCharNet(in_ch=in_ch, stem_ch=stem_ch, n_blocks=n_blocks,\n",
    "                                    d_model=d_model, nhead=nhead, n_layers=n_layers,\n",
    "                                    e_char_dim=e_char_dim, use_cls_token=use_cls_token, last_k=last_k)\n",
    "        self.head = PredHead(d_model=d_model, e_char_dim=e_char_dim, dropout=dropout,\n",
    "                             cell_vocab=cell_vocab)\n",
    "\n",
    "        self.last_k = last_k\n",
    "\n",
    "    def forward(self, grid_seq: torch.Tensor, tmask: torch.Tensor, choices_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # grid_seq: [B,T,H,W,C]\n",
    "        C = grid_seq.size(-1)\n",
    "        assert C % 6 == 0, f\"Expect channels = 6 × n_agents (no obstacle). Got C={C}\"\n",
    "        e_char, h_time = self.vision(grid_seq, tmask)\n",
    "        logits = self.head(e_char, h_time, tmask, choices_ids, last_k=self.last_k)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import random, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset \n",
    "from grid_dataset import GridSeqDataset\n",
    "from collate_grid import collate_grid\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_MODE_ALIASES = {\n",
    "    \"rulemap\":\"rulemap\", \"random\":\"random\", \"logic\":\"logic\",\n",
    "    \"intermediate_case1\":\"intermediate_case1\", \"intermediate_case2\":\"intermediate_case2\",\n",
    "    \"intermediate_1\":\"intermediate_case1\", \"intermediate_2\":\"intermediate_case2\",\n",
    "    \"intemediete_1\":\"intermediate_case1\", \"intemediete_2\":\"intermediate_case2\",\n",
    "    \"intermediat_case1\":\"intermediate_case1\", \"intermediat_case2\":\"intermediate_case2\",\n",
    "    \"all\":\"all\",\n",
    "}\n",
    "_ALL_MODES = [\"rulemap\",\"random\",\"intermediate_case1\",\"intermediate_case2\"]  # 先不含 logic\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    data_root: Path\n",
    "    sim_name: str = \"sim1\"\n",
    "    mode: str = \"all\"   # or single mode\n",
    "    # model\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    n_layers: int = 2\n",
    "    stem_ch: int = 32\n",
    "    n_blocks: int = 4\n",
    "    e_char_dim: int = 64\n",
    "    dropout: float = 0.1\n",
    "    last_k: int = 3\n",
    "    # train\n",
    "    batch_size: int = 32\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    max_epochs: int = 20\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_dir: Path = Path(\"../checkpoints\")\n",
    "\n",
    "    def canonical_mode(self) -> str:\n",
    "        key = self.mode.strip().lower()\n",
    "        if key not in _MODE_ALIASES: \n",
    "            raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "        return _MODE_ALIASES[key]\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_loader(cfg: TrainConfig, split: str, mode: str) -> Tuple[DataLoader, int]:\n",
    "    ds = GridSeqDataset(cfg.data_root, cfg.sim_name, split, mode)\n",
    "    one = ds[0][\"grid_seq\"]\n",
    "    in_ch = int(one.shape[-1])\n",
    "    ld = DataLoader(ds, batch_size=cfg.batch_size, shuffle=(split==\"train\"),\n",
    "                    num_workers=0, pin_memory=True, collate_fn=collate_grid)\n",
    "    return ld, in_ch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    n, corr, tot_loss = 0, 0, 0.0\n",
    "    for batch in loader:\n",
    "        x = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "        logits = model(x[\"grid_seq\"], x[\"tmask\"], x[\"choices_ids\"])\n",
    "        loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        corr += (pred == x[\"labels\"]).sum().item()\n",
    "        tot_loss += loss.item() * logits.size(0)\n",
    "        n += logits.size(0)\n",
    "    return corr/max(n,1), tot_loss/max(n,1)\n",
    "\n",
    "def train_one_mode(cfg: TrainConfig, mode: str):\n",
    "    print(f\"\\n=== Training mode = {mode} ===\")\n",
    "    train_ld, in_ch = make_loader(cfg, \"train\", mode)\n",
    "    val_ld, _ = make_loader(cfg, \"val\", mode)\n",
    "    test_ld, _ = make_loader(cfg, \"test\", mode)\n",
    "\n",
    "    model = ToMNetVisionTransformer(\n",
    "        in_ch=in_ch, d_model=cfg.d_model, nhead=cfg.nhead,\n",
    "        n_layers=cfg.n_layers, stem_ch=cfg.stem_ch, n_blocks=cfg.n_blocks,\n",
    "        e_char_dim=cfg.e_char_dim, dropout=cfg.dropout,\n",
    "        use_cls_token=True, last_k=cfg.last_k\n",
    "    ).to(cfg.device)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.max_epochs)\n",
    "\n",
    "    best_va, best_state = 0.0, None\n",
    "    for ep in range(1, cfg.max_epochs+1):\n",
    "        model.train()\n",
    "        n = corr = 0\n",
    "        tot_loss = 0.0\n",
    "        \n",
    "        for batch in train_ld:\n",
    "            x = {k:(v.to(cfg.device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}\n",
    "            logits = model(x[\"grid_seq\"], x[\"tmask\"], x[\"choices_ids\"])\n",
    "            loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            \n",
    "            pred = logits.argmax(dim=-1)\n",
    "            corr += (pred == x[\"labels\"]).sum().item()\n",
    "            tot_loss += loss.item() * logits.size(0)\n",
    "            n += logits.size(0)\n",
    "            \n",
    "        tr_acc, tr_loss = corr/max(n,1), tot_loss/max(n,1)\n",
    "        va_acc, va_loss = evaluate(model, val_ld, cfg.device)\n",
    "        sch.step()\n",
    "        \n",
    "        print(f\"[{mode}][Ep {ep:02d}] train acc={tr_acc:.3f} loss={tr_loss:.3f} | val acc={va_acc:.3f} loss={va_loss:.3f}\")\n",
    "        \n",
    "        if va_acc > best_va:\n",
    "            best_va, best_state = va_acc, {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None: \n",
    "        model.load_state_dict(best_state)\n",
    "    te_acc, te_loss = evaluate(model, test_ld, cfg.device)\n",
    "    print(f\"[{mode}][TEST] acc={te_acc:.3f} loss={te_loss:.3f}\")\n",
    "    return model, {\"val_acc\": best_va, \"test_acc\": te_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "_ALL_MODES = [\"random\", \"rulemap\", \"logic\", \"intermediate_case1\", \"intermediate_case2\"]\n",
    "\n",
    "def _model_name(sim_name: str, mode: str, kind: str = \"ToMNet\") -> str:\n",
    "    return f\"{kind}_{sim_name}_{mode}\"\n",
    "\n",
    "def _save_model(model: nn.Module, cfg: TrainConfig, mode: str, kind: str = \"ToMNet\") -> Path:\n",
    "    cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    name = _model_name(cfg.sim_name, mode, kind)\n",
    "    path = cfg.save_dir / f\"{name}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"[SAVE] {name} -> {path}\")\n",
    "    return path\n",
    "\n",
    "def run(cfg: TrainConfig, kind: str = \"ToMNet\") -> Dict[str, nn.Module]:\n",
    "    \"\"\"\n",
    "    kind: \"ToMNet\" / \"ToMNetSNN\" / \"ToMNetConv\", for filename\n",
    "    Returns: {model_name: model_instance}\n",
    "    \"\"\"\n",
    "    models: Dict[str, nn.Module] = {}\n",
    "\n",
    "    if cfg.mode == \"all\":\n",
    "        modes = _ALL_MODES\n",
    "    else:\n",
    "        modes = [cfg.mode]\n",
    "\n",
    "    for mode in modes:\n",
    "        print(f\"[TRAIN] sim={cfg.sim_name} mode={mode}\")\n",
    "        model, best_acc = train_one_mode(cfg, mode)\n",
    "\n",
    "        name = _model_name(cfg.sim_name, mode, kind)\n",
    "        _save_model(model, cfg, mode, kind=kind)\n",
    "        models[name] = model\n",
    "        print(f\"[DONE] {name} (best_acc={best_acc})\")\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a23b6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbe63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=random\n",
      "\n",
      "=== Training mode = random ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random][Ep 01] train acc=0.320 loss=1.309 | val acc=0.370 loss=1.268\n",
      "[random][Ep 02] train acc=0.374 loss=1.201 | val acc=0.370 loss=1.215\n",
      "[random][Ep 03] train acc=0.414 loss=1.158 | val acc=0.340 loss=1.201\n",
      "[random][Ep 04] train acc=0.434 loss=1.133 | val acc=0.370 loss=1.207\n",
      "[random][Ep 05] train acc=0.435 loss=1.124 | val acc=0.370 loss=1.210\n",
      "[random][Ep 06] train acc=0.458 loss=1.110 | val acc=0.360 loss=1.213\n",
      "[random][Ep 07] train acc=0.480 loss=1.101 | val acc=0.340 loss=1.225\n",
      "[random][Ep 08] train acc=0.492 loss=1.086 | val acc=0.360 loss=1.224\n",
      "[random][Ep 09] train acc=0.507 loss=1.079 | val acc=0.360 loss=1.224\n",
      "[random][Ep 10] train acc=0.516 loss=1.067 | val acc=0.390 loss=1.222\n",
      "[random][Ep 11] train acc=0.522 loss=1.051 | val acc=0.380 loss=1.246\n",
      "[random][Ep 12] train acc=0.526 loss=1.031 | val acc=0.340 loss=1.250\n",
      "[random][Ep 13] train acc=0.535 loss=1.002 | val acc=0.340 loss=1.256\n",
      "[random][Ep 14] train acc=0.580 loss=0.977 | val acc=0.360 loss=1.258\n",
      "[random][Ep 15] train acc=0.608 loss=0.941 | val acc=0.390 loss=1.273\n",
      "[random][Ep 16] train acc=0.654 loss=0.879 | val acc=0.340 loss=1.284\n",
      "[random][Ep 17] train acc=0.688 loss=0.816 | val acc=0.400 loss=1.378\n",
      "[random][Ep 18] train acc=0.724 loss=0.757 | val acc=0.390 loss=1.363\n",
      "[random][Ep 19] train acc=0.744 loss=0.703 | val acc=0.380 loss=1.456\n",
      "[random][Ep 20] train acc=0.764 loss=0.639 | val acc=0.410 loss=1.556\n",
      "[random][Ep 21] train acc=0.799 loss=0.592 | val acc=0.370 loss=1.467\n",
      "[random][Ep 22] train acc=0.826 loss=0.538 | val acc=0.400 loss=1.503\n",
      "[random][Ep 23] train acc=0.859 loss=0.476 | val acc=0.400 loss=1.570\n",
      "[random][Ep 24] train acc=0.871 loss=0.441 | val acc=0.400 loss=1.800\n",
      "[random][Ep 25] train acc=0.890 loss=0.397 | val acc=0.320 loss=1.697\n",
      "[random][Ep 26] train acc=0.889 loss=0.367 | val acc=0.390 loss=1.671\n",
      "[random][Ep 27] train acc=0.917 loss=0.319 | val acc=0.340 loss=1.765\n",
      "[random][Ep 28] train acc=0.941 loss=0.282 | val acc=0.320 loss=1.834\n",
      "[random][Ep 29] train acc=0.961 loss=0.249 | val acc=0.440 loss=1.916\n",
      "[random][Ep 30] train acc=0.961 loss=0.222 | val acc=0.370 loss=1.882\n",
      "[random][Ep 31] train acc=0.966 loss=0.191 | val acc=0.470 loss=1.778\n",
      "[random][Ep 32] train acc=0.983 loss=0.155 | val acc=0.380 loss=2.180\n",
      "[random][Ep 33] train acc=0.986 loss=0.133 | val acc=0.360 loss=2.108\n",
      "[random][Ep 34] train acc=0.990 loss=0.109 | val acc=0.390 loss=2.234\n",
      "[random][Ep 35] train acc=0.996 loss=0.094 | val acc=0.340 loss=2.134\n",
      "[random][Ep 36] train acc=0.999 loss=0.077 | val acc=0.430 loss=1.972\n",
      "[random][Ep 37] train acc=0.995 loss=0.064 | val acc=0.390 loss=2.321\n",
      "[random][Ep 38] train acc=0.995 loss=0.053 | val acc=0.440 loss=2.099\n",
      "[random][Ep 39] train acc=0.998 loss=0.044 | val acc=0.410 loss=2.432\n",
      "[random][Ep 40] train acc=0.999 loss=0.039 | val acc=0.390 loss=2.293\n",
      "[random][Ep 41] train acc=1.000 loss=0.032 | val acc=0.390 loss=2.580\n",
      "[random][Ep 42] train acc=1.000 loss=0.023 | val acc=0.360 loss=2.547\n",
      "[random][Ep 43] train acc=1.000 loss=0.019 | val acc=0.420 loss=2.671\n",
      "[random][Ep 44] train acc=1.000 loss=0.016 | val acc=0.430 loss=2.709\n",
      "[random][Ep 45] train acc=0.999 loss=0.014 | val acc=0.360 loss=3.005\n",
      "[random][Ep 46] train acc=1.000 loss=0.011 | val acc=0.420 loss=2.684\n",
      "[random][Ep 47] train acc=1.000 loss=0.009 | val acc=0.410 loss=3.042\n",
      "[random][Ep 48] train acc=1.000 loss=0.008 | val acc=0.430 loss=3.061\n",
      "[random][Ep 49] train acc=1.000 loss=0.006 | val acc=0.390 loss=2.708\n",
      "[random][Ep 50] train acc=1.000 loss=0.005 | val acc=0.450 loss=2.747\n",
      "[random][Ep 51] train acc=1.000 loss=0.005 | val acc=0.400 loss=2.817\n",
      "[random][Ep 52] train acc=1.000 loss=0.004 | val acc=0.390 loss=2.780\n",
      "[random][Ep 53] train acc=1.000 loss=0.004 | val acc=0.390 loss=2.776\n",
      "[random][Ep 54] train acc=1.000 loss=0.004 | val acc=0.420 loss=2.825\n",
      "[random][Ep 55] train acc=1.000 loss=0.004 | val acc=0.420 loss=2.830\n",
      "[random][Ep 56] train acc=1.000 loss=0.004 | val acc=0.380 loss=2.871\n",
      "[random][Ep 57] train acc=1.000 loss=0.003 | val acc=0.390 loss=2.927\n",
      "[random][Ep 58] train acc=1.000 loss=0.003 | val acc=0.390 loss=2.929\n",
      "[random][Ep 59] train acc=1.000 loss=0.003 | val acc=0.370 loss=2.935\n",
      "[random][Ep 60] train acc=1.000 loss=0.003 | val acc=0.410 loss=2.907\n",
      "[random][Ep 61] train acc=1.000 loss=0.003 | val acc=0.410 loss=2.934\n",
      "[random][Ep 62] train acc=1.000 loss=0.003 | val acc=0.390 loss=2.926\n",
      "[random][Ep 63] train acc=1.000 loss=0.003 | val acc=0.420 loss=2.957\n",
      "[random][Ep 64] train acc=1.000 loss=0.003 | val acc=0.410 loss=2.935\n",
      "[random][Ep 65] train acc=1.000 loss=0.003 | val acc=0.370 loss=2.961\n",
      "[random][Ep 66] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.931\n",
      "[random][Ep 67] train acc=1.000 loss=0.002 | val acc=0.410 loss=2.926\n",
      "[random][Ep 68] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.932\n",
      "[random][Ep 69] train acc=1.000 loss=0.002 | val acc=0.410 loss=2.940\n",
      "[random][Ep 70] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.935\n",
      "[random][Ep 71] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.940\n",
      "[random][Ep 72] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.936\n",
      "[random][Ep 73] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.937\n",
      "[random][Ep 74] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.938\n",
      "[random][Ep 75] train acc=1.000 loss=0.002 | val acc=0.410 loss=2.945\n",
      "[random][Ep 76] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.944\n",
      "[random][Ep 77] train acc=1.000 loss=0.002 | val acc=0.410 loss=2.941\n",
      "[random][Ep 78] train acc=1.000 loss=0.002 | val acc=0.410 loss=2.941\n",
      "[random][Ep 79] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.946\n",
      "[random][Ep 80] train acc=1.000 loss=0.002 | val acc=0.400 loss=2.945\n",
      "[random][TEST] acc=0.400 loss=3.171\n",
      "[SAVE] ToMNet_Transformer_3_12_random -> ../checkpoints/ToMNet_Transformer_3_12_random.pt\n",
      "[DONE] ToMNet_Transformer_3_12_random (best_acc={'val_acc': 0.47, 'test_acc': 0.4})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"random\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "\n",
    "models = run(cfg, kind=\"ToMNet_Transformer\")  \n",
    "TomNet_Random_Model = models[\"ToMNet_Transformer_3_12_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02879139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=rulemap\n",
      "\n",
      "=== Training mode = rulemap ===\n",
      "[rulemap][Ep 01] train acc=0.649 loss=1.123 | val acc=0.840 loss=0.988\n",
      "[rulemap][Ep 02] train acc=0.811 loss=0.695 | val acc=0.860 loss=0.645\n",
      "[rulemap][Ep 03] train acc=0.844 loss=0.462 | val acc=0.860 loss=0.448\n",
      "[rulemap][Ep 04] train acc=0.874 loss=0.345 | val acc=0.900 loss=0.342\n",
      "[rulemap][Ep 05] train acc=0.904 loss=0.272 | val acc=0.900 loss=0.272\n",
      "[rulemap][Ep 06] train acc=0.917 loss=0.228 | val acc=0.910 loss=0.233\n",
      "[rulemap][Ep 07] train acc=0.936 loss=0.186 | val acc=0.920 loss=0.203\n",
      "[rulemap][Ep 08] train acc=0.948 loss=0.158 | val acc=0.920 loss=0.173\n",
      "[rulemap][Ep 09] train acc=0.961 loss=0.138 | val acc=0.920 loss=0.152\n",
      "[rulemap][Ep 10] train acc=0.965 loss=0.124 | val acc=0.940 loss=0.141\n",
      "[rulemap][Ep 11] train acc=0.966 loss=0.109 | val acc=0.960 loss=0.131\n",
      "[rulemap][Ep 12] train acc=0.968 loss=0.104 | val acc=0.970 loss=0.118\n",
      "[rulemap][Ep 13] train acc=0.974 loss=0.094 | val acc=0.960 loss=0.118\n",
      "[rulemap][Ep 14] train acc=0.979 loss=0.079 | val acc=0.970 loss=0.111\n",
      "[rulemap][Ep 15] train acc=0.979 loss=0.071 | val acc=0.980 loss=0.098\n",
      "[rulemap][Ep 16] train acc=0.985 loss=0.067 | val acc=0.980 loss=0.095\n",
      "[rulemap][Ep 17] train acc=0.988 loss=0.055 | val acc=0.980 loss=0.091\n",
      "[rulemap][Ep 18] train acc=0.990 loss=0.049 | val acc=0.980 loss=0.085\n",
      "[rulemap][Ep 19] train acc=0.991 loss=0.042 | val acc=0.980 loss=0.079\n",
      "[rulemap][Ep 20] train acc=0.999 loss=0.031 | val acc=0.980 loss=0.087\n",
      "[rulemap][Ep 21] train acc=1.000 loss=0.025 | val acc=0.980 loss=0.073\n",
      "[rulemap][Ep 22] train acc=0.999 loss=0.022 | val acc=0.980 loss=0.078\n",
      "[rulemap][Ep 23] train acc=1.000 loss=0.019 | val acc=0.970 loss=0.100\n",
      "[rulemap][Ep 24] train acc=1.000 loss=0.016 | val acc=0.990 loss=0.087\n",
      "[rulemap][Ep 25] train acc=1.000 loss=0.012 | val acc=0.980 loss=0.079\n",
      "[rulemap][Ep 26] train acc=1.000 loss=0.010 | val acc=0.980 loss=0.084\n",
      "[rulemap][Ep 27] train acc=1.000 loss=0.009 | val acc=0.980 loss=0.094\n",
      "[rulemap][Ep 28] train acc=1.000 loss=0.008 | val acc=0.980 loss=0.102\n",
      "[rulemap][Ep 29] train acc=1.000 loss=0.007 | val acc=0.970 loss=0.102\n",
      "[rulemap][Ep 30] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.089\n",
      "[rulemap][Ep 31] train acc=1.000 loss=0.005 | val acc=0.970 loss=0.103\n",
      "[rulemap][Ep 32] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.087\n",
      "[rulemap][Ep 33] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.089\n",
      "[rulemap][Ep 34] train acc=1.000 loss=0.003 | val acc=0.960 loss=0.121\n",
      "[rulemap][Ep 35] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.102\n",
      "[rulemap][Ep 36] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.114\n",
      "[rulemap][Ep 37] train acc=1.000 loss=0.003 | val acc=0.960 loss=0.112\n",
      "[rulemap][Ep 38] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.119\n",
      "[rulemap][Ep 39] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.119\n",
      "[rulemap][Ep 40] train acc=1.000 loss=0.002 | val acc=0.970 loss=0.111\n",
      "[rulemap][Ep 41] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.122\n",
      "[rulemap][Ep 42] train acc=1.000 loss=0.002 | val acc=0.970 loss=0.115\n",
      "[rulemap][Ep 43] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.127\n",
      "[rulemap][Ep 44] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.121\n",
      "[rulemap][Ep 45] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.121\n",
      "[rulemap][Ep 46] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.123\n",
      "[rulemap][Ep 47] train acc=1.000 loss=0.002 | val acc=0.960 loss=0.131\n",
      "[rulemap][Ep 48] train acc=1.000 loss=0.001 | val acc=0.960 loss=0.135\n",
      "[rulemap][Ep 49] train acc=1.000 loss=0.001 | val acc=0.960 loss=0.122\n",
      "[rulemap][Ep 50] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.120\n",
      "[rulemap][Ep 51] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.129\n",
      "[rulemap][Ep 52] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.126\n",
      "[rulemap][Ep 53] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.123\n",
      "[rulemap][Ep 54] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.121\n",
      "[rulemap][Ep 55] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.128\n",
      "[rulemap][Ep 56] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.132\n",
      "[rulemap][Ep 57] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][Ep 58] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.128\n",
      "[rulemap][Ep 59] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.126\n",
      "[rulemap][Ep 60] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.127\n",
      "[rulemap][Ep 61] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.127\n",
      "[rulemap][Ep 62] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.127\n",
      "[rulemap][Ep 63] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.128\n",
      "[rulemap][Ep 64] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.135\n",
      "[rulemap][Ep 65] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.129\n",
      "[rulemap][Ep 66] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.133\n",
      "[rulemap][Ep 67] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.132\n",
      "[rulemap][Ep 68] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.128\n",
      "[rulemap][Ep 69] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.128\n",
      "[rulemap][Ep 70] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][Ep 71] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.129\n",
      "[rulemap][Ep 72] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.129\n",
      "[rulemap][Ep 73] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.131\n",
      "[rulemap][Ep 74] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.131\n",
      "[rulemap][Ep 75] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][Ep 76] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][Ep 77] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.131\n",
      "[rulemap][Ep 78] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][Ep 79] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.131\n",
      "[rulemap][Ep 80] train acc=1.000 loss=0.001 | val acc=0.950 loss=0.130\n",
      "[rulemap][TEST] acc=0.920 loss=0.331\n",
      "[SAVE] ToMNet_Transformer_3_12_rulemap -> ../checkpoints/ToMNet_Transformer_3_12_rulemap.pt\n",
      "[DONE] ToMNet_Transformer_3_12_rulemap (best_acc={'val_acc': 0.99, 'test_acc': 0.92})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"rulemap\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNet_Transformer\")   \n",
    "TomNet_Rulemap_Model = models[\"ToMNet_Transformer_3_12_rulemap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab8edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=logic\n",
      "\n",
      "=== Training mode = logic ===\n",
      "[logic][Ep 01] train acc=0.676 loss=1.126 | val acc=0.820 loss=0.938\n",
      "[logic][Ep 02] train acc=0.797 loss=0.697 | val acc=0.840 loss=0.569\n",
      "[logic][Ep 03] train acc=0.830 loss=0.454 | val acc=0.890 loss=0.383\n",
      "[logic][Ep 04] train acc=0.885 loss=0.336 | val acc=0.910 loss=0.297\n",
      "[logic][Ep 05] train acc=0.919 loss=0.274 | val acc=0.930 loss=0.248\n",
      "[logic][Ep 06] train acc=0.922 loss=0.235 | val acc=0.940 loss=0.210\n",
      "[logic][Ep 07] train acc=0.935 loss=0.205 | val acc=0.950 loss=0.181\n",
      "[logic][Ep 08] train acc=0.954 loss=0.170 | val acc=0.960 loss=0.156\n",
      "[logic][Ep 09] train acc=0.961 loss=0.150 | val acc=0.970 loss=0.133\n",
      "[logic][Ep 10] train acc=0.968 loss=0.131 | val acc=0.970 loss=0.115\n",
      "[logic][Ep 11] train acc=0.984 loss=0.111 | val acc=0.970 loss=0.101\n",
      "[logic][Ep 12] train acc=0.988 loss=0.093 | val acc=0.990 loss=0.089\n",
      "[logic][Ep 13] train acc=0.988 loss=0.083 | val acc=0.990 loss=0.078\n",
      "[logic][Ep 14] train acc=0.991 loss=0.065 | val acc=0.990 loss=0.074\n",
      "[logic][Ep 15] train acc=0.995 loss=0.051 | val acc=0.990 loss=0.070\n",
      "[logic][Ep 16] train acc=0.995 loss=0.043 | val acc=0.980 loss=0.067\n",
      "[logic][Ep 17] train acc=1.000 loss=0.036 | val acc=0.990 loss=0.064\n",
      "[logic][Ep 18] train acc=1.000 loss=0.026 | val acc=0.980 loss=0.063\n",
      "[logic][Ep 19] train acc=1.000 loss=0.023 | val acc=0.990 loss=0.052\n",
      "[logic][Ep 20] train acc=1.000 loss=0.019 | val acc=0.970 loss=0.076\n",
      "[logic][Ep 21] train acc=1.000 loss=0.016 | val acc=0.980 loss=0.061\n",
      "[logic][Ep 22] train acc=1.000 loss=0.013 | val acc=0.970 loss=0.056\n",
      "[logic][Ep 23] train acc=1.000 loss=0.011 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 24] train acc=1.000 loss=0.009 | val acc=0.980 loss=0.054\n",
      "[logic][Ep 25] train acc=1.000 loss=0.007 | val acc=0.990 loss=0.054\n",
      "[logic][Ep 26] train acc=1.000 loss=0.007 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 27] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 28] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.055\n",
      "[logic][Ep 29] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.051\n",
      "[logic][Ep 30] train acc=1.000 loss=0.004 | val acc=0.970 loss=0.056\n",
      "[logic][Ep 31] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 32] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 33] train acc=1.000 loss=0.003 | val acc=0.960 loss=0.060\n",
      "[logic][Ep 34] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 35] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 36] train acc=1.000 loss=0.003 | val acc=0.970 loss=0.057\n",
      "[logic][Ep 37] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.052\n",
      "[logic][Ep 38] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 39] train acc=1.000 loss=0.002 | val acc=0.970 loss=0.057\n",
      "[logic][Ep 40] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 41] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.052\n",
      "[logic][Ep 42] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.047\n",
      "[logic][Ep 43] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.053\n",
      "[logic][Ep 44] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.052\n",
      "[logic][Ep 45] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.051\n",
      "[logic][Ep 46] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 47] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 48] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 49] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.048\n",
      "[logic][Ep 50] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.049\n",
      "[logic][Ep 51] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.060\n",
      "[logic][Ep 52] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.047\n",
      "[logic][Ep 53] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 54] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.047\n",
      "[logic][Ep 55] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.049\n",
      "[logic][Ep 56] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.049\n",
      "[logic][Ep 57] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.051\n",
      "[logic][Ep 58] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.048\n",
      "[logic][Ep 59] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.050\n",
      "[logic][Ep 60] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.052\n",
      "[logic][Ep 61] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.047\n",
      "[logic][Ep 62] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.051\n",
      "[logic][Ep 63] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.052\n",
      "[logic][Ep 64] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.051\n",
      "[logic][Ep 65] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.049\n",
      "[logic][Ep 66] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 67] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 68] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 69] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.051\n",
      "[logic][Ep 70] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.052\n",
      "[logic][Ep 71] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 72] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 73] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 74] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 75] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 76] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 77] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.051\n",
      "[logic][Ep 78] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 79] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][Ep 80] train acc=1.000 loss=0.001 | val acc=0.970 loss=0.050\n",
      "[logic][TEST] acc=0.930 loss=0.145\n",
      "[SAVE] ToMNet_Transformer_3_12_logic -> ../checkpoints/ToMNet_Transformer_3_12_logic.pt\n",
      "[DONE] ToMNet_Transformer_3_12_logic (best_acc={'val_acc': 0.99, 'test_acc': 0.93})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"logic\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNet_Transformer\")   \n",
    "TomNet_Logic_Model = models[\"ToMNet_Transformer_3_12_logic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f78e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case1\n",
      "\n",
      "=== Training mode = intermediate_case1 ===\n",
      "[intermediate_case1][Ep 01] train acc=0.328 loss=1.300 | val acc=0.330 loss=1.264\n",
      "[intermediate_case1][Ep 02] train acc=0.347 loss=1.195 | val acc=0.350 loss=1.224\n",
      "[intermediate_case1][Ep 03] train acc=0.393 loss=1.160 | val acc=0.360 loss=1.215\n",
      "[intermediate_case1][Ep 04] train acc=0.438 loss=1.135 | val acc=0.360 loss=1.206\n",
      "[intermediate_case1][Ep 05] train acc=0.430 loss=1.119 | val acc=0.430 loss=1.201\n",
      "[intermediate_case1][Ep 06] train acc=0.454 loss=1.106 | val acc=0.420 loss=1.197\n",
      "[intermediate_case1][Ep 07] train acc=0.477 loss=1.089 | val acc=0.370 loss=1.193\n",
      "[intermediate_case1][Ep 08] train acc=0.479 loss=1.083 | val acc=0.390 loss=1.188\n",
      "[intermediate_case1][Ep 09] train acc=0.477 loss=1.081 | val acc=0.360 loss=1.188\n",
      "[intermediate_case1][Ep 10] train acc=0.489 loss=1.071 | val acc=0.400 loss=1.187\n",
      "[intermediate_case1][Ep 11] train acc=0.482 loss=1.056 | val acc=0.370 loss=1.194\n",
      "[intermediate_case1][Ep 12] train acc=0.496 loss=1.053 | val acc=0.380 loss=1.197\n",
      "[intermediate_case1][Ep 13] train acc=0.517 loss=1.025 | val acc=0.380 loss=1.196\n",
      "[intermediate_case1][Ep 14] train acc=0.527 loss=1.006 | val acc=0.410 loss=1.206\n",
      "[intermediate_case1][Ep 15] train acc=0.551 loss=0.980 | val acc=0.410 loss=1.219\n",
      "[intermediate_case1][Ep 16] train acc=0.604 loss=0.929 | val acc=0.360 loss=1.237\n",
      "[intermediate_case1][Ep 17] train acc=0.640 loss=0.875 | val acc=0.360 loss=1.231\n",
      "[intermediate_case1][Ep 18] train acc=0.723 loss=0.792 | val acc=0.300 loss=1.287\n",
      "[intermediate_case1][Ep 19] train acc=0.749 loss=0.730 | val acc=0.300 loss=1.398\n",
      "[intermediate_case1][Ep 20] train acc=0.797 loss=0.647 | val acc=0.350 loss=1.475\n",
      "[intermediate_case1][Ep 21] train acc=0.826 loss=0.590 | val acc=0.300 loss=1.428\n",
      "[intermediate_case1][Ep 22] train acc=0.873 loss=0.510 | val acc=0.330 loss=1.606\n",
      "[intermediate_case1][Ep 23] train acc=0.882 loss=0.449 | val acc=0.300 loss=1.516\n",
      "[intermediate_case1][Ep 24] train acc=0.900 loss=0.397 | val acc=0.330 loss=1.772\n",
      "[intermediate_case1][Ep 25] train acc=0.934 loss=0.338 | val acc=0.280 loss=1.698\n",
      "[intermediate_case1][Ep 26] train acc=0.940 loss=0.293 | val acc=0.340 loss=1.771\n",
      "[intermediate_case1][Ep 27] train acc=0.948 loss=0.248 | val acc=0.250 loss=1.938\n",
      "[intermediate_case1][Ep 28] train acc=0.970 loss=0.212 | val acc=0.330 loss=2.223\n",
      "[intermediate_case1][Ep 29] train acc=0.975 loss=0.178 | val acc=0.300 loss=2.334\n",
      "[intermediate_case1][Ep 30] train acc=0.983 loss=0.152 | val acc=0.260 loss=2.318\n",
      "[intermediate_case1][Ep 31] train acc=0.985 loss=0.122 | val acc=0.300 loss=2.110\n",
      "[intermediate_case1][Ep 32] train acc=0.994 loss=0.098 | val acc=0.270 loss=2.597\n",
      "[intermediate_case1][Ep 33] train acc=0.993 loss=0.079 | val acc=0.250 loss=2.844\n",
      "[intermediate_case1][Ep 34] train acc=1.000 loss=0.061 | val acc=0.270 loss=2.720\n",
      "[intermediate_case1][Ep 35] train acc=1.000 loss=0.051 | val acc=0.270 loss=2.790\n",
      "[intermediate_case1][Ep 36] train acc=1.000 loss=0.041 | val acc=0.290 loss=3.071\n",
      "[intermediate_case1][Ep 37] train acc=1.000 loss=0.029 | val acc=0.260 loss=3.060\n",
      "[intermediate_case1][Ep 38] train acc=1.000 loss=0.028 | val acc=0.250 loss=3.287\n",
      "[intermediate_case1][Ep 39] train acc=1.000 loss=0.021 | val acc=0.240 loss=3.476\n",
      "[intermediate_case1][Ep 40] train acc=1.000 loss=0.016 | val acc=0.300 loss=3.089\n",
      "[intermediate_case1][Ep 41] train acc=1.000 loss=0.015 | val acc=0.240 loss=3.545\n",
      "[intermediate_case1][Ep 42] train acc=1.000 loss=0.012 | val acc=0.240 loss=3.617\n",
      "[intermediate_case1][Ep 43] train acc=1.000 loss=0.010 | val acc=0.260 loss=3.540\n",
      "[intermediate_case1][Ep 44] train acc=1.000 loss=0.008 | val acc=0.240 loss=3.950\n",
      "[intermediate_case1][Ep 45] train acc=1.000 loss=0.007 | val acc=0.250 loss=3.677\n",
      "[intermediate_case1][Ep 46] train acc=1.000 loss=0.006 | val acc=0.290 loss=3.683\n",
      "[intermediate_case1][Ep 47] train acc=1.000 loss=0.005 | val acc=0.250 loss=3.952\n",
      "[intermediate_case1][Ep 48] train acc=1.000 loss=0.004 | val acc=0.220 loss=3.991\n",
      "[intermediate_case1][Ep 49] train acc=1.000 loss=0.004 | val acc=0.250 loss=3.986\n",
      "[intermediate_case1][Ep 50] train acc=1.000 loss=0.004 | val acc=0.240 loss=3.948\n",
      "[intermediate_case1][Ep 51] train acc=1.000 loss=0.003 | val acc=0.230 loss=3.922\n",
      "[intermediate_case1][Ep 52] train acc=1.000 loss=0.003 | val acc=0.210 loss=4.110\n",
      "[intermediate_case1][Ep 53] train acc=1.000 loss=0.003 | val acc=0.220 loss=3.980\n",
      "[intermediate_case1][Ep 54] train acc=1.000 loss=0.003 | val acc=0.210 loss=4.084\n",
      "[intermediate_case1][Ep 55] train acc=1.000 loss=0.003 | val acc=0.230 loss=4.115\n",
      "[intermediate_case1][Ep 56] train acc=1.000 loss=0.003 | val acc=0.220 loss=4.052\n",
      "[intermediate_case1][Ep 57] train acc=1.000 loss=0.003 | val acc=0.240 loss=4.081\n",
      "[intermediate_case1][Ep 58] train acc=1.000 loss=0.002 | val acc=0.220 loss=4.074\n",
      "[intermediate_case1][Ep 59] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.146\n",
      "[intermediate_case1][Ep 60] train acc=1.000 loss=0.002 | val acc=0.260 loss=4.178\n",
      "[intermediate_case1][Ep 61] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.164\n",
      "[intermediate_case1][Ep 62] train acc=1.000 loss=0.002 | val acc=0.220 loss=4.175\n",
      "[intermediate_case1][Ep 63] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.179\n",
      "[intermediate_case1][Ep 64] train acc=1.000 loss=0.002 | val acc=0.220 loss=4.182\n",
      "[intermediate_case1][Ep 65] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.199\n",
      "[intermediate_case1][Ep 66] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.201\n",
      "[intermediate_case1][Ep 67] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.200\n",
      "[intermediate_case1][Ep 68] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.193\n",
      "[intermediate_case1][Ep 69] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.197\n",
      "[intermediate_case1][Ep 70] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.194\n",
      "[intermediate_case1][Ep 71] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.195\n",
      "[intermediate_case1][Ep 72] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.208\n",
      "[intermediate_case1][Ep 73] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.228\n",
      "[intermediate_case1][Ep 74] train acc=1.000 loss=0.002 | val acc=0.240 loss=4.218\n",
      "[intermediate_case1][Ep 75] train acc=1.000 loss=0.002 | val acc=0.220 loss=4.228\n",
      "[intermediate_case1][Ep 76] train acc=1.000 loss=0.002 | val acc=0.220 loss=4.227\n",
      "[intermediate_case1][Ep 77] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.222\n",
      "[intermediate_case1][Ep 78] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.219\n",
      "[intermediate_case1][Ep 79] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.228\n",
      "[intermediate_case1][Ep 80] train acc=1.000 loss=0.002 | val acc=0.230 loss=4.214\n",
      "[intermediate_case1][TEST] acc=0.350 loss=3.067\n",
      "[SAVE] ToMNet_Transformer_3_12_intermediate_case1 -> ../checkpoints/ToMNet_Transformer_3_12_intermediate_case1.pt\n",
      "[DONE] ToMNet_Transformer_3_12_intermediate_case1 (best_acc={'val_acc': 0.43, 'test_acc': 0.35})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case1\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNet_Transformer\")   \n",
    "TomNet_Intermediate1_Model = models[\"ToMNet_Transformer_3_12_intermediate_case1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891af56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case2\n",
      "\n",
      "=== Training mode = intermediate_case2 ===\n",
      "[intermediate_case2][Ep 01] train acc=0.326 loss=1.300 | val acc=0.400 loss=1.259\n",
      "[intermediate_case2][Ep 02] train acc=0.417 loss=1.196 | val acc=0.340 loss=1.191\n",
      "[intermediate_case2][Ep 03] train acc=0.415 loss=1.159 | val acc=0.320 loss=1.169\n",
      "[intermediate_case2][Ep 04] train acc=0.424 loss=1.138 | val acc=0.340 loss=1.171\n",
      "[intermediate_case2][Ep 05] train acc=0.454 loss=1.122 | val acc=0.340 loss=1.177\n",
      "[intermediate_case2][Ep 06] train acc=0.465 loss=1.108 | val acc=0.340 loss=1.181\n",
      "[intermediate_case2][Ep 07] train acc=0.484 loss=1.096 | val acc=0.380 loss=1.191\n",
      "[intermediate_case2][Ep 08] train acc=0.481 loss=1.076 | val acc=0.400 loss=1.189\n",
      "[intermediate_case2][Ep 09] train acc=0.492 loss=1.076 | val acc=0.380 loss=1.193\n",
      "[intermediate_case2][Ep 10] train acc=0.482 loss=1.074 | val acc=0.360 loss=1.201\n",
      "[intermediate_case2][Ep 11] train acc=0.491 loss=1.046 | val acc=0.360 loss=1.209\n",
      "[intermediate_case2][Ep 12] train acc=0.491 loss=1.049 | val acc=0.410 loss=1.205\n",
      "[intermediate_case2][Ep 13] train acc=0.524 loss=1.029 | val acc=0.380 loss=1.221\n",
      "[intermediate_case2][Ep 14] train acc=0.546 loss=1.011 | val acc=0.400 loss=1.228\n",
      "[intermediate_case2][Ep 15] train acc=0.556 loss=0.990 | val acc=0.370 loss=1.231\n",
      "[intermediate_case2][Ep 16] train acc=0.581 loss=0.972 | val acc=0.370 loss=1.231\n",
      "[intermediate_case2][Ep 17] train acc=0.611 loss=0.940 | val acc=0.350 loss=1.247\n",
      "[intermediate_case2][Ep 18] train acc=0.630 loss=0.900 | val acc=0.370 loss=1.271\n",
      "[intermediate_case2][Ep 19] train acc=0.667 loss=0.847 | val acc=0.360 loss=1.278\n",
      "[intermediate_case2][Ep 20] train acc=0.699 loss=0.789 | val acc=0.330 loss=1.347\n",
      "[intermediate_case2][Ep 21] train acc=0.734 loss=0.733 | val acc=0.350 loss=1.384\n",
      "[intermediate_case2][Ep 22] train acc=0.760 loss=0.688 | val acc=0.370 loss=1.451\n",
      "[intermediate_case2][Ep 23] train acc=0.791 loss=0.622 | val acc=0.330 loss=1.562\n",
      "[intermediate_case2][Ep 24] train acc=0.825 loss=0.566 | val acc=0.280 loss=1.533\n",
      "[intermediate_case2][Ep 25] train acc=0.848 loss=0.526 | val acc=0.320 loss=1.611\n",
      "[intermediate_case2][Ep 26] train acc=0.860 loss=0.488 | val acc=0.380 loss=1.637\n",
      "[intermediate_case2][Ep 27] train acc=0.874 loss=0.445 | val acc=0.310 loss=1.760\n",
      "[intermediate_case2][Ep 28] train acc=0.892 loss=0.403 | val acc=0.280 loss=1.898\n",
      "[intermediate_case2][Ep 29] train acc=0.897 loss=0.370 | val acc=0.360 loss=1.773\n",
      "[intermediate_case2][Ep 30] train acc=0.921 loss=0.328 | val acc=0.330 loss=1.891\n",
      "[intermediate_case2][Ep 31] train acc=0.922 loss=0.298 | val acc=0.300 loss=1.958\n",
      "[intermediate_case2][Ep 32] train acc=0.931 loss=0.275 | val acc=0.310 loss=2.121\n",
      "[intermediate_case2][Ep 33] train acc=0.949 loss=0.234 | val acc=0.310 loss=2.119\n",
      "[intermediate_case2][Ep 34] train acc=0.954 loss=0.208 | val acc=0.350 loss=2.123\n",
      "[intermediate_case2][Ep 35] train acc=0.970 loss=0.180 | val acc=0.300 loss=2.201\n",
      "[intermediate_case2][Ep 36] train acc=0.979 loss=0.155 | val acc=0.340 loss=2.339\n",
      "[intermediate_case2][Ep 37] train acc=0.985 loss=0.137 | val acc=0.310 loss=2.417\n",
      "[intermediate_case2][Ep 38] train acc=0.980 loss=0.119 | val acc=0.380 loss=2.608\n",
      "[intermediate_case2][Ep 39] train acc=0.990 loss=0.099 | val acc=0.300 loss=2.587\n",
      "[intermediate_case2][Ep 40] train acc=0.995 loss=0.084 | val acc=0.270 loss=2.791\n",
      "[intermediate_case2][Ep 41] train acc=0.995 loss=0.074 | val acc=0.310 loss=2.612\n",
      "[intermediate_case2][Ep 42] train acc=0.998 loss=0.062 | val acc=0.320 loss=2.780\n",
      "[intermediate_case2][Ep 43] train acc=0.998 loss=0.054 | val acc=0.300 loss=2.836\n",
      "[intermediate_case2][Ep 44] train acc=0.994 loss=0.046 | val acc=0.260 loss=3.079\n",
      "[intermediate_case2][Ep 45] train acc=0.998 loss=0.043 | val acc=0.330 loss=2.935\n",
      "[intermediate_case2][Ep 46] train acc=0.999 loss=0.039 | val acc=0.320 loss=3.073\n",
      "[intermediate_case2][Ep 47] train acc=0.999 loss=0.028 | val acc=0.320 loss=3.131\n",
      "[intermediate_case2][Ep 48] train acc=1.000 loss=0.028 | val acc=0.300 loss=3.216\n",
      "[intermediate_case2][Ep 49] train acc=0.999 loss=0.023 | val acc=0.330 loss=3.289\n",
      "[intermediate_case2][Ep 50] train acc=1.000 loss=0.020 | val acc=0.290 loss=3.339\n",
      "[intermediate_case2][Ep 51] train acc=0.998 loss=0.019 | val acc=0.330 loss=3.305\n",
      "[intermediate_case2][Ep 52] train acc=0.996 loss=0.016 | val acc=0.340 loss=3.363\n",
      "[intermediate_case2][Ep 53] train acc=0.996 loss=0.016 | val acc=0.330 loss=3.365\n",
      "[intermediate_case2][Ep 54] train acc=0.999 loss=0.013 | val acc=0.310 loss=3.599\n",
      "[intermediate_case2][Ep 55] train acc=0.999 loss=0.013 | val acc=0.320 loss=3.498\n",
      "[intermediate_case2][Ep 56] train acc=0.998 loss=0.012 | val acc=0.300 loss=3.586\n",
      "[intermediate_case2][Ep 57] train acc=0.998 loss=0.010 | val acc=0.350 loss=3.685\n",
      "[intermediate_case2][Ep 58] train acc=0.999 loss=0.009 | val acc=0.320 loss=3.607\n",
      "[intermediate_case2][Ep 59] train acc=0.996 loss=0.011 | val acc=0.290 loss=3.627\n",
      "[intermediate_case2][Ep 60] train acc=0.999 loss=0.009 | val acc=0.310 loss=3.694\n",
      "[intermediate_case2][Ep 61] train acc=0.999 loss=0.009 | val acc=0.320 loss=3.773\n",
      "[intermediate_case2][Ep 62] train acc=0.998 loss=0.008 | val acc=0.320 loss=3.771\n",
      "[intermediate_case2][Ep 63] train acc=0.998 loss=0.008 | val acc=0.360 loss=3.738\n",
      "[intermediate_case2][Ep 64] train acc=0.998 loss=0.008 | val acc=0.340 loss=3.739\n",
      "[intermediate_case2][Ep 65] train acc=0.998 loss=0.008 | val acc=0.320 loss=3.738\n",
      "[intermediate_case2][Ep 66] train acc=0.998 loss=0.009 | val acc=0.340 loss=3.742\n",
      "[intermediate_case2][Ep 67] train acc=0.999 loss=0.007 | val acc=0.330 loss=3.765\n",
      "[intermediate_case2][Ep 68] train acc=0.998 loss=0.008 | val acc=0.340 loss=3.772\n",
      "[intermediate_case2][Ep 69] train acc=0.999 loss=0.008 | val acc=0.330 loss=3.740\n",
      "[intermediate_case2][Ep 70] train acc=0.998 loss=0.008 | val acc=0.350 loss=3.761\n",
      "[intermediate_case2][Ep 71] train acc=0.998 loss=0.009 | val acc=0.340 loss=3.762\n",
      "[intermediate_case2][Ep 72] train acc=0.998 loss=0.008 | val acc=0.330 loss=3.763\n",
      "[intermediate_case2][Ep 73] train acc=0.999 loss=0.007 | val acc=0.330 loss=3.779\n",
      "[intermediate_case2][Ep 74] train acc=0.998 loss=0.008 | val acc=0.350 loss=3.792\n",
      "[intermediate_case2][Ep 75] train acc=1.000 loss=0.007 | val acc=0.340 loss=3.794\n",
      "[intermediate_case2][Ep 76] train acc=0.998 loss=0.007 | val acc=0.340 loss=3.789\n",
      "[intermediate_case2][Ep 77] train acc=0.998 loss=0.008 | val acc=0.350 loss=3.800\n",
      "[intermediate_case2][Ep 78] train acc=1.000 loss=0.007 | val acc=0.350 loss=3.795\n",
      "[intermediate_case2][Ep 79] train acc=0.996 loss=0.009 | val acc=0.350 loss=3.796\n",
      "[intermediate_case2][Ep 80] train acc=0.999 loss=0.007 | val acc=0.350 loss=3.800\n",
      "[intermediate_case2][TEST] acc=0.450 loss=3.069\n",
      "[SAVE] ToMNet_Transformer_3_12_intermediate_case2 -> ../checkpoints/ToMNet_Transformer_3_12_intermediate_case2.pt\n",
      "[DONE] ToMNet_Transformer_3_12_intermediate_case2 (best_acc={'val_acc': 0.41, 'test_acc': 0.45})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"..\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case2\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "models = run(cfg, kind=\"ToMNet_Transformer\")   \n",
    "TomNet_Intermediate2_Model = models[\"ToMNet_Transformer_3_12_intermediate_case2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c90e7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23af09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluate import evaluate_model_performance\n",
    "\n",
    "def forward_tomnet(m: nn.Module, b: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    _ = m(b[\"grid_seq\"], b[\"tmask\"], b[\"choices_ids\"])\n",
    "    return _  # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb994ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet_Transformer_3_12_random model evaluation ===\n",
      "accuracy: 0.4000\n",
      "precision: 0.4845\n",
      "recall rate: 0.4000\n",
      "F1 score: 0.4137\n",
      "ROC AUC: 0.7123\n",
      "R2 Score: -0.4060\n",
      "Prediction Entropy: 0.3449\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 133,230,592\n",
      "Energy per sample: 6.13e-04 J\n",
      "Energy per accuracy unit: 1.53e-03 J/acc\n",
      "MACs per accuracy unit: 333,076,480 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"random\")\n",
    "\n",
    "ds = GridSeqDataset(cfg.data_root, cfg.sim_name, \"val\", \"random\")\n",
    "one_sample = ds[0][\"grid_seq\"]\n",
    "input_hw = one_sample.shape[2:4]\n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet,\n",
    "    energy_forward_fn=forward_tomnet)\n",
    "\n",
    "print(\"=== ToMNet_Transformer_3_12_random model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e895520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_rulemap model evaluation ===\n",
      "accuracy: 0.9500\n",
      "precision: 0.9510\n",
      "recall rate: 0.9500\n",
      "F1 score: 0.9503\n",
      "ROC AUC: 0.9975\n",
      "R2 Score: 0.9009\n",
      "Prediction Entropy: 0.0554\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 131,706,880\n",
      "Energy per sample: 6.06e-04 J\n",
      "Energy per accuracy unit: 6.38e-04 J/acc\n",
      "MACs per accuracy unit: 138,638,821 MACs/acc\n"
     ]
    }
   ],
   "source": [
    "# rulemap\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"rulemap\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Rulemap_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet,\n",
    "    energy_forward_fn=forward_tomnet)\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_rulemap model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f29957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_logic model evaluation ===\n",
      "accuracy: 0.9700\n",
      "precision: 0.9705\n",
      "recall rate: 0.9700\n",
      "F1 score: 0.9701\n",
      "ROC AUC: 0.9995\n",
      "R2 Score: 0.9697\n",
      "Prediction Entropy: 0.0774\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 131,674,112\n",
      "Energy per sample: 6.06e-04 J\n",
      "Energy per accuracy unit: 6.24e-04 J/acc\n",
      "MACs per accuracy unit: 135,746,507 MACs/acc\n"
     ]
    }
   ],
   "source": [
    "# logic\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"logic\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Logic_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet,\n",
    "    energy_forward_fn=forward_tomnet)\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_logic model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb22908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_intermediate_case1 model evaluation ===\n",
      "accuracy: 0.2300\n",
      "precision: 0.3100\n",
      "recall rate: 0.2300\n",
      "F1 score: 0.2553\n",
      "ROC AUC: 0.5454\n",
      "R2 Score: -0.7368\n",
      "Prediction Entropy: 0.3855\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 133,230,592\n",
      "Energy per sample: 6.13e-04 J\n",
      "Energy per accuracy unit: 2.66e-03 J/acc\n",
      "MACs per accuracy unit: 579,263,443 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# intermediate_case1\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"intermediate_case1\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Intermediate1_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet,\n",
    "    energy_forward_fn=forward_tomnet)\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_intermediate_case1 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "329df7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNetSNN_3_12_intermediate_case2 model evaluation ===\n",
      "accuracy: 0.3500\n",
      "precision: 0.4400\n",
      "recall rate: 0.3500\n",
      "F1 score: 0.3696\n",
      "ROC AUC: 0.6114\n",
      "R2 Score: -0.5414\n",
      "Prediction Entropy: 0.3304\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 133,230,592\n",
      "Energy per sample: 6.13e-04 J\n",
      "Energy per accuracy unit: 1.75e-03 J/acc\n",
      "MACs per accuracy unit: 380,658,834 MACs/acc\n"
     ]
    }
   ],
   "source": [
    "# intermediate_case2\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"intermediate_case2\")\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Intermediate2_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet,\n",
    "    energy_forward_fn=forward_tomnet)\n",
    "\n",
    "print(\"=== ToMNetSNN_3_12_intermediate_case2 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fb35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
