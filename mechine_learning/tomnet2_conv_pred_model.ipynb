{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/gen_grid.py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collate_grid import collate_grid\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/Jer_ry/Desktop/script_tom\")\n",
    "SIM_NAME     = \"3_12\"\n",
    "SPLITS       = [\"training\", \"validation\", \"testing\"]\n",
    "H = W = 12\n",
    "N_AGENTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39af556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_fixed_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Residual Block。\n",
    "      in:  [B, ch, H, W]\n",
    "      out: [B, ch, H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, ch: int):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.c2 = nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.act(self.bn1(self.c1(x)))\n",
    "        h = self.bn2(self.c2(h))\n",
    "        return self.act(x + h)\n",
    "\n",
    "\n",
    "class CharacterNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Character Network: from τ_j compute e_char in all sequences\n",
    "\n",
    "    Input:\n",
    "      - grid_seq: [B, T, H, W, C]\n",
    "      - tmask:    [B, T]  True=valiation\n",
    "\n",
    "    Output:\n",
    "      - e_char:   [B, e_char_dim]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        stem_ch: int = 32,\n",
    "        n_blocks: int = 4,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        n_layers: int = 2,\n",
    "        e_char_dim: int = 64,\n",
    "        use_cls_token: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_cls_token = use_cls_token\n",
    "\n",
    "        # per-frame CNN\n",
    "        self.stem1x1 = nn.Conv2d(in_ch, stem_ch, 1)\n",
    "        self.c3 = nn.Conv2d(stem_ch, stem_ch, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(stem_ch)\n",
    "        self.blocks = nn.Sequential(*[BasicBlock(stem_ch) for _ in range(n_blocks)])\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # -> [B*T, stem_ch, 1, 1]\n",
    "\n",
    "        # temporal Transformer\n",
    "        self.in_proj = nn.Linear(stem_ch, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.to_e = nn.Linear(d_model, e_char_dim)\n",
    "\n",
    "    def forward(self, grid_seq: torch.Tensor, tmask: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, H, W, C = grid_seq.shape\n",
    "\n",
    "        x = grid_seq.permute(0, 1, 4, 2, 3).contiguous()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "\n",
    "        x = self.stem1x1(x)\n",
    "        x = self.act(self.bn3(self.c3(x)))\n",
    "        x = self.blocks(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  # [B*T, stem_ch]\n",
    "        x = x.view(B, T, -1)  # [B,T,stem_ch]\n",
    "\n",
    "        # 時序 Transformer\n",
    "        h = self.in_proj(x)                  # [B,T,d_model]\n",
    "        key_pad = ~tmask                     # True=padding\n",
    "        h = self.encoder(h, src_key_padding_mask=key_pad)\n",
    "        h = self.ln(h)\n",
    "\n",
    "        # 時間維度聚合\n",
    "        if self.use_cls_token:\n",
    "            last_idx = tmask.sum(dim=1) - 1        # [B]\n",
    "            e_src = h.gather(\n",
    "                1, last_idx.view(B, 1, 1).expand(B, 1, h.size(-1))\n",
    "            ).squeeze(1)                            # [B,d_model]\n",
    "        else:\n",
    "            m = tmask.unsqueeze(-1).float()\n",
    "            e_src = (h * m).sum(dim=1) / m.sum(dim=1).clamp_min(1.0)\n",
    "\n",
    "        e_char = self.to_e(e_src)                   # [B,e_char_dim]\n",
    "        return e_char\n",
    "\n",
    "\n",
    "class OptionScoringHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Four-choice head:\n",
    "\n",
    "    - e_char + τ_k first frame q_k(0) → ctx\n",
    "    - Each option is a set of cell ids (N cells) → embedding + masked mean → ch_vec\n",
    "    - Score each (ctx, ch_vec) pair\n",
    "\n",
    "    Input:\n",
    "      e_char:       [B, e_char_dim]\n",
    "      q0:           [B, H, W, C]\n",
    "      choices_ids:  [B, 4, Nmax]\n",
    "      choices_mask: [B, 4, Nmax]  True=valid cell\n",
    "\n",
    "    Output:\n",
    "      logits:       [B, 4]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        stem_ch: int = 32,\n",
    "        n_blocks: int = 3,\n",
    "        e_char_dim: int = 64,\n",
    "        cell_vocab: int = 24 * 24,\n",
    "        choice_emb_dim: int = 128,\n",
    "        d_ctx: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # CNN for q_k(0)\n",
    "        self.query_stem = nn.Conv2d(in_ch, stem_ch, 1)\n",
    "        self.query_c3 = nn.Conv2d(stem_ch, stem_ch, 3, padding=1)\n",
    "        self.query_bn3 = nn.BatchNorm2d(stem_ch)\n",
    "        self.blocks = nn.Sequential(*[BasicBlock(stem_ch) for _ in range(n_blocks)])\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # ctx\n",
    "        self.proj_ctx = nn.Sequential(\n",
    "            nn.Linear(stem_ch + e_char_dim, d_ctx),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(d_ctx),\n",
    "        )\n",
    "\n",
    "        # cell embedding\n",
    "        self.choice_emb = nn.Embedding(cell_vocab, choice_emb_dim)\n",
    "        self.proj_choice = nn.Sequential(\n",
    "            nn.Linear(choice_emb_dim, d_ctx),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(d_ctx),\n",
    "        )\n",
    "\n",
    "        # Dual-tower + scoring\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(d_ctx * 4, d_ctx),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ctx, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        e_char: torch.Tensor,\n",
    "        q0: torch.Tensor,\n",
    "        choices_ids: torch.Tensor,\n",
    "        choices_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        B, H, W, C = q0.shape\n",
    "\n",
    "        # q_k 第一幀\n",
    "        q = q0.permute(0, 3, 1, 2).contiguous()   # [B,C,H,W]\n",
    "        q = self.query_stem(q)\n",
    "        q = self.act(self.query_bn3(self.query_c3(q)))\n",
    "        q = self.blocks(q)\n",
    "        q_vec = self.gap(q).squeeze(-1).squeeze(-1)  # [B,stem_ch]\n",
    "\n",
    "        # ctx\n",
    "        ctx = self.proj_ctx(torch.cat([q_vec, e_char], dim=-1))  # [B,d_ctx]\n",
    "\n",
    "        # 選項嵌入 + masked mean\n",
    "        B2, K, Nmax = choices_ids.shape  # K=4\n",
    "        emb = self.choice_emb(choices_ids)           # [B,4,Nmax,D_emb]\n",
    "        m = choices_mask.unsqueeze(-1).float()       # [B,4,Nmax,1]\n",
    "        ch_vec = (emb * m).sum(dim=2) / m.sum(dim=2).clamp_min(1.0)  # [B,4,D_emb]\n",
    "        ch_vec = self.proj_choice(ch_vec)           # [B,4,d_ctx]\n",
    "\n",
    "        # 雙塔打分\n",
    "        ctx4 = ctx.unsqueeze(1).expand_as(ch_vec)   # [B,4,d_ctx]\n",
    "        feat = torch.cat(\n",
    "            [ctx4, ch_vec, torch.abs(ctx4 - ch_vec), ctx4 * ch_vec],\n",
    "            dim=-1,\n",
    "        )                                           # [B,4,4*d_ctx]\n",
    "        logits = self.scorer(feat).squeeze(-1)      # [B,4]\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ToMNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Overall ToMNet v2:\n",
    "      - CharacterNetwork: from τ_j compute e_char\n",
    "      - OptionScoringHead: (e_char, τ_k(0), choices_cell) → 4 logits\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        n_layers: int = 2,\n",
    "        stem_ch: int = 32,\n",
    "        n_blocks: int = 4,\n",
    "        e_char_dim: int = 64,\n",
    "        cell_vocab: int = 24 * 24,\n",
    "        dropout: float = 0.1,\n",
    "        use_cls_token: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.char_net = CharacterNetwork(\n",
    "            in_ch=in_ch,\n",
    "            stem_ch=stem_ch,\n",
    "            n_blocks=n_blocks,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            n_layers=n_layers,\n",
    "            e_char_dim=e_char_dim,\n",
    "            use_cls_token=use_cls_token,\n",
    "        )\n",
    "        self.head = OptionScoringHead(\n",
    "            in_ch=in_ch,\n",
    "            stem_ch=stem_ch,\n",
    "            n_blocks=n_blocks,\n",
    "            e_char_dim=e_char_dim,\n",
    "            cell_vocab=cell_vocab,\n",
    "            choice_emb_dim=128,\n",
    "            d_ctx=256,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        grid_seq_j: torch.Tensor,\n",
    "        tmask_j: torch.Tensor,\n",
    "        grid_seq_k: torch.Tensor,\n",
    "        tmask_k: torch.Tensor,\n",
    "        choices_ids: torch.Tensor,\n",
    "        choices_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        e_char = self.char_net(grid_seq_j, tmask_j)      # [B,e_char]\n",
    "        q0 = grid_seq_k[:, 0, ...]                       # [B,H,W,C]\n",
    "        logits = self.head(e_char, q0, choices_ids, choices_mask)  # [B,4]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e43dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_fixed_dataset.py\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "_num_pat = re.compile(r\"-?\\d+(?:\\.\\d+)?\")\n",
    "\n",
    "\n",
    "def _infer_n_agents_from_len(K: int) -> int:\n",
    "    if K % N_AGENTS != 0:\n",
    "        raise ValueError(f\" Length of trail {K} is not divisible by N_AGENTS={N_AGENTS} (possibly mixed with 5-agent data)\")\n",
    "    \n",
    "    return N_AGENTS\n",
    "\n",
    "def _read_trail_xy(step_dir: Path) -> np.ndarray:\n",
    "    trail = Path(step_dir) / \"trail.txt\"\n",
    "    if not trail.exists():\n",
    "        raise FileNotFoundError(f\"trail.txt not found at {trail}\")\n",
    "\n",
    "    try:\n",
    "        text = trail.read_text(encoding=\"utf-8\")\n",
    "        nums = list(map(float, _num_pat.findall(text)))\n",
    "        if len(nums) == 0 or len(nums) % 2 != 0:\n",
    "            raise ValueError(f\"Cannot parse into (x,y) sequence, data length: {len(nums)}\")\n",
    "\n",
    "        arr = np.asarray(nums, dtype=float).reshape(-1, 2)\n",
    "\n",
    "        # Simple check: if world coordinates (e.g. 7..18), shift back to 1..12\n",
    "        mn, mx = float(arr.min()), float(arr.max())\n",
    "        if mn >= 6.5:\n",
    "            arr = arr - 6.0\n",
    "\n",
    "        # Modification: clip to 1..12\n",
    "        arr[..., 0] = np.clip(arr[..., 0], 1.0, float(W))\n",
    "        arr[..., 1] = np.clip(arr[..., 1], 1.0, float(H))\n",
    "        return arr\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"[Error reading] {trail}: {e}\") from e\n",
    "\n",
    "def _load_grid_seq(step_dir: Path) -> np.ndarray:\n",
    "    step_dir = Path(step_dir)\n",
    "\n",
    "    # 1. Prefer loading npy\n",
    "    p = step_dir / \"grid_seq.npy\"\n",
    "    if p.exists():\n",
    "        arr = np.load(p)\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "    # 2. Load npz\n",
    "    pz = step_dir / \"grid_seq.npz\"\n",
    "    if pz.exists():\n",
    "        arr = np.load(pz)[\"grid_seq\"]\n",
    "        return arr.astype(np.float32)\n",
    "\n",
    "    # 3. [Modification] If neither exists, raise an error instead of trying to generate from txt\n",
    "    # Because the logic for generating from txt (3 channels) is incompatible with npy (18 channels)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing grid_seq.npy in {step_dir}. \"\n",
    "        \"Please run the gridizer script (F_run_gridizer.py) to generate it first.\"\n",
    "    )\n",
    "\n",
    "def _make_tmask(T: int) -> np.ndarray:\n",
    "    return np.ones(T, dtype=bool)\n",
    "\n",
    "class ToMNet2JsonlDataset(Dataset):\n",
    "    def __init__(self, split_root: Path, sim_name: str = \"sim5_24\", use_modes: Optional[List[str]] = None, seed: int = 42):\n",
    "        super().__init__()\n",
    "        base_root = Path(split_root).resolve()\n",
    "        cand1 = base_root / \"tomnet2_format.jsonl\"\n",
    "        \n",
    "        if cand1.exists():\n",
    "            self.split_root = base_root\n",
    "            self.jsonl = cand1\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"jsonl not found at {cand1}\")\n",
    "\n",
    "        self.rng = random.Random(seed)\n",
    "        self.recs: List[Dict[str, Any]] = []\n",
    "\n",
    "        for line in self.jsonl.read_text(encoding=\"utf-8\").splitlines():\n",
    "            rec = json.loads(line)\n",
    "            if use_modes and rec.get(\"mode\") not in use_modes:\n",
    "                continue\n",
    "            self.recs.append(rec)\n",
    "\n",
    "        if not self.recs:\n",
    "            raise RuntimeError(f\"No records in {self.jsonl} for modes={use_modes}\")\n",
    "\n",
    "        self.in_ch: Optional[int] = None \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.recs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        r = self.recs[idx]\n",
    "        hdir = self.split_root / r[\"history_dir\"]\n",
    "        qdir = self.split_root / r[\"query_dir\"]\n",
    "\n",
    "        # If an error occurs during loading, it will be caught by the try-except in _load_grid_seq and the path will be displayed\n",
    "        gs_j = _load_grid_seq(hdir)     \n",
    "        gs_k = _load_grid_seq(qdir)     \n",
    "        \n",
    "        Tj, Tk = gs_j.shape[0], gs_k.shape[0]\n",
    "        tm_j = _make_tmask(Tj)          \n",
    "        tm_k = _make_tmask(Tk)          \n",
    "\n",
    "        choices = r[\"choices_cell\"]    \n",
    "        N_max = max(len(x) for x in choices)\n",
    "\n",
    "        pad_choices: List[List[int]] = []\n",
    "        pad_masks:   List[List[bool]] = []\n",
    "\n",
    "        for arr in choices:\n",
    "            m = [True] * len(arr) + [False] * (N_max - len(arr))   \n",
    "            a = arr + [0] * (N_max - len(arr))                     \n",
    "            pad_choices.append(a)\n",
    "            pad_masks.append(m)\n",
    "\n",
    "        item = {\n",
    "            \"grid_seq_j\": gs_j,\n",
    "            \"tmask_j\": tm_j,\n",
    "            \"grid_seq_k\": gs_k,\n",
    "            \"tmask_k\": tm_k,\n",
    "            \"choices_ids\": np.asarray(pad_choices, dtype=np.int64),  \n",
    "            \"choices_mask\": np.asarray(pad_masks, dtype=bool),       \n",
    "            \"label_idx\": int(r[\"label_idx\"]),\n",
    "        }\n",
    "\n",
    "        if self.in_ch is None:\n",
    "            self.in_ch = int(gs_j.shape[-1])\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e18934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_fixed_collate.py\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def _pad_time(batch_np: List[np.ndarray]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    batch_np: list of (T, H, W, C_i)，C_i can be different\n",
    "    Pads to (B, T_max, H, W, C_max), with smaller C\n",
    "    \"\"\"\n",
    "    B = len(batch_np)\n",
    "    T_max = max(x.shape[0] for x in batch_np)\n",
    "    H, W = batch_np[0].shape[1], batch_np[0].shape[2]\n",
    "    C_max = max(x.shape[3] for x in batch_np)\n",
    "\n",
    "    out = np.zeros((B, T_max, H, W, C_max), dtype=np.float32)\n",
    "    for i, x in enumerate(batch_np):\n",
    "        t, h, w, c = x.shape\n",
    "        out[i, :t, :h, :w, :c] = x  # fewer channels naturally only overwrite the first c channels\n",
    "\n",
    "    return torch.from_numpy(out)\n",
    "\n",
    "def _pad_mask(batch_mask: List[np.ndarray]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    [T_i] -> [B,T_max] bool\n",
    "    \"\"\"\n",
    "    T_max = max(x.shape[0] for x in batch_mask)\n",
    "    B = len(batch_mask)\n",
    "    out = np.zeros((B, T_max), dtype=bool)\n",
    "    for i, m in enumerate(batch_mask):\n",
    "        t = m.shape[0]\n",
    "        out[i, :t] = m\n",
    "    return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "def collate_tomnet2(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    # τ_j\n",
    "    gs_j = _pad_time([b[\"grid_seq_j\"] for b in batch])\n",
    "    tm_j = _pad_mask([b[\"tmask_j\"] for b in batch])\n",
    "\n",
    "    # τ_k\n",
    "    gs_k = _pad_time([b[\"grid_seq_k\"] for b in batch])\n",
    "    tm_k = _pad_mask([b[\"tmask_k\"] for b in batch])\n",
    "\n",
    "    choices_ids = torch.stack([torch.from_numpy(b[\"choices_ids\"]) for b in batch], dim=0)\n",
    "    choices_mask = torch.stack([torch.from_numpy(b[\"choices_mask\"]) for b in batch], dim=0)\n",
    "    \n",
    "    labels = torch.tensor([b[\"label_idx\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    return dict(\n",
    "        grid_seq_j=gs_j,\n",
    "        tmask_j=tm_j,\n",
    "        grid_seq_k=gs_k,\n",
    "        tmask_k=tm_k,\n",
    "        choices_ids=choices_ids,\n",
    "        choices_mask=choices_mask,\n",
    "        labels=labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_build_tomnet2_pairs.py\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# split： training / validation / testing\n",
    "SPLITS = [\"training\", \"validation\", \"testing\"]\n",
    "\n",
    "MODE_DIRS = {\n",
    "    \"logic\":              \"logic\",\n",
    "    \"rulemap\":            \"rulemap\",\n",
    "    \"random\":             \"random\",\n",
    "    \"intermediate_case1\": \"intermediate_case1\",\n",
    "    \"intermediate_case2\": \"intermediate_case2\",\n",
    "}\n",
    "\n",
    "\n",
    "RNG_SEED = 1234\n",
    "\n",
    "NUM_DISTRACTORS = 3\n",
    "MIN_CENTROID_L2_FROM_GOLD = 5.0\n",
    "MAX_JACCARD_OVERLAP = 0.0\n",
    "MAX_RETRY_PER_DISTRACTOR = 500\n",
    "\n",
    "_num_pat = re.compile(r\"-?\\d+(?:\\.\\d+)?\")\n",
    "\n",
    "# ---------- tools---------\n",
    "\n",
    "def _step_id(p: Path) -> Optional[int]:\n",
    "    m = re.search(r\"step_plot_\\w+_(\\d+)$\", p.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def _read_trail(trail_txt: Path) -> np.ndarray:\n",
    "    text = trail_txt.read_text(encoding=\"utf-8\")\n",
    "    nums = list(map(float, _num_pat.findall(text)))\n",
    "    if len(nums) == 0 or len(nums) % 2 != 0:\n",
    "        raise ValueError(f\"{trail_txt} cannot be parsed into (x,y) sequence\")\n",
    "    arr = np.asarray(nums, dtype=float).reshape(-1, 2)\n",
    "\n",
    "    # Simple check: if world coordinates (e.g. 7..18), shift back to 1..12\n",
    "    mn, mx = float(arr.min()), float(arr.max())\n",
    "    if mn >= 6.5: \n",
    "        arr = arr - 6.0\n",
    "        \n",
    "    arr = np.clip(arr, 1.0, float(W))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _infer_n_agents_from_len(K: int) -> int:\n",
    "    \"\"\"\n",
    "    For the 3_12 dataset, only 3 agents are allowed.\n",
    "    \"\"\"\n",
    "    if K % N_AGENTS != 0:\n",
    "        raise ValueError(f\"trail length {K} is not divisible by N_AGENTS={N_AGENTS}\")\n",
    "    return N_AGENTS\n",
    "\n",
    "\n",
    "def _coord_to_cid(x: float, y: float, W: int = 12) -> int: # 修改 W 預設值\n",
    "    r = int(round(y)) - 1\n",
    "    c = int(round(x)) - 1\n",
    "    r = min(max(r, 0), W - 1)\n",
    "    c = min(max(c, 0), W - 1)\n",
    "    return r * W + c\n",
    "\n",
    "\n",
    "def _xy_to_cells(arr: np.ndarray) -> List[int]:\n",
    "    return [_coord_to_cid(float(x), float(y), W=W) for x, y in arr]\n",
    "\n",
    "\n",
    "def _centroid(arr: np.ndarray) -> Tuple[float, float]:\n",
    "    return float(arr[:, 0].mean()), float(arr[:, 1].mean())\n",
    "\n",
    "\n",
    "def _l2(p: Tuple[float, float], q: Tuple[float, float]) -> float:\n",
    "    return math.sqrt((p[0] - q[0]) ** 2 + (p[1] - q[1]) ** 2)\n",
    "\n",
    "\n",
    "def _jaccard(a: Set[int], b: Set[int]) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "\n",
    "def _sample_cluster_far_from_gold(\n",
    "    N: int,\n",
    "    gold_cells: Set[int],\n",
    "    gold_centroid: Tuple[float, float],\n",
    "    min_centroid_l2: float,\n",
    "    max_jaccard_overlap: float,\n",
    "    max_retry: int,\n",
    ") -> np.ndarray:\n",
    "    for _ in range(max_retry):\n",
    "        cells = random.sample(range(W * H), k=N)\n",
    "        if len(set(cells)) != N:\n",
    "            continue\n",
    "        if _jaccard(set(cells), gold_cells) > max_jaccard_overlap:\n",
    "            continue\n",
    "\n",
    "        xy = []\n",
    "        for cid in cells:\n",
    "            r, c = divmod(cid, W)\n",
    "            xy.append([c + 1, r + 1])   # (x,y)\n",
    "        arr = np.asarray(xy, dtype=float)\n",
    "\n",
    "        if _l2(_centroid(arr), gold_centroid) < min_centroid_l2:\n",
    "            continue\n",
    "\n",
    "        return arr\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Cant satisfy the distance / overlap constraints after MAX_RETRY attempts\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _mode_from_dir(step_dir: Path) -> str:\n",
    "    parts = [p.name.lower() for p in step_dir.parents] + [step_dir.name.lower()]\n",
    "    if \"logic\" in parts: return \"logic\"\n",
    "    if \"rulemap\" in parts: return \"rulemap\"\n",
    "    if \"random\" in parts: return \"random\"\n",
    "    if \"intermediate_case1\" in parts: return \"intermediate_case1\"\n",
    "    if \"intermediate_case2\" in parts: return \"intermediate_case2\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def _list_odd_even(split_root: Path):\n",
    "    odd, even = [], []\n",
    "    for sub in MODE_DIRS.values():\n",
    "        base = split_root / sub\n",
    "        if not base.exists():\n",
    "            continue\n",
    "        for p in base.glob(\"step_plot_*\"):\n",
    "            if not p.is_dir():\n",
    "                continue\n",
    "            sid = _step_id(p)\n",
    "            if sid is None:\n",
    "                continue\n",
    "            (odd if sid % 2 == 1 else even).append(p)\n",
    "    odd.sort(key=lambda x: _step_id(x))\n",
    "    even.sort(key=lambda x: _step_id(x))\n",
    "    return odd, even\n",
    "\n",
    "# ---------- build one split ----------\n",
    "\n",
    "def build_one_split(split_root: Path) -> int:\n",
    "    split_root = split_root.resolve()\n",
    "    print(f\"[INFO] build split at {split_root}\")\n",
    "\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n",
    "\n",
    "    out_jsonl = split_root / \"tomnet2_format.jsonl\"\n",
    "    if out_jsonl.exists():\n",
    "        out_jsonl.unlink() \n",
    "\n",
    "    out_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    odd_dirs, even_dirs = _list_odd_even(split_root)\n",
    "\n",
    "    if not even_dirs:\n",
    "        print(f\"[WARN] {split_root}: no even steps, skipping\")\n",
    "        return 0\n",
    "    if not odd_dirs:\n",
    "        print(f\"[WARN] {split_root}: no odd steps (history), skipping\")\n",
    "        return 0\n",
    "\n",
    "    total = 0\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for qdir in even_dirs:\n",
    "            trail = qdir / \"trail.txt\"\n",
    "            if not trail.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                xy = _read_trail(trail)\n",
    "                N = _infer_n_agents_from_len(xy.shape[0]) \n",
    "                T = xy.shape[0] // N\n",
    "                gold = xy[-N:, :].copy()\n",
    "                gold_cells = set(_xy_to_cells(gold))\n",
    "                gold_cent = _centroid(gold)\n",
    "            except Exception as e:\n",
    "                print(f\"[skip] {qdir.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            distractors: List[np.ndarray] = []\n",
    "            seen_keys = {tuple(map(int, gold.flatten()))}\n",
    "\n",
    "            for _ in range(NUM_DISTRACTORS):\n",
    "                try:\n",
    "                    arr = _sample_cluster_far_from_gold(\n",
    "                        N=N,\n",
    "                        gold_cells=gold_cells,\n",
    "                        gold_centroid=gold_cent,\n",
    "                        min_centroid_l2=MIN_CENTROID_L2_FROM_GOLD,\n",
    "                        max_jaccard_overlap=MAX_JACCARD_OVERLAP,\n",
    "                        max_retry=MAX_RETRY_PER_DISTRACTOR,\n",
    "                    )\n",
    "                except RuntimeError:\n",
    "                    break\n",
    "                k = tuple(map(int, arr.flatten()))\n",
    "                if k in seen_keys:\n",
    "                    continue\n",
    "                seen_keys.add(k)\n",
    "                distractors.append(arr)\n",
    "\n",
    "            if len(distractors) < NUM_DISTRACTORS:\n",
    "                need = NUM_DISTRACTORS - len(distractors)\n",
    "                for _ in range(need):\n",
    "                    try:\n",
    "                        arr = _sample_cluster_far_from_gold(\n",
    "                            N=N,\n",
    "                            gold_cells=gold_cells,\n",
    "                            gold_centroid=gold_cent,\n",
    "                            min_centroid_l2=max(1.0, MIN_CENTROID_L2_FROM_GOLD * 0.6),\n",
    "                            max_jaccard_overlap=MAX_JACCARD_OVERLAP,\n",
    "                            max_retry=MAX_RETRY_PER_DISTRACTOR * 2,\n",
    "                        )\n",
    "                    except RuntimeError:\n",
    "                        break\n",
    "                    k = tuple(map(int, arr.flatten()))\n",
    "                    if k in seen_keys:\n",
    "                        continue\n",
    "                    seen_keys.add(k)\n",
    "                    distractors.append(arr)\n",
    "\n",
    "            if len(distractors) != NUM_DISTRACTORS:\n",
    "                print(f\"[skip] {qdir.name}: insufficient distractors, skipping\")\n",
    "                continue\n",
    "\n",
    "            all_choices = [gold] + distractors\n",
    "            cell_choices = [[int(c) for c in _xy_to_cells(a)] for a in all_choices]\n",
    "\n",
    "            order = list(range(4))\n",
    "            random.shuffle(order)\n",
    "            shuffled = [cell_choices[i] for i in order]\n",
    "            label_idx = order.index(0)\n",
    "\n",
    "            hdir = random.choice(odd_dirs)\n",
    "\n",
    "            rec = {\n",
    "                \"history_dir\": str(hdir.relative_to(split_root)),\n",
    "                \"query_dir\":   str(qdir.relative_to(split_root)),\n",
    "                \"mode\":        _mode_from_dir(qdir),\n",
    "                \"n_agents\":    N,\n",
    "                \"T\":           T,\n",
    "                \"choices_cell\": shuffled,\n",
    "                \"label_idx\":   label_idx,\n",
    "                \"label_letter\": [\"A\",\"B\",\"C\",\"D\"][label_idx],\n",
    "            }\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            total += 1\n",
    "\n",
    "    print(f\"✓ {split_root}: wrote {total} pairs -> {out_jsonl.name}\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a90da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] use base = /Users/Jer_ry/Desktop/script_tom/data/3_12\n",
      "[DEBUG] check split dir: /Users/Jer_ry/Desktop/script_tom/data/3_12/training -> exists=True\n",
      "[DEBUG] check split dir: /Users/Jer_ry/Desktop/script_tom/data/3_12/validation -> exists=True\n",
      "[DEBUG] check split dir: /Users/Jer_ry/Desktop/script_tom/data/3_12/testing -> exists=True\n",
      "[INFO] build split at /Users/Jer_ry/Desktop/script_tom/data/3_12/training\n",
      "✓ /Users/Jer_ry/Desktop/script_tom/data/3_12/training: wrote 4000 pairs -> tomnet2_format.jsonl\n",
      "[INFO] build split at /Users/Jer_ry/Desktop/script_tom/data/3_12/validation\n",
      "✓ /Users/Jer_ry/Desktop/script_tom/data/3_12/validation: wrote 500 pairs -> tomnet2_format.jsonl\n",
      "[INFO] build split at /Users/Jer_ry/Desktop/script_tom/data/3_12/testing\n",
      "✓ /Users/Jer_ry/Desktop/script_tom/data/3_12/testing: wrote 500 pairs -> tomnet2_format.jsonl\n",
      "Done. total=5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_3_12 = Path(\"/Users/Jer_ry/Desktop/script_tom/data/3_12\")\n",
    "\n",
    "def main() -> None:\n",
    "    base = BASE_3_12.resolve()\n",
    "    print(f\"[INFO] use base = {base}\")\n",
    "\n",
    "    for name in SPLITS:\n",
    "        p = base / name\n",
    "        print(f\"[DEBUG] check split dir: {p} -> exists={p.is_dir()}\")\n",
    "        if not p.is_dir():\n",
    "            print(f\"[WARN] split {name} does not exist: {p}\")\n",
    "\n",
    "    tot = 0\n",
    "    for sp in SPLITS:\n",
    "        split_root = base / sp      # e.g. /.../3_12/training\n",
    "        if not split_root.exists():\n",
    "            print(f\"[INFO] skip {split_root} (not found)\")\n",
    "            continue\n",
    "        tot += build_one_split(split_root)\n",
    "\n",
    "    print(f\"Done. total={tot}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E_fixed_train.py\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_MODE_ALIASES = {\n",
    "    \"rulemap\": \"rulemap\",\n",
    "    \"random\": \"random\",\n",
    "    \"logic\": \"logic\",\n",
    "    \"intermediate_case1\": \"intermediate_case1\",\n",
    "    \"intermediate_case2\": \"intermediate_case2\",\n",
    "    \"all\": \"all\",\n",
    "}\n",
    "\n",
    "_ALL_MODES: List[str] = [\n",
    "    \"rulemap\",\n",
    "    \"random\",\n",
    "    \"intermediate_case1\",\n",
    "    \"intermediate_case2\",\n",
    "    \"logic\",\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    data_root: Path = Path(\".\")    \n",
    "    sim_name: str = \"sim5_24\"\n",
    "    mode: str = \"all\"              \n",
    "\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    n_layers: int = 2\n",
    "    stem_ch: int = 32\n",
    "    n_blocks: int = 4\n",
    "    e_char_dim: int = 64\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    batch_size: int = 32\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    max_epochs: int = 30\n",
    "\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    save_dir: Path = Path(\"../checkpoints\")\n",
    "\n",
    "    def canonical_mode(self) -> str:\n",
    "        key = self.mode.strip().lower()\n",
    "        if key not in _MODE_ALIASES:\n",
    "            raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "        return _MODE_ALIASES[key]\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def _probe_in_ch(ds: ToMNet2JsonlDataset) -> int:\n",
    "    one = ds[0]\n",
    "    return int(one[\"grid_seq_j\"].shape[-1])\n",
    "\n",
    "\n",
    "def make_loader(cfg: TrainConfig, split: str, mode: str) -> Tuple[DataLoader, int]:\n",
    "    split_map = {\n",
    "        \"train\": \"training\",\n",
    "        \"val\":   \"validation\",\n",
    "        \"test\":  \"testing\",\n",
    "    }\n",
    "    split_dir = cfg.data_root / split_map[split]\n",
    "    use_modes = None if mode == \"all\" else [mode]\n",
    "\n",
    "    ds = ToMNet2JsonlDataset(\n",
    "        split_root=split_dir,\n",
    "        sim_name=cfg.sim_name,\n",
    "        use_modes=use_modes,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    in_ch = ds.in_ch if ds.in_ch is not None else _probe_in_ch(ds)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=(split == \"train\"),\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_tomnet2,\n",
    "    )\n",
    "    return loader, in_ch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module, loader: DataLoader, device: str\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    n = 0\n",
    "    corr = 0\n",
    "    tot_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = {\n",
    "            k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        logits = model(\n",
    "            x[\"grid_seq_j\"],\n",
    "            x[\"tmask_j\"],\n",
    "            x[\"grid_seq_k\"],\n",
    "            x[\"tmask_k\"],\n",
    "            x[\"choices_ids\"],\n",
    "            x[\"choices_mask\"],\n",
    "        )\n",
    "        loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        corr += (pred == x[\"labels\"]).sum().item()\n",
    "        tot_loss += loss.item() * logits.size(0)\n",
    "        n += logits.size(0)\n",
    "\n",
    "    acc = corr / max(n, 1)\n",
    "    avg_loss = tot_loss / max(n, 1)\n",
    "    return acc, avg_loss\n",
    "\n",
    "\n",
    "def train_one_mode(cfg: TrainConfig, mode: str):\n",
    "    print(f\"\\n=== Training mode = {mode} ===\")\n",
    "\n",
    "    \n",
    "    train_ld, in_ch = make_loader(cfg, \"train\", mode)\n",
    "    val_ld, _ = make_loader(cfg, \"val\", mode)\n",
    "    test_ld, _ = make_loader(cfg, \"test\", mode)\n",
    "\n",
    "    model = ToMNet(\n",
    "        in_ch=in_ch,\n",
    "        d_model=cfg.d_model,\n",
    "        nhead=cfg.nhead,\n",
    "        n_layers=cfg.n_layers,\n",
    "        stem_ch=cfg.stem_ch,\n",
    "        n_blocks=cfg.n_blocks,\n",
    "        e_char_dim=cfg.e_char_dim,\n",
    "        dropout=cfg.dropout,\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay\n",
    "    )\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        opt, T_max=cfg.max_epochs\n",
    "    )\n",
    "\n",
    "    best_va = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, cfg.max_epochs + 1):\n",
    "        model.train()\n",
    "        n = 0\n",
    "        corr = 0\n",
    "        tot_loss = 0.0\n",
    "\n",
    "        for batch in train_ld:\n",
    "            x = {\n",
    "                k: (v.to(cfg.device) if isinstance(v, torch.Tensor) else v)\n",
    "                for k, v in batch.items()\n",
    "            }\n",
    "            logits = model(\n",
    "                x[\"grid_seq_j\"],\n",
    "                x[\"tmask_j\"],\n",
    "                x[\"grid_seq_k\"],\n",
    "                x[\"tmask_k\"],\n",
    "                x[\"choices_ids\"],\n",
    "                x[\"choices_mask\"],\n",
    "            )\n",
    "            loss = F.cross_entropy(logits, x[\"labels\"])\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            corr += (pred == x[\"labels\"]).sum().item()\n",
    "            tot_loss += loss.item() * logits.size(0)\n",
    "            n += logits.size(0)\n",
    "\n",
    "        tr_acc = corr / max(n, 1)\n",
    "        tr_loss = tot_loss / max(n, 1)\n",
    "        va_acc, va_loss = evaluate(model, val_ld, cfg.device)\n",
    "        sch.step()\n",
    "\n",
    "        print(\n",
    "            f\"[{mode}][Ep {ep:02d}] \"\n",
    "            f\"train acc={tr_acc:.3f} loss={tr_loss:.3f} | \"\n",
    "            f\"val acc={va_acc:.3f} loss={va_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "        if va_acc > best_va:\n",
    "            best_va = va_acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    te_acc, te_loss = evaluate(model, test_ld, cfg.device)\n",
    "    print(f\"[{mode}][TEST] acc={te_acc:.3f} loss={te_loss:.3f}\")\n",
    "\n",
    "    return model, {\"val_acc\": best_va, \"test_acc\": te_acc}\n",
    "\n",
    "\n",
    "_ALL_MODES = [\"random\", \"rulemap\", \"logic\", \"intermediate_case1\", \"intermediate_case2\"]\n",
    "\n",
    "def _model_name(sim_name: str, mode: str, kind: str = \"ToMNet\") -> str:\n",
    "    return f\"{kind}_{sim_name}_{mode}\"\n",
    "\n",
    "def _save_model(model: nn.Module, cfg: TrainConfig, mode: str, kind: str = \"ToMNet\") -> Path:\n",
    "    cfg.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    name = _model_name(cfg.sim_name, mode, kind)\n",
    "    path = cfg.save_dir / f\"{name}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"[SAVE] {name} -> {path}\")\n",
    "    return path\n",
    "\n",
    "def run(cfg: TrainConfig, kind: str = \"ToMNet\") -> Dict[str, nn.Module]:\n",
    "    models: Dict[str, nn.Module] = {}\n",
    "\n",
    "    if cfg.mode == \"all\":\n",
    "        modes = _ALL_MODES\n",
    "    else:\n",
    "        modes = [cfg.mode]\n",
    "\n",
    "    for mode in modes:\n",
    "        print(f\"[TRAIN] sim={cfg.sim_name} mode={mode}\")\n",
    "        model, best_acc = train_one_mode(cfg, mode)\n",
    "\n",
    "        name = _model_name(cfg.sim_name, mode, kind)\n",
    "        _save_model(model, cfg, mode, kind=kind)\n",
    "        models[name] = model\n",
    "        print(f\"[DONE] {name} (best_acc={best_acc})\")\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning ../data/3_12 for missing .npy files...\n",
      "All looks good! No missing .npy files.\n"
     ]
    }
   ],
   "source": [
    "# F_run_gridizer.py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "try:\n",
    "    from scripts.gridizer_multitrack import coords_to_grid_seq_multitrack\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"can not find gridizer_multitrack.py, please check {e}\")\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"../data/3_12\") \n",
    "N_AGENTS = 3\n",
    "MAZE_SIZE = 12\n",
    "\n",
    "def process_missing_npy():\n",
    "    print(f\"Scanning {DATA_ROOT} for missing .npy files...\")\n",
    "    count = 0\n",
    "    for trail_txt in DATA_ROOT.rglob(\"trail.txt\"):\n",
    "        step_dir = trail_txt.parent\n",
    "        npy_path = step_dir / \"grid_seq.npy\"\n",
    "        \n",
    "        # If .npy does not exist, generate it\n",
    "        if not npy_path.exists():\n",
    "            try:\n",
    "                # Call the standard gridizer\n",
    "                # Note: This assumes the coordinate format in trail.txt is compatible with the gridizer \n",
    "                grid = coords_to_grid_seq_multitrack(\n",
    "                    trail_txt_path=trail_txt,\n",
    "                    n_agents=N_AGENTS,\n",
    "                    H=MAZE_SIZE,\n",
    "                    W=MAZE_SIZE,\n",
    "                    keep_init_every_frame=True\n",
    "                )\n",
    "                \n",
    "                np.save(npy_path, grid)\n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"Generated {count} npy files...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to gridize {step_dir}: {e}\")\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"All looks good! No missing .npy files.\")\n",
    "    else:\n",
    "        print(f\"Done. Generated {count} missing .npy files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_missing_npy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bdad2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "024d9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=random\n",
      "\n",
      "=== Training mode = random ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random][Ep 01] train acc=0.280 loss=1.386 | val acc=0.470 loss=1.320\n",
      "[random][Ep 02] train acc=0.398 loss=1.327 | val acc=0.460 loss=1.281\n",
      "[random][Ep 03] train acc=0.444 loss=1.278 | val acc=0.450 loss=1.236\n",
      "[random][Ep 04] train acc=0.455 loss=1.243 | val acc=0.480 loss=1.191\n",
      "[random][Ep 05] train acc=0.521 loss=1.190 | val acc=0.520 loss=1.161\n",
      "[random][Ep 06] train acc=0.535 loss=1.152 | val acc=0.540 loss=1.141\n",
      "[random][Ep 07] train acc=0.554 loss=1.118 | val acc=0.560 loss=1.114\n",
      "[random][Ep 08] train acc=0.562 loss=1.087 | val acc=0.540 loss=1.106\n",
      "[random][Ep 09] train acc=0.568 loss=1.067 | val acc=0.590 loss=1.099\n",
      "[random][Ep 10] train acc=0.588 loss=1.036 | val acc=0.600 loss=1.083\n",
      "[random][Ep 11] train acc=0.611 loss=1.009 | val acc=0.550 loss=1.100\n",
      "[random][Ep 12] train acc=0.620 loss=0.967 | val acc=0.560 loss=1.091\n",
      "[random][Ep 13] train acc=0.645 loss=0.932 | val acc=0.580 loss=1.095\n",
      "[random][Ep 14] train acc=0.647 loss=0.922 | val acc=0.600 loss=1.066\n",
      "[random][Ep 15] train acc=0.676 loss=0.877 | val acc=0.570 loss=1.099\n",
      "[random][Ep 16] train acc=0.675 loss=0.857 | val acc=0.620 loss=1.075\n",
      "[random][Ep 17] train acc=0.704 loss=0.795 | val acc=0.580 loss=1.077\n",
      "[random][Ep 18] train acc=0.715 loss=0.777 | val acc=0.610 loss=1.049\n",
      "[random][Ep 19] train acc=0.744 loss=0.727 | val acc=0.570 loss=1.089\n",
      "[random][Ep 20] train acc=0.755 loss=0.665 | val acc=0.560 loss=1.109\n",
      "[random][Ep 21] train acc=0.796 loss=0.621 | val acc=0.550 loss=1.164\n",
      "[random][Ep 22] train acc=0.825 loss=0.578 | val acc=0.560 loss=1.107\n",
      "[random][Ep 23] train acc=0.828 loss=0.543 | val acc=0.500 loss=1.188\n",
      "[random][Ep 24] train acc=0.843 loss=0.506 | val acc=0.560 loss=1.172\n",
      "[random][Ep 25] train acc=0.861 loss=0.467 | val acc=0.570 loss=1.250\n",
      "[random][Ep 26] train acc=0.890 loss=0.407 | val acc=0.530 loss=1.180\n",
      "[random][Ep 27] train acc=0.887 loss=0.378 | val acc=0.550 loss=1.165\n",
      "[random][Ep 28] train acc=0.886 loss=0.366 | val acc=0.470 loss=1.389\n",
      "[random][Ep 29] train acc=0.895 loss=0.338 | val acc=0.550 loss=1.288\n",
      "[random][Ep 30] train acc=0.927 loss=0.293 | val acc=0.580 loss=1.277\n",
      "[random][Ep 31] train acc=0.924 loss=0.278 | val acc=0.540 loss=1.437\n",
      "[random][Ep 32] train acc=0.921 loss=0.284 | val acc=0.440 loss=1.536\n",
      "[random][Ep 33] train acc=0.931 loss=0.240 | val acc=0.600 loss=1.271\n",
      "[random][Ep 34] train acc=0.936 loss=0.231 | val acc=0.500 loss=1.338\n",
      "[random][Ep 35] train acc=0.959 loss=0.190 | val acc=0.550 loss=1.388\n",
      "[random][Ep 36] train acc=0.939 loss=0.221 | val acc=0.490 loss=1.460\n",
      "[random][Ep 37] train acc=0.955 loss=0.174 | val acc=0.520 loss=1.643\n",
      "[random][Ep 38] train acc=0.959 loss=0.175 | val acc=0.580 loss=1.453\n",
      "[random][Ep 39] train acc=0.968 loss=0.154 | val acc=0.560 loss=1.465\n",
      "[random][Ep 40] train acc=0.963 loss=0.156 | val acc=0.510 loss=1.451\n",
      "[random][Ep 41] train acc=0.961 loss=0.150 | val acc=0.540 loss=1.584\n",
      "[random][Ep 42] train acc=0.965 loss=0.155 | val acc=0.500 loss=1.477\n",
      "[random][Ep 43] train acc=0.958 loss=0.151 | val acc=0.540 loss=1.363\n",
      "[random][Ep 44] train acc=0.976 loss=0.123 | val acc=0.520 loss=1.375\n",
      "[random][Ep 45] train acc=0.978 loss=0.113 | val acc=0.520 loss=1.501\n",
      "[random][Ep 46] train acc=0.973 loss=0.111 | val acc=0.620 loss=1.454\n",
      "[random][Ep 47] train acc=0.979 loss=0.101 | val acc=0.500 loss=1.690\n",
      "[random][Ep 48] train acc=0.974 loss=0.104 | val acc=0.500 loss=1.677\n",
      "[random][Ep 49] train acc=0.979 loss=0.094 | val acc=0.530 loss=1.693\n",
      "[random][Ep 50] train acc=0.981 loss=0.097 | val acc=0.480 loss=1.570\n",
      "[random][Ep 51] train acc=0.981 loss=0.095 | val acc=0.490 loss=1.652\n",
      "[random][Ep 52] train acc=0.981 loss=0.087 | val acc=0.510 loss=1.599\n",
      "[random][Ep 53] train acc=0.993 loss=0.065 | val acc=0.570 loss=1.544\n",
      "[random][Ep 54] train acc=0.975 loss=0.096 | val acc=0.570 loss=1.570\n",
      "[random][Ep 55] train acc=0.984 loss=0.082 | val acc=0.540 loss=1.671\n",
      "[random][Ep 56] train acc=0.985 loss=0.068 | val acc=0.520 loss=1.679\n",
      "[random][Ep 57] train acc=0.981 loss=0.083 | val acc=0.550 loss=1.531\n",
      "[random][Ep 58] train acc=0.990 loss=0.062 | val acc=0.540 loss=1.644\n",
      "[random][Ep 59] train acc=0.986 loss=0.071 | val acc=0.550 loss=1.625\n",
      "[random][Ep 60] train acc=0.986 loss=0.065 | val acc=0.550 loss=1.633\n",
      "[random][Ep 61] train acc=0.986 loss=0.063 | val acc=0.530 loss=1.639\n",
      "[random][Ep 62] train acc=0.979 loss=0.070 | val acc=0.530 loss=1.650\n",
      "[random][Ep 63] train acc=0.991 loss=0.063 | val acc=0.520 loss=1.620\n",
      "[random][Ep 64] train acc=0.986 loss=0.067 | val acc=0.540 loss=1.617\n",
      "[random][Ep 65] train acc=0.988 loss=0.064 | val acc=0.490 loss=1.703\n",
      "[random][Ep 66] train acc=0.986 loss=0.065 | val acc=0.510 loss=1.773\n",
      "[random][Ep 67] train acc=0.989 loss=0.057 | val acc=0.510 loss=1.738\n",
      "[random][Ep 68] train acc=0.985 loss=0.067 | val acc=0.530 loss=1.723\n",
      "[random][Ep 69] train acc=0.980 loss=0.068 | val acc=0.520 loss=1.709\n",
      "[random][Ep 70] train acc=0.981 loss=0.063 | val acc=0.560 loss=1.697\n",
      "[random][Ep 71] train acc=0.989 loss=0.059 | val acc=0.550 loss=1.676\n",
      "[random][Ep 72] train acc=0.991 loss=0.054 | val acc=0.550 loss=1.687\n",
      "[random][Ep 73] train acc=0.991 loss=0.052 | val acc=0.530 loss=1.714\n",
      "[random][Ep 74] train acc=0.993 loss=0.049 | val acc=0.530 loss=1.718\n",
      "[random][Ep 75] train acc=0.989 loss=0.061 | val acc=0.530 loss=1.725\n",
      "[random][Ep 76] train acc=0.990 loss=0.055 | val acc=0.530 loss=1.721\n",
      "[random][Ep 77] train acc=0.994 loss=0.049 | val acc=0.530 loss=1.722\n",
      "[random][Ep 78] train acc=0.990 loss=0.052 | val acc=0.530 loss=1.720\n",
      "[random][Ep 79] train acc=0.989 loss=0.054 | val acc=0.530 loss=1.713\n",
      "[random][Ep 80] train acc=0.986 loss=0.053 | val acc=0.530 loss=1.713\n",
      "[random][TEST] acc=0.610 loss=1.504\n",
      "[SAVE] ToMNet2_3_12_random -> ../checkpoints/ToMNet2_3_12_random.pt\n",
      "[DONE] ToMNet2_3_12_random (best_acc={'val_acc': 0.62, 'test_acc': 0.61})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"../data/3_12\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"random\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "\n",
    "models = run(cfg, kind=\"ToMNet2\")\n",
    "TomNet_Random_Model = models[\"ToMNet2_3_12_random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb05916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=rulemap\n",
      "\n",
      "=== Training mode = rulemap ===\n",
      "[rulemap][Ep 01] train acc=0.676 loss=1.060 | val acc=0.860 loss=0.759\n",
      "[rulemap][Ep 02] train acc=0.839 loss=0.620 | val acc=0.900 loss=0.403\n",
      "[rulemap][Ep 03] train acc=0.882 loss=0.396 | val acc=0.920 loss=0.268\n",
      "[rulemap][Ep 04] train acc=0.894 loss=0.311 | val acc=0.940 loss=0.199\n",
      "[rulemap][Ep 05] train acc=0.917 loss=0.246 | val acc=0.960 loss=0.147\n",
      "[rulemap][Ep 06] train acc=0.938 loss=0.196 | val acc=0.970 loss=0.118\n",
      "[rulemap][Ep 07] train acc=0.951 loss=0.153 | val acc=0.970 loss=0.114\n",
      "[rulemap][Ep 08] train acc=0.955 loss=0.136 | val acc=0.970 loss=0.108\n",
      "[rulemap][Ep 09] train acc=0.963 loss=0.117 | val acc=0.960 loss=0.109\n",
      "[rulemap][Ep 10] train acc=0.979 loss=0.093 | val acc=0.970 loss=0.082\n",
      "[rulemap][Ep 11] train acc=0.983 loss=0.077 | val acc=0.970 loss=0.077\n",
      "[rulemap][Ep 12] train acc=0.984 loss=0.065 | val acc=0.970 loss=0.056\n",
      "[rulemap][Ep 13] train acc=0.986 loss=0.056 | val acc=0.970 loss=0.065\n",
      "[rulemap][Ep 14] train acc=0.990 loss=0.044 | val acc=0.970 loss=0.066\n",
      "[rulemap][Ep 15] train acc=0.989 loss=0.035 | val acc=0.970 loss=0.071\n",
      "[rulemap][Ep 16] train acc=0.998 loss=0.028 | val acc=0.970 loss=0.062\n",
      "[rulemap][Ep 17] train acc=0.994 loss=0.030 | val acc=0.970 loss=0.055\n",
      "[rulemap][Ep 18] train acc=1.000 loss=0.019 | val acc=0.980 loss=0.041\n",
      "[rulemap][Ep 19] train acc=0.994 loss=0.025 | val acc=0.980 loss=0.044\n",
      "[rulemap][Ep 20] train acc=1.000 loss=0.016 | val acc=0.970 loss=0.047\n",
      "[rulemap][Ep 21] train acc=0.995 loss=0.019 | val acc=0.980 loss=0.048\n",
      "[rulemap][Ep 22] train acc=0.999 loss=0.016 | val acc=0.980 loss=0.038\n",
      "[rulemap][Ep 23] train acc=1.000 loss=0.010 | val acc=0.950 loss=0.101\n",
      "[rulemap][Ep 24] train acc=0.998 loss=0.011 | val acc=0.980 loss=0.054\n",
      "[rulemap][Ep 25] train acc=1.000 loss=0.008 | val acc=0.970 loss=0.063\n",
      "[rulemap][Ep 26] train acc=0.999 loss=0.009 | val acc=0.950 loss=0.069\n",
      "[rulemap][Ep 27] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.041\n",
      "[rulemap][Ep 28] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.045\n",
      "[rulemap][Ep 29] train acc=1.000 loss=0.006 | val acc=0.980 loss=0.047\n",
      "[rulemap][Ep 30] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.038\n",
      "[rulemap][Ep 31] train acc=1.000 loss=0.005 | val acc=0.980 loss=0.052\n",
      "[rulemap][Ep 32] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.058\n",
      "[rulemap][Ep 33] train acc=0.999 loss=0.004 | val acc=0.980 loss=0.048\n",
      "[rulemap][Ep 34] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.044\n",
      "[rulemap][Ep 35] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.043\n",
      "[rulemap][Ep 36] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.048\n",
      "[rulemap][Ep 37] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.051\n",
      "[rulemap][Ep 38] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.041\n",
      "[rulemap][Ep 39] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.042\n",
      "[rulemap][Ep 40] train acc=1.000 loss=0.004 | val acc=0.980 loss=0.051\n",
      "[rulemap][Ep 41] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.049\n",
      "[rulemap][Ep 42] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.055\n",
      "[rulemap][Ep 43] train acc=1.000 loss=0.003 | val acc=0.980 loss=0.052\n",
      "[rulemap][Ep 44] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.052\n",
      "[rulemap][Ep 45] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.043\n",
      "[rulemap][Ep 46] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.050\n",
      "[rulemap][Ep 47] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.060\n",
      "[rulemap][Ep 48] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.058\n",
      "[rulemap][Ep 49] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.053\n",
      "[rulemap][Ep 50] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.060\n",
      "[rulemap][Ep 51] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.069\n",
      "[rulemap][Ep 52] train acc=1.000 loss=0.002 | val acc=0.970 loss=0.072\n",
      "[rulemap][Ep 53] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.064\n",
      "[rulemap][Ep 54] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.051\n",
      "[rulemap][Ep 55] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.052\n",
      "[rulemap][Ep 56] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.052\n",
      "[rulemap][Ep 57] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.051\n",
      "[rulemap][Ep 58] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.047\n",
      "[rulemap][Ep 59] train acc=0.999 loss=0.003 | val acc=0.980 loss=0.049\n",
      "[rulemap][Ep 60] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.050\n",
      "[rulemap][Ep 61] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.051\n",
      "[rulemap][Ep 62] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.055\n",
      "[rulemap][Ep 63] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.055\n",
      "[rulemap][Ep 64] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.055\n",
      "[rulemap][Ep 65] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 66] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.059\n",
      "[rulemap][Ep 67] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.059\n",
      "[rulemap][Ep 68] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.059\n",
      "[rulemap][Ep 69] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.059\n",
      "[rulemap][Ep 70] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.058\n",
      "[rulemap][Ep 71] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 72] train acc=0.999 loss=0.002 | val acc=0.980 loss=0.056\n",
      "[rulemap][Ep 73] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 74] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 75] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 76] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 77] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.058\n",
      "[rulemap][Ep 78] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 79] train acc=1.000 loss=0.002 | val acc=0.980 loss=0.057\n",
      "[rulemap][Ep 80] train acc=1.000 loss=0.001 | val acc=0.980 loss=0.057\n",
      "[rulemap][TEST] acc=0.960 loss=0.164\n",
      "[SAVE] ToMNet2_3_12_rulemap -> ../checkpoints/ToMNet2_3_12_rulemap.pt\n",
      "[DONE] ToMNet2_3_12_rulemap (best_acc={'val_acc': 0.98, 'test_acc': 0.96})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"../data/3_12\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"rulemap\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "\n",
    "models = run(cfg, kind=\"ToMNet2\")\n",
    "TomNet_Rulemap_Model = models[\"ToMNet2_3_12_rulemap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a8a1551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=logic\n",
      "\n",
      "=== Training mode = logic ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[logic][Ep 01] train acc=0.644 loss=1.129 | val acc=0.850 loss=0.849\n",
      "[logic][Ep 02] train acc=0.877 loss=0.677 | val acc=0.890 loss=0.433\n",
      "[logic][Ep 03] train acc=0.922 loss=0.380 | val acc=0.970 loss=0.199\n",
      "[logic][Ep 04] train acc=0.950 loss=0.206 | val acc=0.980 loss=0.091\n",
      "[logic][Ep 05] train acc=0.961 loss=0.146 | val acc=0.990 loss=0.058\n",
      "[logic][Ep 06] train acc=0.965 loss=0.111 | val acc=0.980 loss=0.049\n",
      "[logic][Ep 07] train acc=0.976 loss=0.087 | val acc=0.970 loss=0.043\n",
      "[logic][Ep 08] train acc=0.975 loss=0.079 | val acc=0.990 loss=0.036\n",
      "[logic][Ep 09] train acc=0.980 loss=0.063 | val acc=0.990 loss=0.033\n",
      "[logic][Ep 10] train acc=0.984 loss=0.052 | val acc=0.990 loss=0.024\n",
      "[logic][Ep 11] train acc=0.986 loss=0.046 | val acc=1.000 loss=0.023\n",
      "[logic][Ep 12] train acc=0.993 loss=0.037 | val acc=0.990 loss=0.024\n",
      "[logic][Ep 13] train acc=0.994 loss=0.034 | val acc=0.990 loss=0.021\n",
      "[logic][Ep 14] train acc=0.998 loss=0.024 | val acc=1.000 loss=0.015\n",
      "[logic][Ep 15] train acc=0.993 loss=0.024 | val acc=1.000 loss=0.013\n",
      "[logic][Ep 16] train acc=0.999 loss=0.018 | val acc=1.000 loss=0.015\n",
      "[logic][Ep 17] train acc=1.000 loss=0.014 | val acc=1.000 loss=0.009\n",
      "[logic][Ep 18] train acc=0.999 loss=0.013 | val acc=1.000 loss=0.011\n",
      "[logic][Ep 19] train acc=0.999 loss=0.012 | val acc=0.990 loss=0.021\n",
      "[logic][Ep 20] train acc=1.000 loss=0.008 | val acc=1.000 loss=0.006\n",
      "[logic][Ep 21] train acc=0.999 loss=0.010 | val acc=1.000 loss=0.004\n",
      "[logic][Ep 22] train acc=1.000 loss=0.007 | val acc=1.000 loss=0.004\n",
      "[logic][Ep 23] train acc=1.000 loss=0.006 | val acc=1.000 loss=0.003\n",
      "[logic][Ep 24] train acc=1.000 loss=0.007 | val acc=1.000 loss=0.004\n",
      "[logic][Ep 25] train acc=1.000 loss=0.005 | val acc=1.000 loss=0.004\n",
      "[logic][Ep 26] train acc=1.000 loss=0.005 | val acc=1.000 loss=0.003\n",
      "[logic][Ep 27] train acc=1.000 loss=0.004 | val acc=1.000 loss=0.003\n",
      "[logic][Ep 28] train acc=1.000 loss=0.004 | val acc=1.000 loss=0.002\n",
      "[logic][Ep 29] train acc=1.000 loss=0.003 | val acc=1.000 loss=0.002\n",
      "[logic][Ep 30] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 31] train acc=1.000 loss=0.003 | val acc=1.000 loss=0.003\n",
      "[logic][Ep 32] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.004\n",
      "[logic][Ep 33] train acc=1.000 loss=0.004 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 34] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 35] train acc=1.000 loss=0.003 | val acc=1.000 loss=0.002\n",
      "[logic][Ep 36] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.002\n",
      "[logic][Ep 37] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 38] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 39] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 40] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 41] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 42] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 43] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 44] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 45] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 46] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 47] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 48] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 49] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 50] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 51] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 52] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 53] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 54] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 55] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 56] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 57] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 58] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 59] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 60] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 61] train acc=0.999 loss=0.003 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 62] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.001\n",
      "[logic][Ep 63] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 64] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 65] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 66] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 67] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 68] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 69] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 70] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 71] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 72] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 73] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 74] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 75] train acc=1.000 loss=0.002 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 76] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 77] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 78] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 79] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][Ep 80] train acc=1.000 loss=0.001 | val acc=1.000 loss=0.000\n",
      "[logic][TEST] acc=0.980 loss=0.028\n",
      "[SAVE] ToMNet2_3_12_logic -> ../checkpoints/ToMNet2_3_12_logic.pt\n",
      "[DONE] ToMNet2_3_12_logic (best_acc={'val_acc': 1.0, 'test_acc': 0.98})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"../data/3_12\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"logic\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")\n",
    "\n",
    "models = run(cfg, kind=\"ToMNet2\")\n",
    "TomNet_Logic_Model = models[\"ToMNet2_3_12_logic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75922d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case1\n",
      "\n",
      "=== Training mode = intermediate_case1 ===\n",
      "[intermediate_case1][Ep 01] train acc=0.251 loss=1.392 | val acc=0.310 loss=1.353\n",
      "[intermediate_case1][Ep 02] train acc=0.351 loss=1.342 | val acc=0.410 loss=1.323\n",
      "[intermediate_case1][Ep 03] train acc=0.409 loss=1.309 | val acc=0.430 loss=1.313\n",
      "[intermediate_case1][Ep 04] train acc=0.409 loss=1.284 | val acc=0.420 loss=1.314\n",
      "[intermediate_case1][Ep 05] train acc=0.425 loss=1.259 | val acc=0.430 loss=1.311\n",
      "[intermediate_case1][Ep 06] train acc=0.451 loss=1.240 | val acc=0.420 loss=1.308\n",
      "[intermediate_case1][Ep 07] train acc=0.481 loss=1.218 | val acc=0.440 loss=1.309\n",
      "[intermediate_case1][Ep 08] train acc=0.499 loss=1.186 | val acc=0.460 loss=1.301\n",
      "[intermediate_case1][Ep 09] train acc=0.515 loss=1.167 | val acc=0.430 loss=1.298\n",
      "[intermediate_case1][Ep 10] train acc=0.517 loss=1.145 | val acc=0.470 loss=1.298\n",
      "[intermediate_case1][Ep 11] train acc=0.532 loss=1.129 | val acc=0.430 loss=1.302\n",
      "[intermediate_case1][Ep 12] train acc=0.547 loss=1.102 | val acc=0.410 loss=1.292\n",
      "[intermediate_case1][Ep 13] train acc=0.552 loss=1.093 | val acc=0.420 loss=1.279\n",
      "[intermediate_case1][Ep 14] train acc=0.596 loss=1.047 | val acc=0.440 loss=1.265\n",
      "[intermediate_case1][Ep 15] train acc=0.586 loss=1.029 | val acc=0.430 loss=1.278\n",
      "[intermediate_case1][Ep 16] train acc=0.619 loss=0.999 | val acc=0.430 loss=1.256\n",
      "[intermediate_case1][Ep 17] train acc=0.637 loss=0.944 | val acc=0.440 loss=1.241\n",
      "[intermediate_case1][Ep 18] train acc=0.672 loss=0.895 | val acc=0.440 loss=1.299\n",
      "[intermediate_case1][Ep 19] train acc=0.694 loss=0.834 | val acc=0.430 loss=1.347\n",
      "[intermediate_case1][Ep 20] train acc=0.718 loss=0.782 | val acc=0.500 loss=1.262\n",
      "[intermediate_case1][Ep 21] train acc=0.762 loss=0.732 | val acc=0.440 loss=1.276\n",
      "[intermediate_case1][Ep 22] train acc=0.761 loss=0.686 | val acc=0.390 loss=1.422\n",
      "[intermediate_case1][Ep 23] train acc=0.802 loss=0.621 | val acc=0.460 loss=1.333\n",
      "[intermediate_case1][Ep 24] train acc=0.804 loss=0.604 | val acc=0.470 loss=1.302\n",
      "[intermediate_case1][Ep 25] train acc=0.840 loss=0.534 | val acc=0.440 loss=1.424\n",
      "[intermediate_case1][Ep 26] train acc=0.868 loss=0.470 | val acc=0.490 loss=1.362\n",
      "[intermediate_case1][Ep 27] train acc=0.879 loss=0.436 | val acc=0.390 loss=1.427\n",
      "[intermediate_case1][Ep 28] train acc=0.896 loss=0.408 | val acc=0.370 loss=1.607\n",
      "[intermediate_case1][Ep 29] train acc=0.896 loss=0.359 | val acc=0.400 loss=1.469\n",
      "[intermediate_case1][Ep 30] train acc=0.902 loss=0.342 | val acc=0.490 loss=1.486\n",
      "[intermediate_case1][Ep 31] train acc=0.920 loss=0.307 | val acc=0.430 loss=1.453\n",
      "[intermediate_case1][Ep 32] train acc=0.919 loss=0.304 | val acc=0.480 loss=1.712\n",
      "[intermediate_case1][Ep 33] train acc=0.921 loss=0.291 | val acc=0.420 loss=1.675\n",
      "[intermediate_case1][Ep 34] train acc=0.919 loss=0.268 | val acc=0.490 loss=1.570\n",
      "[intermediate_case1][Ep 35] train acc=0.946 loss=0.235 | val acc=0.460 loss=1.476\n",
      "[intermediate_case1][Ep 36] train acc=0.944 loss=0.223 | val acc=0.400 loss=1.845\n",
      "[intermediate_case1][Ep 37] train acc=0.944 loss=0.200 | val acc=0.410 loss=1.817\n",
      "[intermediate_case1][Ep 38] train acc=0.943 loss=0.203 | val acc=0.470 loss=1.732\n",
      "[intermediate_case1][Ep 39] train acc=0.964 loss=0.166 | val acc=0.400 loss=1.664\n",
      "[intermediate_case1][Ep 40] train acc=0.960 loss=0.161 | val acc=0.480 loss=1.719\n",
      "[intermediate_case1][Ep 41] train acc=0.971 loss=0.158 | val acc=0.440 loss=1.862\n",
      "[intermediate_case1][Ep 42] train acc=0.969 loss=0.142 | val acc=0.510 loss=1.656\n",
      "[intermediate_case1][Ep 43] train acc=0.980 loss=0.129 | val acc=0.440 loss=1.698\n",
      "[intermediate_case1][Ep 44] train acc=0.969 loss=0.139 | val acc=0.450 loss=1.792\n",
      "[intermediate_case1][Ep 45] train acc=0.965 loss=0.132 | val acc=0.440 loss=1.801\n",
      "[intermediate_case1][Ep 46] train acc=0.975 loss=0.118 | val acc=0.440 loss=1.711\n",
      "[intermediate_case1][Ep 47] train acc=0.985 loss=0.095 | val acc=0.450 loss=1.725\n",
      "[intermediate_case1][Ep 48] train acc=0.978 loss=0.108 | val acc=0.390 loss=1.853\n",
      "[intermediate_case1][Ep 49] train acc=0.983 loss=0.102 | val acc=0.480 loss=1.646\n",
      "[intermediate_case1][Ep 50] train acc=0.971 loss=0.102 | val acc=0.530 loss=1.695\n",
      "[intermediate_case1][Ep 51] train acc=0.988 loss=0.087 | val acc=0.420 loss=1.756\n",
      "[intermediate_case1][Ep 52] train acc=0.976 loss=0.100 | val acc=0.480 loss=1.855\n",
      "[intermediate_case1][Ep 53] train acc=0.976 loss=0.098 | val acc=0.450 loss=1.868\n",
      "[intermediate_case1][Ep 54] train acc=0.980 loss=0.086 | val acc=0.420 loss=2.054\n",
      "[intermediate_case1][Ep 55] train acc=0.978 loss=0.096 | val acc=0.490 loss=1.866\n",
      "[intermediate_case1][Ep 56] train acc=0.978 loss=0.092 | val acc=0.470 loss=1.854\n",
      "[intermediate_case1][Ep 57] train acc=0.983 loss=0.079 | val acc=0.510 loss=1.923\n",
      "[intermediate_case1][Ep 58] train acc=0.985 loss=0.075 | val acc=0.480 loss=1.943\n",
      "[intermediate_case1][Ep 59] train acc=0.985 loss=0.075 | val acc=0.450 loss=1.910\n",
      "[intermediate_case1][Ep 60] train acc=0.981 loss=0.078 | val acc=0.470 loss=1.941\n",
      "[intermediate_case1][Ep 61] train acc=0.979 loss=0.078 | val acc=0.450 loss=1.972\n",
      "[intermediate_case1][Ep 62] train acc=0.984 loss=0.071 | val acc=0.470 loss=1.860\n",
      "[intermediate_case1][Ep 63] train acc=0.990 loss=0.065 | val acc=0.460 loss=1.970\n",
      "[intermediate_case1][Ep 64] train acc=0.985 loss=0.066 | val acc=0.450 loss=1.982\n",
      "[intermediate_case1][Ep 65] train acc=0.990 loss=0.061 | val acc=0.470 loss=1.998\n",
      "[intermediate_case1][Ep 66] train acc=0.986 loss=0.067 | val acc=0.470 loss=1.975\n",
      "[intermediate_case1][Ep 67] train acc=0.986 loss=0.068 | val acc=0.450 loss=2.001\n",
      "[intermediate_case1][Ep 68] train acc=0.993 loss=0.054 | val acc=0.480 loss=2.003\n",
      "[intermediate_case1][Ep 69] train acc=0.986 loss=0.066 | val acc=0.480 loss=2.000\n",
      "[intermediate_case1][Ep 70] train acc=0.986 loss=0.065 | val acc=0.490 loss=1.974\n",
      "[intermediate_case1][Ep 71] train acc=0.989 loss=0.062 | val acc=0.460 loss=1.980\n",
      "[intermediate_case1][Ep 72] train acc=0.986 loss=0.057 | val acc=0.460 loss=1.983\n",
      "[intermediate_case1][Ep 73] train acc=0.986 loss=0.066 | val acc=0.470 loss=1.995\n",
      "[intermediate_case1][Ep 74] train acc=0.993 loss=0.050 | val acc=0.480 loss=1.982\n",
      "[intermediate_case1][Ep 75] train acc=0.988 loss=0.062 | val acc=0.470 loss=1.987\n",
      "[intermediate_case1][Ep 76] train acc=0.995 loss=0.052 | val acc=0.470 loss=1.986\n",
      "[intermediate_case1][Ep 77] train acc=0.985 loss=0.070 | val acc=0.470 loss=1.985\n",
      "[intermediate_case1][Ep 78] train acc=0.991 loss=0.059 | val acc=0.460 loss=1.974\n",
      "[intermediate_case1][Ep 79] train acc=0.990 loss=0.049 | val acc=0.480 loss=1.981\n",
      "[intermediate_case1][Ep 80] train acc=0.988 loss=0.063 | val acc=0.480 loss=1.976\n",
      "[intermediate_case1][TEST] acc=0.510 loss=2.070\n",
      "[SAVE] ToMNet2_3_12_intermediate_case1 -> ../checkpoints/ToMNet2_3_12_intermediate_case1.pt\n",
      "[DONE] ToMNet2_3_12_intermediate_case1 (best_acc={'val_acc': 0.53, 'test_acc': 0.51})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"../data/3_12\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case1\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")   \n",
    "models = run(cfg, kind=\"ToMNet2\")\n",
    "TomNet_Intermediate1_Model = models[\"ToMNet2_3_12_intermediate_case1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fb7d95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] sim=3_12 mode=intermediate_case2\n",
      "\n",
      "=== Training mode = intermediate_case2 ===\n",
      "[intermediate_case2][Ep 01] train acc=0.279 loss=1.379 | val acc=0.510 loss=1.317\n",
      "[intermediate_case2][Ep 02] train acc=0.438 loss=1.301 | val acc=0.540 loss=1.254\n",
      "[intermediate_case2][Ep 03] train acc=0.484 loss=1.248 | val acc=0.530 loss=1.213\n",
      "[intermediate_case2][Ep 04] train acc=0.501 loss=1.191 | val acc=0.510 loss=1.185\n",
      "[intermediate_case2][Ep 05] train acc=0.524 loss=1.139 | val acc=0.530 loss=1.150\n",
      "[intermediate_case2][Ep 06] train acc=0.542 loss=1.112 | val acc=0.520 loss=1.135\n",
      "[intermediate_case2][Ep 07] train acc=0.569 loss=1.081 | val acc=0.530 loss=1.123\n",
      "[intermediate_case2][Ep 08] train acc=0.575 loss=1.057 | val acc=0.540 loss=1.109\n",
      "[intermediate_case2][Ep 09] train acc=0.595 loss=1.042 | val acc=0.540 loss=1.111\n",
      "[intermediate_case2][Ep 10] train acc=0.603 loss=0.984 | val acc=0.540 loss=1.099\n",
      "[intermediate_case2][Ep 11] train acc=0.608 loss=0.965 | val acc=0.530 loss=1.075\n",
      "[intermediate_case2][Ep 12] train acc=0.651 loss=0.924 | val acc=0.540 loss=1.061\n",
      "[intermediate_case2][Ep 13] train acc=0.666 loss=0.893 | val acc=0.570 loss=1.042\n",
      "[intermediate_case2][Ep 14] train acc=0.671 loss=0.864 | val acc=0.570 loss=1.035\n",
      "[intermediate_case2][Ep 15] train acc=0.705 loss=0.801 | val acc=0.580 loss=1.025\n",
      "[intermediate_case2][Ep 16] train acc=0.704 loss=0.801 | val acc=0.610 loss=1.008\n",
      "[intermediate_case2][Ep 17] train acc=0.743 loss=0.748 | val acc=0.630 loss=1.023\n",
      "[intermediate_case2][Ep 18] train acc=0.751 loss=0.700 | val acc=0.620 loss=1.029\n",
      "[intermediate_case2][Ep 19] train acc=0.757 loss=0.674 | val acc=0.630 loss=0.965\n",
      "[intermediate_case2][Ep 20] train acc=0.795 loss=0.588 | val acc=0.570 loss=1.134\n",
      "[intermediate_case2][Ep 21] train acc=0.824 loss=0.559 | val acc=0.660 loss=0.997\n",
      "[intermediate_case2][Ep 22] train acc=0.819 loss=0.537 | val acc=0.610 loss=0.952\n",
      "[intermediate_case2][Ep 23] train acc=0.828 loss=0.495 | val acc=0.640 loss=0.960\n",
      "[intermediate_case2][Ep 24] train acc=0.851 loss=0.444 | val acc=0.640 loss=1.018\n",
      "[intermediate_case2][Ep 25] train acc=0.865 loss=0.404 | val acc=0.640 loss=0.995\n",
      "[intermediate_case2][Ep 26] train acc=0.860 loss=0.394 | val acc=0.610 loss=1.025\n",
      "[intermediate_case2][Ep 27] train acc=0.875 loss=0.358 | val acc=0.620 loss=1.060\n",
      "[intermediate_case2][Ep 28] train acc=0.905 loss=0.310 | val acc=0.620 loss=1.029\n",
      "[intermediate_case2][Ep 29] train acc=0.902 loss=0.303 | val acc=0.650 loss=0.986\n",
      "[intermediate_case2][Ep 30] train acc=0.906 loss=0.287 | val acc=0.610 loss=1.089\n",
      "[intermediate_case2][Ep 31] train acc=0.926 loss=0.255 | val acc=0.610 loss=1.016\n",
      "[intermediate_case2][Ep 32] train acc=0.935 loss=0.233 | val acc=0.640 loss=1.046\n",
      "[intermediate_case2][Ep 33] train acc=0.934 loss=0.232 | val acc=0.660 loss=1.073\n",
      "[intermediate_case2][Ep 34] train acc=0.951 loss=0.206 | val acc=0.620 loss=1.149\n",
      "[intermediate_case2][Ep 35] train acc=0.949 loss=0.204 | val acc=0.630 loss=1.110\n",
      "[intermediate_case2][Ep 36] train acc=0.946 loss=0.191 | val acc=0.690 loss=1.085\n",
      "[intermediate_case2][Ep 37] train acc=0.965 loss=0.145 | val acc=0.630 loss=1.127\n",
      "[intermediate_case2][Ep 38] train acc=0.950 loss=0.159 | val acc=0.660 loss=1.182\n",
      "[intermediate_case2][Ep 39] train acc=0.966 loss=0.139 | val acc=0.680 loss=1.105\n",
      "[intermediate_case2][Ep 40] train acc=0.960 loss=0.139 | val acc=0.570 loss=1.252\n",
      "[intermediate_case2][Ep 41] train acc=0.973 loss=0.120 | val acc=0.540 loss=1.328\n",
      "[intermediate_case2][Ep 42] train acc=0.970 loss=0.124 | val acc=0.640 loss=1.074\n",
      "[intermediate_case2][Ep 43] train acc=0.958 loss=0.127 | val acc=0.660 loss=1.069\n",
      "[intermediate_case2][Ep 44] train acc=0.973 loss=0.116 | val acc=0.610 loss=1.288\n",
      "[intermediate_case2][Ep 45] train acc=0.970 loss=0.110 | val acc=0.550 loss=1.312\n",
      "[intermediate_case2][Ep 46] train acc=0.969 loss=0.103 | val acc=0.630 loss=1.130\n",
      "[intermediate_case2][Ep 47] train acc=0.980 loss=0.091 | val acc=0.670 loss=1.246\n",
      "[intermediate_case2][Ep 48] train acc=0.970 loss=0.105 | val acc=0.570 loss=1.181\n",
      "[intermediate_case2][Ep 49] train acc=0.976 loss=0.091 | val acc=0.660 loss=1.258\n",
      "[intermediate_case2][Ep 50] train acc=0.970 loss=0.115 | val acc=0.620 loss=1.332\n",
      "[intermediate_case2][Ep 51] train acc=0.979 loss=0.088 | val acc=0.610 loss=1.247\n",
      "[intermediate_case2][Ep 52] train acc=0.976 loss=0.082 | val acc=0.630 loss=1.261\n",
      "[intermediate_case2][Ep 53] train acc=0.980 loss=0.078 | val acc=0.660 loss=1.230\n",
      "[intermediate_case2][Ep 54] train acc=0.990 loss=0.063 | val acc=0.620 loss=1.195\n",
      "[intermediate_case2][Ep 55] train acc=0.985 loss=0.066 | val acc=0.640 loss=1.228\n",
      "[intermediate_case2][Ep 56] train acc=0.995 loss=0.064 | val acc=0.680 loss=1.246\n",
      "[intermediate_case2][Ep 57] train acc=0.989 loss=0.051 | val acc=0.700 loss=1.189\n",
      "[intermediate_case2][Ep 58] train acc=0.985 loss=0.068 | val acc=0.710 loss=1.147\n",
      "[intermediate_case2][Ep 59] train acc=0.989 loss=0.060 | val acc=0.640 loss=1.258\n",
      "[intermediate_case2][Ep 60] train acc=0.991 loss=0.059 | val acc=0.650 loss=1.238\n",
      "[intermediate_case2][Ep 61] train acc=0.988 loss=0.057 | val acc=0.660 loss=1.234\n",
      "[intermediate_case2][Ep 62] train acc=0.988 loss=0.057 | val acc=0.690 loss=1.231\n",
      "[intermediate_case2][Ep 63] train acc=0.989 loss=0.047 | val acc=0.680 loss=1.231\n",
      "[intermediate_case2][Ep 64] train acc=0.996 loss=0.043 | val acc=0.660 loss=1.235\n",
      "[intermediate_case2][Ep 65] train acc=0.990 loss=0.046 | val acc=0.680 loss=1.194\n",
      "[intermediate_case2][Ep 66] train acc=0.991 loss=0.052 | val acc=0.680 loss=1.227\n",
      "[intermediate_case2][Ep 67] train acc=0.990 loss=0.050 | val acc=0.710 loss=1.246\n",
      "[intermediate_case2][Ep 68] train acc=0.993 loss=0.054 | val acc=0.690 loss=1.272\n",
      "[intermediate_case2][Ep 69] train acc=0.989 loss=0.048 | val acc=0.660 loss=1.296\n",
      "[intermediate_case2][Ep 70] train acc=0.990 loss=0.045 | val acc=0.680 loss=1.297\n",
      "[intermediate_case2][Ep 71] train acc=0.985 loss=0.052 | val acc=0.680 loss=1.278\n",
      "[intermediate_case2][Ep 72] train acc=0.989 loss=0.045 | val acc=0.690 loss=1.255\n",
      "[intermediate_case2][Ep 73] train acc=0.989 loss=0.046 | val acc=0.670 loss=1.236\n",
      "[intermediate_case2][Ep 74] train acc=0.989 loss=0.051 | val acc=0.670 loss=1.235\n",
      "[intermediate_case2][Ep 75] train acc=0.994 loss=0.046 | val acc=0.670 loss=1.236\n",
      "[intermediate_case2][Ep 76] train acc=0.991 loss=0.053 | val acc=0.670 loss=1.243\n",
      "[intermediate_case2][Ep 77] train acc=0.994 loss=0.041 | val acc=0.670 loss=1.242\n",
      "[intermediate_case2][Ep 78] train acc=0.991 loss=0.045 | val acc=0.670 loss=1.248\n",
      "[intermediate_case2][Ep 79] train acc=0.995 loss=0.042 | val acc=0.670 loss=1.245\n",
      "[intermediate_case2][Ep 80] train acc=0.990 loss=0.045 | val acc=0.670 loss=1.245\n",
      "[intermediate_case2][TEST] acc=0.590 loss=1.873\n",
      "[SAVE] ToMNet2_3_12_intermediate_case2 -> ../checkpoints/ToMNet2_3_12_intermediate_case2.pt\n",
      "[DONE] ToMNet2_3_12_intermediate_case2 (best_acc={'val_acc': 0.71, 'test_acc': 0.59})\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig(\n",
    "    data_root=Path(\"../data/3_12\"),\n",
    "    sim_name=\"3_12\",\n",
    "    mode=\"intermediate_case2\",\n",
    "    seed=42,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    dropout=0.2,\n",
    "    max_epochs=80,\n",
    ")   \n",
    "models = run(cfg, kind=\"ToMNet2\")\n",
    "TomNet_Intermediate2_Model = models[\"ToMNet2_3_12_intermediate_case2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb8e50",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluate import evaluate_model_performance\n",
    "\n",
    "def forward_tomnet2(m: nn.Module, b: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"ToMNet2\"\"\"\n",
    "    return m(\n",
    "        b[\"grid_seq_j\"],\n",
    "        b[\"tmask_j\"], \n",
    "        b[\"grid_seq_k\"],\n",
    "        b[\"tmask_k\"],\n",
    "        b[\"choices_ids\"],\n",
    "        b[\"choices_mask\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bbd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet2_3_12_random model evaluation ===\n",
      "accuracy: 0.5300\n",
      "precision: 0.5299\n",
      "recall rate: 0.5300\n",
      "F1 score: 0.5293\n",
      "ROC AUC: 0.7805\n",
      "R2 Score: -0.5215\n",
      "Prediction Entropy: 0.4337\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 155,808,768\n",
      "Energy per sample: 7.17e-04 J\n",
      "Energy per accuracy unit: 1.35e-03 J/acc\n",
      "MACs per accuracy unit: 293,978,808 MACs/acc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"random\")\n",
    "\n",
    "\n",
    "dataset = ToMNet2JsonlDataset(\n",
    "    split_root=cfg.data_root / \"validation\", \n",
    "    sim_name=cfg.sim_name,\n",
    "    use_modes=[\"random\"],\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "one_sample = dataset[0][\"grid_seq_j\"] \n",
    "input_hw = one_sample.shape[1:3]  \n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet2,\n",
    "    energy_forward_fn=forward_tomnet2\n",
    ")\n",
    "\n",
    "print(\"=== ToMNet2_3_12_random model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68d1bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet2_3_12_rulemap model evaluation ===\n",
      "accuracy: 0.1900\n",
      "precision: 0.1922\n",
      "recall rate: 0.1900\n",
      "F1 score: 0.1866\n",
      "ROC AUC: 0.3764\n",
      "R2 Score: -1.2249\n",
      "Prediction Entropy: 0.5308\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 155,628,544\n",
      "Energy per sample: 7.16e-04 J\n",
      "Energy per accuracy unit: 3.77e-03 J/acc\n",
      "MACs per accuracy unit: 819,097,600 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"rulemap\")\n",
    "\n",
    "dataset = ToMNet2JsonlDataset(\n",
    "    split_root=cfg.data_root / \"validation\", \n",
    "    sim_name=cfg.sim_name,\n",
    "    use_modes=[\"rulemap\"],\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "one_sample = dataset[0][\"grid_seq_j\"]  # 使用 grid_seq_j 而不是 grid_seq\n",
    "input_hw = one_sample.shape[1:3]  # 获取 H, W\n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet2,\n",
    "    energy_forward_fn=forward_tomnet2\n",
    ")\n",
    "\n",
    "print(\"=== ToMNet2_3_12_rulemap model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b78d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet2_3_12_logic model evaluation ===\n",
      "accuracy: 0.1700\n",
      "precision: 0.1629\n",
      "recall rate: 0.1700\n",
      "F1 score: 0.1650\n",
      "ROC AUC: 0.4153\n",
      "R2 Score: -1.5187\n",
      "Prediction Entropy: 0.5656\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 155,776,000\n",
      "Energy per sample: 7.17e-04 J\n",
      "Energy per accuracy unit: 4.22e-03 J/acc\n",
      "MACs per accuracy unit: 916,329,412 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"logic\")\n",
    "\n",
    "dataset = ToMNet2JsonlDataset(\n",
    "    split_root=cfg.data_root / \"validation\", \n",
    "    sim_name=cfg.sim_name,\n",
    "    use_modes=[\"logic\"],\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "one_sample = dataset[0][\"grid_seq_j\"] \n",
    "input_hw = one_sample.shape[1:3]  \n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet2,\n",
    "    energy_forward_fn=forward_tomnet2\n",
    ")\n",
    "\n",
    "print(\"=== ToMNet2_3_12_logic model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfb7be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet2_3_12_intermediate_case1 model evaluation ===\n",
      "accuracy: 0.5300\n",
      "precision: 0.5513\n",
      "recall rate: 0.5300\n",
      "F1 score: 0.5341\n",
      "ROC AUC: 0.7775\n",
      "R2 Score: -0.2288\n",
      "Prediction Entropy: 0.4121\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 155,972,608\n",
      "Energy per sample: 7.17e-04 J\n",
      "Energy per accuracy unit: 1.35e-03 J/acc\n",
      "MACs per accuracy unit: 294,287,940 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"intermediate_case1\")\n",
    "\n",
    "dataset = ToMNet2JsonlDataset(\n",
    "    split_root=cfg.data_root / \"validation\", \n",
    "    sim_name=cfg.sim_name,\n",
    "    use_modes=[\"intermediate_case1\"],\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "one_sample = dataset[0][\"grid_seq_j\"]  \n",
    "input_hw = one_sample.shape[1:3]\n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet2,\n",
    "    energy_forward_fn=forward_tomnet2\n",
    ")\n",
    "\n",
    "print(\"=== ToMNet2_3_12_intermediate_case1 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65c12de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ToMNet2_3_12_intermediate_case2 model evaluation ===\n",
      "accuracy: 0.6100\n",
      "precision: 0.6146\n",
      "recall rate: 0.6100\n",
      "F1 score: 0.6112\n",
      "ROC AUC: 0.8442\n",
      "R2 Score: -0.0007\n",
      "Prediction Entropy: 0.4194\n",
      "\n",
      "=== Energy and Efficiency Evaluation ===\n",
      "MACs per sample: 155,939,840\n",
      "Energy per sample: 7.17e-04 J\n",
      "Energy per accuracy unit: 1.18e-03 J/acc\n",
      "MACs per accuracy unit: 255,639,082 MACs/acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用 ToMNet2JsonlDataset 而不是 GridSeqDataset\n",
    "val_loader, in_ch = make_loader(cfg, split=\"val\", mode=\"intermediate_case2\")\n",
    "\n",
    "# 获取输入尺寸 - 从数据集中获取一个样本\n",
    "dataset = ToMNet2JsonlDataset(\n",
    "    split_root=cfg.data_root / \"validation\",  # 验证集路径\n",
    "    sim_name=cfg.sim_name,\n",
    "    use_modes=[\"intermediate_case2\"],\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "one_sample = dataset[0][\"grid_seq_j\"]  # 使用 grid_seq_j 而不是 grid_seq\n",
    "input_hw = one_sample.shape[1:3]  # 获取 H, W\n",
    "\n",
    "metrics = evaluate_model_performance(\n",
    "    TomNet_Random_Model,\n",
    "    val_loader,\n",
    "    device=\"cpu\",\n",
    "    forward_fn=forward_tomnet2,\n",
    "    energy_forward_fn=forward_tomnet2\n",
    ")\n",
    "\n",
    "print(\"=== ToMNet2_3_12_intermediate_case2 model evaluation ===\")\n",
    "print(f\"accuracy: {metrics.accuracy:.4f}\")\n",
    "print(f\"precision: {metrics.precision:.4f}\")\n",
    "print(f\"recall rate: {metrics.recall:.4f}\")\n",
    "print(f\"F1 score: {metrics.f1_score:.4f}\")\n",
    "print(f\"ROC AUC: {metrics.roc_auc:.4f}\")\n",
    "print(f\"R2 Score: {metrics.r2_score:.4f}\")\n",
    "print(f\"Prediction Entropy: {metrics.prediction_entropy:.4f}\")\n",
    "print(\"\\n=== Energy and Efficiency Evaluation ===\")\n",
    "print(f\"MACs per sample: {metrics.energy_report.macs_per_sample:,.0f}\")\n",
    "print(f\"Energy per sample: {metrics.energy_report.ann_energy_per_sample:.2e} J\")\n",
    "print(f\"Energy per accuracy unit: {metrics.energy_per_accuracy:.2e} J/acc\")\n",
    "print(f\"MACs per accuracy unit: {metrics.macs_per_accuracy:,.0f} MACs/acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2311d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
